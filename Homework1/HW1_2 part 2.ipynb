{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2S84TTPP9BT"
   },
   "source": [
    "**2.Visualize the Optimization Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oK0pnZWgQIkg",
    "outputId": "67637e91-2065-4b91-a807-ac083c3a1e9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xshape: torch.Size([300, 1]) \n",
      " Yshape: torch.Size([300, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-faa07d40a19f>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1) \n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, 300), dim=1)  # x data (tensor), shape=(300, 1)\n",
    "y = (np.sin(5*np.pi*x))/(5*np.pi*x) #non-linear y function\n",
    "#y = (np.cos(7 * x * np.pi))\n",
    "print('Xshape:',x.shape,\"\\n Yshape:\",y.shape)\n",
    "\n",
    "#x, y = Variable(x), Variable(y) #converting data into variables as pytorch requirments\n",
    "x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "n_samples, n_features = x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "W6w3ITSmQeSL"
   },
   "outputs": [],
   "source": [
    "class M1(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.fc1 = nn.Linear(1, 500)\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # flatten as one dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TOG6GJDCQeXP"
   },
   "outputs": [],
   "source": [
    "def trainFunc(model,x,y):\n",
    "    max_epoch = 2500\n",
    "    epoch_arr,loss_arr=[],[]\n",
    "    not_converged = True\n",
    "    epoch = 0\n",
    "    gradArr = []\n",
    "    while not_converged:\n",
    "        epoch += 1\n",
    "        prediction = model(x)     # input x and predict based on x\n",
    "        loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        #optimizer.step() \n",
    "        \n",
    "        epoch_arr.append(epoch)\n",
    "        loss_arr.append(loss.detach().numpy())\n",
    "\n",
    "        #Generating Gradient Norm\n",
    "        grad_all = 0.0\n",
    "        for p in model.parameters():\n",
    "            grad = 0.0\n",
    "            if p.grad is not None:\n",
    "                grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            grad_all += grad\n",
    "        grad_norm = grad_all ** 0.5\n",
    "        \n",
    "\n",
    "        optimizer.step() \n",
    "        \n",
    "        gradArr.append(grad_norm)\n",
    "        \n",
    "        if epoch%100 == 0 : print(f'epoch: {epoch}, loss = {loss.item():.4f}, grad_norm = {grad_norm}') #, weight = {model.weight.item()},  bias ={model.bias.item()}')\n",
    "                \n",
    "        if epoch == max_epoch:\n",
    "                print(\"Max Epoch Reached\")\n",
    "                not_converged = False\n",
    "        elif (epoch > 5) and  (loss_arr[-1] < 0.001):\n",
    "            if abs(loss_arr[-3] - loss_arr[-2]) < 1.0e-05 and abs(loss_arr[-2] - loss_arr[-1]) < 1.0e-05:\n",
    "                print(\"Convergeance reached for loss:\",loss_arr[-1])\n",
    "                not_converged = False\n",
    "        \n",
    "        \n",
    "                \n",
    "    return epoch_arr,loss_arr,prediction,grad_norm,gradArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHSTG_QVQIoc",
    "outputId": "a315fb85-780c-4236-b3ac-e5449a7958fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of parameters in Model 1: 1501\n",
      "epoch: 100, loss = 0.0518, grad_norm = 0.042833168430706926\n",
      "epoch: 200, loss = 0.0310, grad_norm = 0.031609784687791964\n",
      "epoch: 300, loss = 0.0158, grad_norm = 0.020875737090560575\n",
      "epoch: 400, loss = 0.0092, grad_norm = 0.012162938301290347\n",
      "epoch: 500, loss = 0.0066, grad_norm = 0.007910376219090675\n",
      "epoch: 600, loss = 0.0053, grad_norm = 0.0057546218075202665\n",
      "epoch: 700, loss = 0.0045, grad_norm = 0.004773614439532592\n",
      "epoch: 800, loss = 0.0038, grad_norm = 0.00421009314738074\n",
      "epoch: 900, loss = 0.0032, grad_norm = 0.003963816600963438\n",
      "epoch: 1000, loss = 0.0027, grad_norm = 0.006400480524769369\n",
      "epoch: 1100, loss = 0.0023, grad_norm = 0.010663530350307836\n",
      "epoch: 1200, loss = 0.0020, grad_norm = 0.010301699507119793\n",
      "epoch: 1300, loss = 0.0017, grad_norm = 0.0045395204436342144\n",
      "epoch: 1400, loss = 0.0015, grad_norm = 0.0023008825129989057\n",
      "epoch: 1500, loss = 0.0014, grad_norm = 0.015228323280006536\n",
      "epoch: 1600, loss = 0.0012, grad_norm = 0.02503499960745954\n",
      "epoch: 1700, loss = 0.0011, grad_norm = 0.002067714825845357\n",
      "epoch: 1800, loss = 0.0010, grad_norm = 0.012088087030096494\n",
      "Convergeance reached for loss: 0.0009996573\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "m1 = M1()     # define the network\n",
    "#optimizer = torch.optim.RMSprop(m1.parameters(), lr=0.0012, weight_decay = 1e-4)\n",
    "optimizer = torch.optim.Adam(m1.parameters(), lr=1e-3, weight_decay = 1e-4)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "a=[]\n",
    "for i in m1.parameters():\n",
    "    a.append(torch.numel(i))\n",
    "print('Total no of parameters in Model 1:', np.sum(a),)\n",
    "M1epoch_arr,M1loss_arr,M1prediction,M1grad_norm,M1gradArr = trainFunc(m1,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "IumGjmkdQIsk",
    "outputId": "24fea74d-5f77-42ff-b46d-7b719a1dd413"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcVZ3+8c+TyT1ACEkWSAIkSESDgOAsigq4sIZEIBEFJPLTgFFAQUW8AaIC67qLrqJAdhFNWBSRm7cgKKiIIIuYgBCMCAwQIBdISALhEsjt+/vjnCGdpieZmUxN9cw879erXl19qrr72zWXp+uc6ipFBGZmZtV6lV2AmZnVJweEmZnV5IAwM7OaHBBmZlaTA8LMzGpyQJiZWU0OCCuFpHdJWlB2HW0l6SxJPyi7jtbqCvVKmifpXR29rm05B0Q3Jmm+pH8t6bXfLukWSc9Lek7S9ZLGlVFLe9UKsYj4ekR8tIDXOl7SOkkv5OkxSZdJev2WPG8R9UrauaLOFySFpBcr7h/Qxhr3iIhbO3pd23IOCOtwkvYHbgZ+CYwAxgD3AXdI2rUT6+jdWa/VQe6MiK2AwcC/AquAuyW9qT1PVtT7j4gnImKr5ik3713RdnvRNVjncED0QJL6SfqOpEV5+o6kfnnZMEm/kvSspOWSbpfUKy/7oqSFea/gQUmHtPAS3wB+GBHfjYjnI2J5RJwN/Bk4p6qWsyQ9k/d2jqtof4+kv+fXWijpcxXLDpd0b67x/yTtVbFsfq5zLvBinr+u6jW/K+nCPH+CpAfy6zwq6aTcPgj4NTCi4pPxCEnnSLqi4rkm5W6PZyXdKumNVbV8TtLcvBd1taT+m/v5RMS6iHgkIj4B/LF5m9Xao6ncS8y1XSfpCkkrgeMr65U0On/anyrpibzdv1TxXAMkXS5pRd4mX2hrN2DeE7pD0gWSlgHnSHpd3ptcll/zx5K23cR7uEbSD/PPZJ6kxnauu6+kv+Zl1+bt/7W2vJ+ezgHRM30JeBvwZmBvYD/g7Lzss8ACYDiwPXAWEJJ2B04F/jkitgYOBeZXP7GkgcDbgWtrvO41wLsr7u8ADANGAlOBS/PrAMwATsqv9Sbglvz8+wAzgZOAocD3gFnNAZdNAQ4DtgWuAt4jaev8+AbgGODKvO4S4HBgG+AE4AJJ+0bEi8BEYFHFJ+NFVe/19cBPgNPy9roRuF5S34rVjgEmkPai9gKOr7FdNuVnQFu6bCYD15He+49bWOedwO7AIcBXKkLtq8BoYFfSz+n/tbHWZm8FHiX9/vw7IOA/SHuTbwR2ouqDQpVJpJ/btsAs4OK2rpt/Bj8H/hfYjvRzOrJ9b6fnckD0TMcB50XEkohYCpwLfCgvWwPsCOwSEWsi4vZIJ+xaB/QDxknqExHzI+KRGs+9Hen3anGNZYtJgVDpyxHxSkT8EbiB9A+1uY5xkraJiBURcU9uPxH4XkTclT9pXw68Qgq8ZhdGxJMRsSoiHgfuYcM/h4OBlyLizwARcUP+tB65hptp/T/kDwA3RMRvI2IN8F/AAFJAVtayKCKWA9eTQrktFpG2aWvdGRG/iIj1EbGqhXXOzdvmPlLX3965/Rjg63l7LwAubGOtr9YcERdFxNr8Ok15G72Sf9++DRy0icf/KSJujIh1wI8q6mvLum8DepO2/5qI+Bnwl3a+nx7LAdEzjQAer7j/eG4D+CbQBNycu1zOAIiIJtIn5XOAJZKukjSC11oBrCeFTLUdgWcq182f1GvV8X7gPcDjkv6oNK4BsAvw2dyl86ykZ0mfSCtrebLqda8k7VUAfJANew9Imijpz0rdac/m16wOsZZstB0jYn1+7ZEV6zxVMf8SsBVtMxJY3ob1q997LS3VNKLq8a15rs3WIGn7/PuyMHd9XcGmt3F1ff3V8lhGS+uOABbGxmcjbe/76bEcED3TItI/2mY75zbymMFnI2JX0u776cpjDRFxZUS8Mz82gPOrnzj/w78TOLrG6x4D/L7i/pDc11+rjtkRMRn4J+AXpO4pSH/k/x4R21ZMAyPiJ5VlVL3utcC7JI0i7UlcCWksBvgp6ZP/9hGxLambSC08T7WNtqMkkcJq4WYe1xZHAs2Dvi8CAyter4HUtVVpS07PvBgYVXF/p3Y+T3UNX89te0bENqSuK73mUR1rMTAy/0yatff99FgOiO6vj6T+FVNvUn/s2ZKGSxoGfIX0qa55AHi3/If1HKlrab2k3SUdnP+pvkw6wmZ9C695BjBV0qckbS1pSB4c3J/UnVXpXEl9lQ6NPBy4Nt8/TtLg3HWzsuK1vg+cLOmtSgZJOqx5jKGW3K1xK3AZ8FhEPJAX9SV1my0F1kqaCIyveOjTwFBJg1t46muAwyQdIqkPafzmFeD/WqqlNSQ1SBoj6SLgXWzYZg+RPiEfll/v7Fx/R7kGODP/vEaSxpw6wtbAC8Bz+Xk/30HPuyl3kn53T5XUW9Jk0libtYEDovu7kfTPvHk6B/gaMAeYC9xP6qNvPrpjLPA70h/0ncB/R8QfSP+I/pPURfQU6ZP9mbVeMCL+RBrEfh/pk9zjwD7AOyPi4YpVnyJ1SS0iDaieHBH/yMs+BMzPXRInk8ZNiIg5wMdIg5ErSN1hx7diO1xJOnT01e6liHge+BTpH+MKUvfTrIrl/yCF6aO5O2ujLrWIeJD0afiivF2OAI6IiNWtqKeW/SW9QArEW0kD5/8cEffn13sO+ATwA9JeyoukAwo6ynn5+R4j/Q5cRwq8LXUusC/pA8cNpIH3QuWfwfuAacCzpJ/Tr+iY99NjyBcMMrNaJH0cODYiNjWg3GVIugu4JCIuK7uWrsJ7EGYGgKQdJb1DUq98uPFnSYeKdkmSDpK0Q+5imko6zPg3ZdfVlfhbjmbWrC/peyVjSN0yVwH/XWpFW2Z3UvfhINL3Mo6KiFqHX1sL3MVkZmY1uYvJzMxq6jZdTMOGDYvRo0eXXYaZWZdy9913PxMR1d+nAbpRQIwePZo5c+aUXYaZWZci6fGWlrmLyczManJAmJlZTQ4IMzOryQFhZmY1OSDMzKwmB4SZmdXkgDAzs5ocEAsWwJe/DA89VHYlZmZ1xQGxaBF87Wvw8MObX9fMrAdxQPTKm8AnLTQz24gDovmStetbunqmmVnP5IBoDgjvQZiZbcQB4YAwM6vJAeExCDOzmhwQHoMwM6vJAeEuJjOzmhwQDggzs5ocEB6DMDOryQHhMQgzs5ocEO5iMjOryQHhgDAzq8kB4TEIM7OaHBAegzAzq8kB4S4mM7OaCg0ISRMkPSipSdIZNZYfKOkeSWslHVW1bKqkh/M0tcAi060DwsxsI4UFhKQGYDowERgHTJE0rmq1J4DjgSurHrsd8FXgrcB+wFclDSmo0HTrgDAz20iRexD7AU0R8WhErAauAiZXrhAR8yNiLlA9AHAo8NuIWB4RK4DfAhMKqbJ5kNpjEGZmGykyIEYCT1bcX5DbOuyxkk6UNEfSnKVLl7avSu9BmJnV1KUHqSPi0ohojIjG4cOHt+9JHBBmZjUVGRALgZ0q7o/KbUU/tm0cEGZmNRUZELOBsZLGSOoLHAvMauVjbwLGSxqSB6fH57aO5y/KmZnVVFhARMRa4FTSP/YHgGsiYp6k8yRNApD0z5IWAEcD35M0Lz92OfBvpJCZDZyX2zqevyhnZlZT7yKfPCJuBG6savtKxfxsUvdRrcfOBGYWWR/gLiYzsxZ06UHqDuGAMDOryQHhMQgzs5ocEB6DMDOryQHhLiYzs5ocEM0Bcfrp5dZhZlZnHBDNYxDr1pVbh5lZnXFANO9BmJnZRhwQDggzs5ocEGvWlF2BmVldckBsu23ZFZiZ1SUHREMDHHMM7L572ZWYmdUVBwRA376wenXZVZiZ1RUHBKSA8FiEmdlGHBDgPQgzsxocEOCAMDOrwQEBDggzsxocEJAC4qWX4Kmnyq7EzKxuOCAAFi9Ot0cfXW4dZmZ1xAEBG7qXFiwotw4zszrigABYtSrd+qJBZmavckAAvPBCuvVFg8zMXuWAgA17EA0N5dZhZlZHHBAAM2ak2332KbcOM7M64oAAGDsW9t7bV5UzM6vggGjWvz+8/HLZVZiZ1Q0HRLP+/eGVV8quwsysbjggmvXr5z0IM7MKhQaEpAmSHpTUJOmMGsv7Sbo6L79L0ujc3kfS5ZLul/SApDOLrBNwF5OZWZXCAkJSAzAdmAiMA6ZIGle12jRgRUTsBlwAnJ/bjwb6RcSewFuAk5rDozAOCDOzjRS5B7Ef0BQRj0bEauAqYHLVOpOBy/P8dcAhkgQEMEhSb2AAsBpYWWCtDggzsypFBsRI4MmK+wtyW811ImIt8BwwlBQWLwKLgSeA/4qI5dUvIOlESXMkzVm6dOmWVeuAMDPbSL0OUu8HrANGAGOAz0ratXqliLg0IhojonH48OFb9opLl8LTT8P992/Z85iZdRNFBsRCYKeK+6NyW811cnfSYGAZ8EHgNxGxJiKWAHcAjQXWCuPHp9srrij0ZczMuooiA2I2MFbSGEl9gWOBWVXrzAKm5vmjgFsiIkjdSgcDSBoEvA34R4G1wkknwYABsHZtoS9jZtZVFBYQeUzhVOAm4AHgmoiYJ+k8SZPyajOAoZKagNOB5kNhpwNbSZpHCprLImJuUbUCIMHIkbBoUaEvY2bWVfQu8skj4kbgxqq2r1TMv0w6pLX6cS/Uai/ciBEOCDOzrF4HqcsRAbfdBkuWlF2JmVnpHBCVjjoq3d55Z7l1mJnVAQdEpSlT0u38+aWWYWZWDxwQlYYNg0GD4LHHyq7EzKx0DohKEowZ44AwM8MB8Vq9esGsWbBsWdmVmJmVygFRbdq0dPub35Rbh5lZyRwQ1U45BbbZBm6/vexKzMxK5YCo1tAAb387/OEP6XsRZmY9lAOilokT4aGH4Iknyq7EzKw0DohaDjoo3bqbycx6MAdELW96UxqHuOWWsisxMyuNA6KWhgY47DD45S9h/fqyqzEzK4UDoiXvfjcsXw7z5pVdiZlZKRwQLTn44HTrbiYz66EcEC3ZZRfYbTe4+eayKzEzK4UDYlMmTIBbb4WXXy67EjOzTueA2JQJE+Cll+BPfyq7EjOzTueA2JR3vQv69vV5mcysR3JAbMqgQXDAAQ4IM+uRHBCbM2FCOtT1ySfLrsTMrFM5IDZnwoR0e9NN5dZhZtbJHBCbs8ceMGoU/OpXZVdiZtapHBCbI8Hkyen7EC+9VHY1ZmadxgHRGkceCatWuZvJzHoUB0RrHHggDBkCv/hF2ZWYmXUaB0Rr9OkDhx8O118Pa9aUXY2ZWacoNCAkTZD0oKQmSWfUWN5P0tV5+V2SRlcs20vSnZLmSbpfUv8ia92sI4+EFSt8ESEz6zEKCwhJDcB0YCIwDpgiaVzVatOAFRGxG3ABcH5+bG/gCuDkiNgDeBdQ7kf38eOhf3/4+c9LLcPMrLMUuQexH9AUEY9GxGrgKmBy1TqTgcvz/HXAIZIEjAfmRsR9ABGxLCLWFVjr5g0aBIcemsYhIkotxcysM7QqICQNktQrz79e0iRJfTbzsJFA5dePF+S2mutExFrgOWAo8HogJN0k6R5JX2ihrhMlzZE0Z+nSpa15K1vmyCNhwQK4++7iX8vMrGSt3YO4DegvaSRwM/Ah4H+LKgroDbwTOC7fHinpkOqVIuLSiGiMiMbhw4cXWE52+OHpcqTuZjKzHqC1AaGIeAl4H/DfEXE0sMdmHrMQ2Kni/qjcVnOdPO4wGFhG2tu4LSKeya97I7BvK2stztChcNBBcO217mYys26v1QEhaX/SJ/obclvDZh4zGxgraYykvsCxwKyqdWYBU/P8UcAtERHATcCekgbm4DgI+Hsray3WlCnw8MPuZjKzbq+1AXEacCbw84iYJ2lX4A+bekAeUziV9M/+AeCa/NjzJE3Kq80AhkpqAk4HzsiPXQF8mxQy9wL3RMQN1a9Rive/P30v4ic/KbsSM7NCKdrYVZIHq7eKiJXFlNQ+jY2NMWfOnM55sUmT4J574IknoJe/a2hmXZekuyOisday1h7FdKWkbSQNAv4G/F3S5zuyyC5lyhRYuNBfmjOzbq21H3/H5T2G9wK/BsaQjmTqmSZNgoED4cory67EzKwwrQ2IPvl7D+8FZkXEGqDnHsYzaFA6Bfh118Hq1WVXY2ZWiNYGxPeA+cAg4DZJuwB1NQbR6Y47DpYvhxtvLLsSM7NCtCogIuLCiBgZEe+J5HHgXwqurb4deijsuCPMmFF2JWZmhWjtIPVgSd9uPq2FpG+R9iZ6rt69YerUtAexaFHZ1ZiZdbjWdjHNBJ4HjsnTSuCyoorqMj7yEVi/Hi6/fPPrmpl1Ma0NiNdFxFfzmVkfjYhzgV2LLKxLGDs2XW1u5kyfesPMup3WBsQqSe9sviPpHcCqYkrqYqZNg6YmfyfCzLqd1gbEycB0SfMlzQcuBk4qrKqu5KijYPBguOSSsisxM+tQrT2K6b6I2BvYC9grIvYBDi60sq5i4EA44YR0htfFi8uuxsysw7TpREIRsbLiHEynF1BP1/SJT8DatfD975ddiZlZh9mSM82pw6ro6saOhQkTUjfTmnIvnW1m1lG2JCB82E6lU05JXUy+2pyZdRObDAhJz0taWWN6HhjRSTV2DRMnwpgxMH162ZWYmXWITQZERGwdEdvUmLaOiN6dVWSX0NCQxiJuuw3mzi27GjOzLear3XSkj3wE+veHiy4quxIzsy3mgOhI220HH/4wXHEFPPNM2dWYmW0RB0RH+/Sn4eWX4XvfK7sSM7Mt4oDoaOPGwfjxabDaFxMysy7MAVGEz3wmHfJ6zTVlV2Jm1m4OiCKMHw9veANccIHP8mpmXZYDogi9eqWxiHvugTvuKLsaM7N2cUAU5cMfhiFD4DvfKbsSM7N2cUAUZeBAOOmkdOqNxx4ruxozszZzQBTplFNSd9PFF5ddiZlZmzkgijRqFBx9NPzgB/D882VXY2bWJoUGhKQJkh6U1CTpjBrL+0m6Oi+/S9LoquU7S3pB0ueKrLNQp50GK1fCZZeVXYmZWZsUFhCSGoDpwERgHDBF0riq1aYBKyJiN+AC4Pyq5d8Gfl1UjZ1iv/1g//3hwgth3bqyqzEza7Ui9yD2A5oi4tGIWA1cBUyuWmcycHmevw44RJIAJL0XeAyYV2CNneMzn4FHHoEbbii7EjOzVisyIEYCT1bcX5Dbaq4TEWuB54ChkrYCvgicW2B9nefII2GnndIX58zMuoh6HaQ+B7ggIl7Y1EqSTpQ0R9KcpUuXdk5l7dG7N3zyk3DrrXDvvWVXY2bWKkUGxEJgp4r7o3JbzXUk9QYGA8uAtwLfkDQfOA04S9Kp1S8QEZdGRGNENA4fPrzj30FH+uhHYdAg+O53y67EzKxVigyI2cBYSWMk9QWOBWZVrTMLmJrnjwJuieSAiBgdEaOB7wBfj4iu/WWCIUPg+OPhyivhqafKrsbMbLMKC4g8pnAqcBPwAHBNRMyTdJ6kSXm1GaQxhybgdOA1h8J2K5/6VDoF+CWXlF2JmdlmKbrJ2UYbGxtjzpw5ZZexeUccAX/5Czz+eLo8qZlZiSTdHRGNtZbV6yB193XaabBkCfzkJ2VXYma2SQ6IznbwwbDnnuksr91k783MuicHRGeT0l7E3LnpsFczszrlgCjDBz8Iw4b5WhFmVtccEGXo3x8+/nG4/npoaiq7GjOzmhwQZfnEJ9I3rC+8sOxKzMxqckCUZYcdYMoUmDkTnn227GrMzF7DAVGm006DF1+EGTPKrsTM7DUcEGXaZx846CC46CJYu7bsaszMNuKAKNtpp6VvVf/yl2VXYma2EQdE2Y44Anbd1deKMLO644AoW0NDOonfHXfA7NllV2Nm9ioHRD044QTYemtfK8LM6ooDoh5ssw1MmwZXXw0Lq6+pZGZWDgdEvfjkJ2H9+nREk5lZHXBA1Itdd4VjjoHp02HZsrKrMTNzQNSVs8+GF17wWISZ1QUHRD3ZYw94//tTQPj0G2ZWMgdEvTn7bFi50ifxM7PSOSDqzZvfDJMnpy/OrVxZdjVm1oM5IOrRl7+cuph8QSEzK5EDoh695S1w5JHwzW/CkiVlV2NmPZQDol79x3/AqlXwb/9WdiVm1kM5IOrV7rvDxz4Gl1ziy5KaWSkcEPXsq1+Ffv3grLPKrsTMeiAHRD3bYQf4/Ofh2mvhD38ouxoz62EcEPXuC1+AMWPglFNg9eqyqzGzHqTQgJA0QdKDkpoknVFjeT9JV+fld0kandvfLeluSffn24OLrLOuDRiQvjT3wAM+7NXMOlVhASGpAZgOTATGAVMkjatabRqwIiJ2Ay4Azs/tzwBHRMSewFTgR0XV2SUcfjhMmgTnngtPPFF2NWbWQxS5B7Ef0BQRj0bEauAqYHLVOpOBy/P8dcAhkhQRf42IRbl9HjBAUr8Ca61/zSfw+9jHIKLcWsysRygyIEYCT1bcX5Dbaq4TEWuB54ChVeu8H7gnIl4pqM6uYfTo9MW5m2+GSy8tuxoz6wHqepBa0h6kbqeTWlh+oqQ5kuYsXbq0c4srw8knwyGHwGc/C48+WnY1ZtbNFRkQC4GdKu6Pym0115HUGxgMLMv3RwE/Bz4cEY/UeoGIuDQiGiOicfjw4R1cfh3q1Qtmzky3U6fC2rVlV2Rm3ViRATEbGCtpjKS+wLHArKp1ZpEGoQGOAm6JiJC0LXADcEZE3FFgjV3PzjvD//wP/OlP6dTgZmYFKSwg8pjCqcBNwAPANRExT9J5kibl1WYAQyU1AacDzYfCngrsBnxF0r15+qeiau1yjjsOTjwRzj8frr++7GrMrJtSdJMjYhobG2POnDlll9F5Xn4Z3v52eOwxmD0bdtut7IrMrAuSdHdENNZaVteD1LYJ/funU3D06gWHHQbLl5ddkZl1Mw6Irux1r4Nf/ALmz0/Xj3ilZx8JbGYdywHR1R1wAFx2Gdx2G3zoQz6yycw6TO+yC7AO8MEPwlNPpe9H9OkDP/whNDSUXZWZdXEOiO7i9NPT2V7PPBOktFfRp0/ZVZlZF+aA6E7OOCOdp+mss2DZsjSIvdVWZVdlZl2UxyC6mzPPhO9/P52z6eCDYfHisisysy7KAdEdffSj6eimefNg333h9tvLrsjMuiAHRHd1xBFw112w9dbwL/8C3/gGrFtXdlVm1oU4ILqzN70pfct68mT44hfhoIPg4YfLrsrMuggHRHc3eDBcd1069PVvf4O994avfz2dqsPMbBMcED2BlL5EN28eTJgAX/oSvPGNKTi6ybm4zKzjOSB6kpEj4Wc/g9/9Lh3+evTRsM8+8NOfwvr1ZVdnZnXGAdETHXII/PWvqdtp1So46qg0XjF9Ojz3XNnVmVmdcED0VL17p26nv/8drrwSBg2CU09NexknnpgOjfVehVmP5oDo6RoaYMqUdLTT7NnwgQ/AFVfAgQemq9edfjrceacPkTXrgRwQtkFjI8yYAUuWpL2Kt7wFLr44XZho+PAUHjNnptOLe3DbrNvzFeVs0559Fm66KU2/+c2GU3eMGAHveEcKj3e8A/bcM13EyMy6lE1dUc4BYa0Xkb5LcfvtcMcdaXr88bSsoQHe8AbYa6/0XYu994bdd0/dVD71uFndckBYcRYuTGMU994L990Hc+fCE09sWN63L+y6a7pm9tix6XbMGNhppzQNHlxe7WbmgLBOtmIF3H8/PPQQNDWl03s0NaXppZc2XnebbTaExc47p9tRo2CHHdK0/fZp/KO3z0xvVoRNBYT/6qzjDRmSjoI68MCN2yPSGMbjj8OTT6Y9jcrbu++GpUtf+3xSConm0GgOjubbYcPS8mHD0jRwYOe8T7NuzgFhnUdKg9sjRsD++9de5+WXU7fV00+n6amnNp6efhoefDDNv/JK7ecYMGBDWGxuGjoUttsuPcbMNuKAsPrSvz+87nVp2pSI9K3vp59OV8975pna09Kl8Nhjaf7ZZ1t+vn790p5Pe6YBA1L4mXUzDgjrmiTYdts0tdaaNbB8+WtDZMWK106LF6eTG65YsfnTj/TtuyEsBg9O4yptmbbeOt36GuJWZxwQ1nP06ZPGLLbfvm2PW7cuhUStIKmeVq6E55+HRYvSfPPUmoNB+vffODQGDao9DRzY8rJaU//+3sOxdnFAmG1OQ0Map9huu/Y9fv36dPRWZWDUmp5/fuP7L76YQmfBgjTfPFUfCdYa/fqlrrD+/Tee2ts2YEDac+rTZ8Nt5Xyttsr5Xj6JQ1fggDArWq9e6fTqW22VBui31Pr1aTC/MjQ2Na1aldZvnqrvv/xyGqupblu1Kk1FnLSxV6/Wh0lbgqd5vnfvFOwNDRvPV9/f0vWa53v1SlPzfGvbevWq6727QgNC0gTgu0AD8IOI+M+q5f2AHwJvAZYBH4iI+XnZmcA0YB3wqYi4qchazbqMXr1SN9PAgenw3qKtXfvacFm1ClavTuM6zbetmW/vuqtWpb2q1j5HVyK1L1wq2w47DL71rQ4vrbCAkNQATAfeDSwAZkuaFRF/r1htGrAiInaTdCxwPvABSeOAY4E9gBHA7yS9PiJ8SlGzzta794Y9oK4gIu31rF2bxo/WrWt5flPLWrPe2rXptdavT/crbzu6bVPLdtqpkE1Z5B7EfkBTRDwKIOkqYDJQGRCTgXPy/HXAxZKU26+KiFeAxyQ15ee7s8B6zaw7kDZ0AdkWKXKkaCTwZMX9Bbmt5joRsRZ4Dhjaysci6URJcyTNWVrrG7hmZtZuXfpQgoi4NCIaI6JxeGf0xZqZ9SBFBsRCoLJjbFRuq7mOpN7AYNJgdWsea2ZmBSoyIGYDYyWNkdSXNOg8q2qdWcDUPH8UcEuk08vOAo6V1E/SGGAs8JcCazUzsyqFDVJHxFpJpwI3kQ5znRkR8ySdB8yJiFnADOBHeRB6OSlEyOtdQxrQXguc4iOYzMw6l68HYWbWg23qehBdepDazMyK44AwM7Oauk0Xk6SlwOPtfPgw4JkOLKcIrrFj1HuN9V4fuMaOUi817hIRNb8n0G0CYktImtNSH1y9cI0do95rrPf6wDV2lK5Qo7uYzMysJgeEmZnV5IBILi27gFZwjR2j3iV3Hw8AAAYPSURBVGus9/rANXaUuq/RYxBmZlaT9yDMzKwmB4SZmdXU4wNC0gRJD0pqknRGSTXsJOkPkv4uaZ6kT+f2cyQtlHRvnt5T8Zgzc80PSjq0k+qcL+n+XMuc3LadpN9KejjfDsntknRhrnGupH07ob7dK7bVvZJWSjqt7O0oaaakJZL+VtHW5u0maWpe/2FJU2u9VgfX+E1J/8h1/FzStrl9tKRVFdvzkorHvCX/jjTl99EhF1xuob42/1yL/HtvocarK+qbL+ne3N7p27BdIqLHTqSTCD4C7Ar0Be4DxpVQx47Avnl+a+AhYBzpanufq7H+uFxrP2BMfg8NnVDnfGBYVds3gDPy/BnA+Xn+PcCvAQFvA+4q4Wf7FLBL2dsROBDYF/hbe7cbsB3waL4dkueHFFzjeKB3nj+/osbRletVPc9fct3K72NigfW16eda9N97rRqrln8L+EpZ27A9U0/fg3j1sqgRsRpovixqp4qIxRFxT55/HniAGlfQq/DqJVkj4jGg+ZKsZZgMXJ7nLwfeW9H+w0j+DGwracdOrOsQ4JGI2NS36ztlO0bEbaSzFVe/dlu226HAbyNieUSsAH4LTCiyxoi4OdKVHgH+TLouS4tyndtExJ8j/af7YcX76vD6NqGln2uhf++bqjHvBRwD/GRTz1HkNmyPnh4Qrbq0aWeSNBrYB7grN52ad/FnNndDUF7dAdws6W5JJ+a27SNicZ5/Cti+5BqbHcvGf4z1tB2h7dut7O35EdKn2WZjJP1V0h8lHZDbRua6mnVGjW35uZa5DQ8Ano6Ihyva6mUbtqinB0RdkbQV8FPgtIhYCfwP8DrgzcBi0i5qmd4ZEfsCE4FTJB1YuTB/4in9uGmlC1RNAq7NTfW2HTdSL9utJZK+RLouy49z02Jg54jYBzgduFLSNiWUVtc/1ypT2PgDS71sw03q6QFRN5c2ldSHFA4/joifAUTE0xGxLiLWA99nQ/dHKXVHxMJ8uwT4ea7n6eauo3y7pMwas4nAPRHxdK63rrZj1tbtVkqtko4HDgeOy0FG7rpZlufvJvXrvz7XU9kNVWiN7fi5lrUNewPvA65ubquXbbg5PT0gWnNZ1MLl/skZwAMR8e2K9so++yOB5qMjOv2SrJIGSdq6eZ40gPk3Nr5s7FTglxU1fjgflfM24LmKLpWibfRprZ62Y4W2brebgPGShuSulPG5rTCSJgBfACZFxEsV7cMlNeT5XUnb7dFc50pJb8u/0x+ueF9F1NfWn2tZf+//CvwjIl7tOqqXbbhZZY2O18tEOmrkIVKCf6mkGt5J6mKYC9ybp/cAPwLuz+2zgB0rHvOlXPODdMJRDqQjP+7L07zmbQUMBX4PPAz8DtgutwuYnmu8H2jspG05CFgGDK5oK3U7ksJqMbCG1Kc8rT3bjTQO0JSnEzqhxiZSn33z7+Qled3359+Be4F7gCMqnqeR9I/6EeBi8tkaCqqvzT/XIv/ea9WY2/8XOLlq3U7fhu2ZfKoNMzOrqad3MZmZWQscEGZmVpMDwszManJAmJlZTQ4IMzOryQFh1gaS1mnjM8Z22BlB8xk+/7b5Nc06R++yCzDrYlZFxJvLLsKsM3gPwqwD5HP9fyOfx/8vknbL7aMl3ZJPKPd7STvn9u2VrrFwX57enp+qQdL3la4LcrOkAaW9KevxHBBmbTOgqovpAxXLnouIPUnffv1ObrsIuDwi9iKd7O7C3H4h8MeI2Jt0DYF5uX0sMD0i9gCeJX3j1qwU/ia1WRtIeiEitqrRPh84OCIezSdefCoihkp6hnQKiDW5fXFEDJO0FBgVEa9UPMdo0jUfxub7XwT6RMTXin9nZq/lPQizjhMtzLfFKxXz6/A4oZXIAWHWcT5QcXtnnv8/0llDAY4Dbs/zvwc+DiCpQdLgzirSrLX86cSsbQYoX3g++01ENB/qOkTSXNJewJTc9kngMkmfB5YCJ+T2TwOXSppG2lP4OOlMoGZ1w2MQZh0gj0E0RsQzZddi1lHcxWRmZjV5D8LMzGryHoSZmdXkgDAzs5ocEGZmVpMDwszManJAmJlZTf8fnAurZ666l8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot 1\n",
    "plt.plot(M1epoch_arr,M1gradArr,color=\"Red\")\n",
    "plt.title('Gradient Norm Observation During Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Grad\")\n",
    "plt.savefig(path+'/GradientNorm.pdf',\n",
    "            dpi=700,\n",
    "            bbox_inches='tight')\n",
    "plt.show()\n",
    "#plot 2\n",
    "plt.plot(M1epoch_arr,M1loss_arr,color=\"Red\")\n",
    "plt.title('Loss Observation During Training')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.savefig(path+'/Loss.pdf',\n",
    "            dpi=700,\n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtHYsziyQ8U0"
   },
   "source": [
    "3.What Happened When Gradient is Almost Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVW2X4ypRGsR",
    "outputId": "e08cdd55-8438-42aa-89e8-ada5fd5f9bde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting autograd_lib\n",
      "  Downloading autograd_lib-0.0.7-py3-none-any.whl (9.2 kB)\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-1.9.0-py3-none-any.whl (825 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.8/dist-packages (from autograd_lib) (0.11.2)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.8/dist-packages (from autograd_lib) (0.5.0)\n",
      "Collecting lightning-utilities>=0.4.2\n",
      "  Downloading lightning_utilities-0.6.0.post0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (4.4.0)\n",
      "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (23.0)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (1.13.1+cu116)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (1.21.6)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning->autograd_lib) (4.64.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.8/dist-packages (from seaborn->autograd_lib) (3.2.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.8/dist-packages (from seaborn->autograd_lib) (1.7.3)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.8/dist-packages (from seaborn->autograd_lib) (1.3.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (2.25.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (3.8.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn->autograd_lib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn->autograd_lib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn->autograd_lib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=2.2->seaborn->autograd_lib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.23->seaborn->autograd_lib) (2022.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (22.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn->autograd_lib) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (1.24.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning->autograd_lib) (4.0.0)\n",
      "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, autograd_lib\n",
      "Successfully installed autograd_lib-0.0.7 lightning-utilities-0.6.0.post0 pytorch-lightning-1.9.0 torchmetrics-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autograd_lib\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from autograd_lib import autograd_lib\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TpXy8hjaRGv2"
   },
   "outputs": [],
   "source": [
    "Y_func = lambda x : (torch.sin(5*np.pi*x)) /(5*np.pi*x) \n",
    "num_of_rows = 300\n",
    "X= torch.unsqueeze(torch.linspace(-1,1,num_of_rows),dim=1)\n",
    "Y = Y_func(X)\n",
    "dataset = TensorDataset(X,Y)\n",
    "data_loader = DataLoader(dataset,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XKbsQAH0RG1l"
   },
   "outputs": [],
   "source": [
    "class MathRegressor(nn.Module):\n",
    "    def __init__(self, num_hidden=128):\n",
    "        super().__init__()\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(1, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 # Generate predictions\n",
    "        loss = loss_fn(out, targets)    # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 # Generate predictions\n",
    "        loss = loss_fn(out, targets)    # Calculate loss\n",
    "        return {'val_loss': loss.detach()}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'val_loss': epoch_loss.item()}\n",
    "    \n",
    "    def train_step(self, batch,loss_fn):\n",
    "        inputs, targets = batch \n",
    "        out = self(inputs)                 # Generate predictions\n",
    "        loss = loss_fn(out, targets)    # Calculate loss\n",
    "        return {'train_loss': loss.detach()}\n",
    "    \n",
    "    def train_epoch_end(self, outputs):\n",
    "        batch_losses = [x['train_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        return {'train_loss': epoch_loss.item()}\n",
    "    \n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}\".format(epoch, result['val_loss']))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "V8uiornERG4L"
   },
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model, criterion, train, target):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    output = model(train)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    grads = []\n",
    "    for p in model.regressor.children():\n",
    "        if isinstance(p, nn.Linear):\n",
    "            param_norm = p.weight.grad.norm(2).item()\n",
    "            grads.append(param_norm)\n",
    "\n",
    "    grad_mean = np.mean(grads) \n",
    "\n",
    "    return grad_mean\n",
    "\n",
    "def save_activations(layer, A, _):\n",
    "    activations[layer] = A\n",
    "\n",
    "def compute_hess(layer, _, B):\n",
    "    A = activations[layer]\n",
    "    BA = torch.einsum('nl,ni->nli', B, A) \n",
    "    hess[layer] += torch.einsum('nli,nkj->likj', BA, BA)\n",
    "    \n",
    "def compute_minimum_ratio(model, criterion, train, target):\n",
    "    model.zero_grad()\n",
    "    \n",
    "    with autograd_lib.module_hook(save_activations):\n",
    "        output = model(train)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "    with autograd_lib.module_hook(compute_hess):\n",
    "        autograd_lib.backward_hessian(output, loss='LeastSquares')\n",
    "\n",
    "    layer_hess = list(hess.values())\n",
    "    minimum_ratio = []\n",
    "\n",
    "    for h in layer_hess:\n",
    "        size = h.shape[0] * h.shape[1]\n",
    "        h = h.reshape(size, size)\n",
    "        h_eig = torch.symeig(h).eigenvalues \n",
    "        num_greater = torch.sum(h_eig > 0).item()\n",
    "        minimum_ratio.append(num_greater / len(h_eig))\n",
    "\n",
    "    ratio_mean = np.mean(minimum_ratio) \n",
    "\n",
    "    return ratio_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "bfad0bb3"
   },
   "outputs": [],
   "source": [
    "def get_norm_minimal_ratio(model,criterion):\n",
    "\n",
    "    gradient_norm = compute_gradient_norm(model, criterion, X, Y)\n",
    "    minimum_ratio = compute_minimum_ratio(model, criterion, X, Y)\n",
    "\n",
    "    print('gradient norm: {}, minimum ratio: {}'.format(gradient_norm, minimum_ratio))\n",
    "    result = {}\n",
    "    result[\"grad_norm\"] = gradient_norm\n",
    "    result[\"ratio\"] = minimum_ratio\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "9e38ccc7"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,loss_fn, val_loader):\n",
    "    outputs = [model.validation_step(batch,loss_fn) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def evaluate2(model,loss_fn, train_loader):\n",
    "    outputs = [model.train_step(batch,loss_fn) for batch in train_loader]\n",
    "    return model.train_epoch_end(outputs)\n",
    "\n",
    "def get_grad_norm(model):\n",
    "    grad_all=0.0\n",
    "    grad =0\n",
    "    \n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad = (p.grad.cpu().data.numpy()**2).sum()\n",
    "            \n",
    "        grad_all+=grad\n",
    "        \n",
    "    grad_norm=grad_all ** 0.5\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, data_loader, criterion,opt_func):\n",
    "    history = []\n",
    "    comparing_epoch_loss =1000.0\n",
    "    grad_norm_per_epoch={}\n",
    "    \n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X)\n",
    "        loss = criterion(prediction, Y)\n",
    "        loss.backward()\n",
    "        grad_norm_per_epoch[epoch] = get_norm_minimal_ratio(model,criterion)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        result = evaluate(model,criterion, data_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        if epoch == 900:\n",
    "            comparing_epoch_loss= result[\"val_loss\"]\n",
    "    return history,grad_norm_per_epoch,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "a77cd0b4"
   },
   "outputs": [],
   "source": [
    "num_of_rows = 300\n",
    "lr = 0.0004\n",
    "gamma_lr_scheduler = 0.1 \n",
    "weight_decay = 1e-4\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2500\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "filename = criterion_name+ optimizer_name+\".png\"\n",
    "grad_norm_name = \"_grad_norm_name1_2.png\"\n",
    "result_folder_name = \"result3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1d32ce98"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam\n",
    "num_epochs =2000\n",
    "criterion_name = \"MSE_LOSS_\"\n",
    "optimizer_name = \"ADAM_opt\"\n",
    "input_size=1\n",
    "output_size=1\n",
    "model= MathRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "633cc590"
   },
   "outputs": [],
   "source": [
    "autograd_lib.register(model)\n",
    "activations = defaultdict(int)\n",
    "hess = defaultdict(float)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d077eee",
    "outputId": "4f6cea57-968d-4255-c227-23b47b21fd35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "result_1 = evaluate(model,criterion,data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "b36eb9e3"
   },
   "outputs": [],
   "source": [
    "filename = criterion_name+ optimizer_name+\".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "201af232"
   },
   "outputs": [],
   "source": [
    "train,target = X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ppSl7KXDApra"
   },
   "outputs": [],
   "source": [
    "class M1(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(M1, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(1, 5)   # hidden layer\n",
    "        self.hidden2 = torch.nn.Linear(5, 10)\n",
    "        self.hidden3 = torch.nn.Linear (10, 10)\n",
    "        self.hidden4 = torch.nn.Linear (10, 10)\n",
    "        self.hidden5 = torch.nn.Linear (10, 10)\n",
    "        self.hidden6 = torch.nn.Linear (10, 10)\n",
    "        self.hidden7 = torch.nn.Linear (10,5)\n",
    "        self.predict = torch.nn.Linear(5, 1)   # output layer\n",
    "        #self.activation = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.hidden1(x))      # activation function for hidden layer\n",
    "        x = F.leaky_relu(self.hidden2(x))\n",
    "        x = F.leaky_relu(self.hidden3(x))\n",
    "        x = F.leaky_relu(self.hidden4(x))\n",
    "        x = F.leaky_relu(self.hidden5(x))\n",
    "        x = F.leaky_relu(self.hidden6(x))\n",
    "        x = F.leaky_relu(self.hidden7(x))\n",
    "        x = self.predict(x)                    # linear output\n",
    "        return x\n",
    "\n",
    "model_1 = M1()     # define the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64cb8bdd",
    "outputId": "ad8e5fed-bbba-4ca4-8675-faf0ea5d4d14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-03f6557ee650>:42: UserWarning: torch.symeig is deprecated in favor of torch.linalg.eigh and will be removed in a future PyTorch release.\n",
      "The default behavior has changed from using the upper triangular portion of the matrix by default to using the lower triangular portion.\n",
      "L, _ = torch.symeig(A, upper=upper)\n",
      "should be replaced with\n",
      "L = torch.linalg.eigvalsh(A, UPLO='U' if upper else 'L')\n",
      "and\n",
      "L, V = torch.symeig(A, eigenvectors=True)\n",
      "should be replaced with\n",
      "L, V = torch.linalg.eigh(A, UPLO='U' if upper else 'L') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2794.)\n",
      "  h_eig = torch.symeig(h).eigenvalues\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient norm: 0.1595542598515749, minimum ratio: 0.6953125\n",
      "Epoch [0], val_loss: 0.0885\n",
      "gradient norm: 0.2094506649300456, minimum ratio: 0.72265625\n",
      "Epoch [1], val_loss: 0.0897\n",
      "gradient norm: 0.26820208225399256, minimum ratio: 0.71875\n",
      "Epoch [2], val_loss: 0.0912\n",
      "gradient norm: 0.33107542991638184, minimum ratio: 0.7109375\n",
      "Epoch [3], val_loss: 0.0933\n",
      "gradient norm: 0.3960586152970791, minimum ratio: 0.72265625\n",
      "Epoch [4], val_loss: 0.0958\n",
      "gradient norm: 0.4622521437704563, minimum ratio: 0.7265625\n",
      "Epoch [5], val_loss: 0.0988\n",
      "gradient norm: 0.5292240530252457, minimum ratio: 0.71875\n",
      "Epoch [6], val_loss: 0.1023\n",
      "gradient norm: 0.5967067517340183, minimum ratio: 0.734375\n",
      "Epoch [7], val_loss: 0.1062\n",
      "gradient norm: 0.6645790655165911, minimum ratio: 0.73828125\n",
      "Epoch [8], val_loss: 0.1106\n",
      "gradient norm: 0.7327027712017298, minimum ratio: 0.7265625\n",
      "Epoch [9], val_loss: 0.1154\n",
      "gradient norm: 0.8010115306824446, minimum ratio: 0.74609375\n",
      "Epoch [10], val_loss: 0.1208\n",
      "gradient norm: 0.8694694936275482, minimum ratio: 0.75390625\n",
      "Epoch [11], val_loss: 0.1265\n",
      "gradient norm: 0.9380294010043144, minimum ratio: 0.7421875\n",
      "Epoch [12], val_loss: 0.1328\n",
      "gradient norm: 1.0066642668098211, minimum ratio: 0.72265625\n",
      "Epoch [13], val_loss: 0.1395\n",
      "gradient norm: 1.0753631498664618, minimum ratio: 0.734375\n",
      "Epoch [14], val_loss: 0.1467\n",
      "gradient norm: 1.1441313102841377, minimum ratio: 0.74609375\n",
      "Epoch [15], val_loss: 0.1543\n",
      "gradient norm: 1.2129315882921219, minimum ratio: 0.74609375\n",
      "Epoch [16], val_loss: 0.1624\n",
      "gradient norm: 1.2817705646157265, minimum ratio: 0.72265625\n",
      "Epoch [17], val_loss: 0.1709\n",
      "gradient norm: 1.3506463151425123, minimum ratio: 0.7265625\n",
      "Epoch [18], val_loss: 0.1800\n",
      "gradient norm: 1.419563189148903, minimum ratio: 0.7421875\n",
      "Epoch [19], val_loss: 0.1894\n",
      "gradient norm: 1.4885116927325726, minimum ratio: 0.75390625\n",
      "Epoch [20], val_loss: 0.1994\n",
      "gradient norm: 1.5574947148561478, minimum ratio: 0.73046875\n",
      "Epoch [21], val_loss: 0.2098\n",
      "gradient norm: 1.6264960542321205, minimum ratio: 0.7578125\n",
      "Epoch [22], val_loss: 0.2207\n",
      "gradient norm: 1.6955148689448833, minimum ratio: 0.73828125\n",
      "Epoch [23], val_loss: 0.2320\n",
      "gradient norm: 1.7645537219941616, minimum ratio: 0.734375\n",
      "Epoch [24], val_loss: 0.2438\n",
      "gradient norm: 1.8336182422935963, minimum ratio: 0.7421875\n",
      "Epoch [25], val_loss: 0.2561\n",
      "gradient norm: 1.902698252350092, minimum ratio: 0.74609375\n",
      "Epoch [26], val_loss: 0.2688\n",
      "gradient norm: 1.9718145467340946, minimum ratio: 0.734375\n",
      "Epoch [27], val_loss: 0.2820\n",
      "gradient norm: 2.040947999805212, minimum ratio: 0.765625\n",
      "Epoch [28], val_loss: 0.2956\n",
      "gradient norm: 2.110126093029976, minimum ratio: 0.75390625\n",
      "Epoch [29], val_loss: 0.3097\n",
      "gradient norm: 2.1793279498815536, minimum ratio: 0.76171875\n",
      "Epoch [30], val_loss: 0.3243\n",
      "gradient norm: 2.2485699616372585, minimum ratio: 0.7578125\n",
      "Epoch [31], val_loss: 0.3393\n",
      "gradient norm: 2.317864004522562, minimum ratio: 0.7421875\n",
      "Epoch [32], val_loss: 0.3549\n",
      "gradient norm: 2.3871997334063053, minimum ratio: 0.74609375\n",
      "Epoch [33], val_loss: 0.3708\n",
      "gradient norm: 2.4565730802714825, minimum ratio: 0.75390625\n",
      "Epoch [34], val_loss: 0.3873\n",
      "gradient norm: 2.5260083004832268, minimum ratio: 0.76953125\n",
      "Epoch [35], val_loss: 0.4042\n",
      "gradient norm: 2.5954735167324543, minimum ratio: 0.75\n",
      "Epoch [36], val_loss: 0.4215\n",
      "gradient norm: 2.6649860218167305, minimum ratio: 0.75\n",
      "Epoch [37], val_loss: 0.4394\n",
      "gradient norm: 2.7345538921654224, minimum ratio: 0.75390625\n",
      "Epoch [38], val_loss: 0.4577\n",
      "gradient norm: 2.8041690476238728, minimum ratio: 0.73828125\n",
      "Epoch [39], val_loss: 0.4765\n",
      "gradient norm: 2.8738328889012337, minimum ratio: 0.7421875\n",
      "Epoch [40], val_loss: 0.4957\n",
      "gradient norm: 2.943562127649784, minimum ratio: 0.78125\n",
      "Epoch [41], val_loss: 0.5154\n",
      "gradient norm: 3.013341136276722, minimum ratio: 0.74609375\n",
      "Epoch [42], val_loss: 0.5356\n",
      "gradient norm: 3.083170846104622, minimum ratio: 0.7421875\n",
      "Epoch [43], val_loss: 0.5563\n",
      "gradient norm: 3.153075151145458, minimum ratio: 0.73046875\n",
      "Epoch [44], val_loss: 0.5774\n",
      "gradient norm: 3.2230469062924385, minimum ratio: 0.7578125\n",
      "Epoch [45], val_loss: 0.5990\n",
      "gradient norm: 3.2931126430630684, minimum ratio: 0.7421875\n",
      "Epoch [46], val_loss: 0.6211\n",
      "gradient norm: 3.3632274121046066, minimum ratio: 0.75390625\n",
      "Epoch [47], val_loss: 0.6436\n",
      "gradient norm: 3.433441236615181, minimum ratio: 0.74609375\n",
      "Epoch [48], val_loss: 0.6666\n",
      "gradient norm: 3.5037543326616287, minimum ratio: 0.75\n",
      "Epoch [49], val_loss: 0.6902\n",
      "gradient norm: 3.5741244927048683, minimum ratio: 0.76953125\n",
      "Epoch [50], val_loss: 0.7142\n",
      "gradient norm: 3.6446010023355484, minimum ratio: 0.75390625\n",
      "Epoch [51], val_loss: 0.7386\n",
      "gradient norm: 3.715147949755192, minimum ratio: 0.76171875\n",
      "Epoch [52], val_loss: 0.7636\n",
      "gradient norm: 3.7857967615127563, minimum ratio: 0.77734375\n",
      "Epoch [53], val_loss: 0.7890\n",
      "gradient norm: 3.8565235286951065, minimum ratio: 0.74609375\n",
      "Epoch [54], val_loss: 0.8149\n",
      "gradient norm: 3.927388660609722, minimum ratio: 0.73828125\n",
      "Epoch [55], val_loss: 0.8413\n",
      "gradient norm: 3.9983451887965202, minimum ratio: 0.7578125\n",
      "Epoch [56], val_loss: 0.8682\n",
      "gradient norm: 4.069390468299389, minimum ratio: 0.75\n",
      "Epoch [57], val_loss: 0.8956\n",
      "gradient norm: 4.140541560947895, minimum ratio: 0.7578125\n",
      "Epoch [58], val_loss: 0.9234\n",
      "gradient norm: 4.211826652288437, minimum ratio: 0.75390625\n",
      "Epoch [59], val_loss: 0.9518\n",
      "gradient norm: 4.283198304474354, minimum ratio: 0.7578125\n",
      "Epoch [60], val_loss: 0.9806\n",
      "gradient norm: 4.354689441621304, minimum ratio: 0.734375\n",
      "Epoch [61], val_loss: 1.0100\n",
      "gradient norm: 4.4263667315244675, minimum ratio: 0.765625\n",
      "Epoch [62], val_loss: 1.0398\n",
      "gradient norm: 4.498207218945026, minimum ratio: 0.7578125\n",
      "Epoch [63], val_loss: 1.0702\n",
      "gradient norm: 4.570139117538929, minimum ratio: 0.7578125\n",
      "Epoch [64], val_loss: 1.1011\n",
      "gradient norm: 4.642175495624542, minimum ratio: 0.76171875\n",
      "Epoch [65], val_loss: 1.1324\n",
      "gradient norm: 4.714385360479355, minimum ratio: 0.76171875\n",
      "Epoch [66], val_loss: 1.1643\n",
      "gradient norm: 4.786804534494877, minimum ratio: 0.76171875\n",
      "Epoch [67], val_loss: 1.1967\n",
      "gradient norm: 4.859292410314083, minimum ratio: 0.7578125\n",
      "Epoch [68], val_loss: 1.2295\n",
      "gradient norm: 4.931917458772659, minimum ratio: 0.7578125\n",
      "Epoch [69], val_loss: 1.2629\n",
      "gradient norm: 5.004721410572529, minimum ratio: 0.7578125\n",
      "Epoch [70], val_loss: 1.2969\n",
      "gradient norm: 5.077643357217312, minimum ratio: 0.7578125\n",
      "Epoch [71], val_loss: 1.3313\n",
      "gradient norm: 5.150816932320595, minimum ratio: 0.76953125\n",
      "Epoch [72], val_loss: 1.3663\n",
      "gradient norm: 5.224122770130634, minimum ratio: 0.74609375\n",
      "Epoch [73], val_loss: 1.4017\n",
      "gradient norm: 5.297606125473976, minimum ratio: 0.7421875\n",
      "Epoch [74], val_loss: 1.4377\n",
      "gradient norm: 5.37124228477478, minimum ratio: 0.7578125\n",
      "Epoch [75], val_loss: 1.4743\n",
      "gradient norm: 5.445077687501907, minimum ratio: 0.76953125\n",
      "Epoch [76], val_loss: 1.5113\n",
      "gradient norm: 5.519049882888794, minimum ratio: 0.765625\n",
      "Epoch [77], val_loss: 1.5489\n",
      "gradient norm: 5.593173161149025, minimum ratio: 0.78125\n",
      "Epoch [78], val_loss: 1.5871\n",
      "gradient norm: 5.667496636509895, minimum ratio: 0.7734375\n",
      "Epoch [79], val_loss: 1.6257\n",
      "gradient norm: 5.742038577795029, minimum ratio: 0.76171875\n",
      "Epoch [80], val_loss: 1.6649\n",
      "gradient norm: 5.816716060042381, minimum ratio: 0.7734375\n",
      "Epoch [81], val_loss: 1.7047\n",
      "gradient norm: 5.891599953174591, minimum ratio: 0.7578125\n",
      "Epoch [82], val_loss: 1.7450\n",
      "gradient norm: 5.966644749045372, minimum ratio: 0.76953125\n",
      "Epoch [83], val_loss: 1.7858\n",
      "gradient norm: 6.0419289618730545, minimum ratio: 0.76171875\n",
      "Epoch [84], val_loss: 1.8272\n",
      "gradient norm: 6.11739182472229, minimum ratio: 0.7578125\n",
      "Epoch [85], val_loss: 1.8691\n",
      "gradient norm: 6.193044945597649, minimum ratio: 0.78125\n",
      "Epoch [86], val_loss: 1.9116\n",
      "gradient norm: 6.268871992826462, minimum ratio: 0.76953125\n",
      "Epoch [87], val_loss: 1.9547\n",
      "gradient norm: 6.345002084970474, minimum ratio: 0.76171875\n",
      "Epoch [88], val_loss: 1.9983\n",
      "gradient norm: 6.421279281377792, minimum ratio: 0.765625\n",
      "Epoch [89], val_loss: 2.0425\n",
      "gradient norm: 6.497784644365311, minimum ratio: 0.75390625\n",
      "Epoch [90], val_loss: 2.0873\n",
      "gradient norm: 6.574507340788841, minimum ratio: 0.76171875\n",
      "Epoch [91], val_loss: 2.1326\n",
      "gradient norm: 6.6515453308820724, minimum ratio: 0.7734375\n",
      "Epoch [92], val_loss: 2.1785\n",
      "gradient norm: 6.728732347488403, minimum ratio: 0.7890625\n",
      "Epoch [93], val_loss: 2.2250\n",
      "gradient norm: 6.806164249777794, minimum ratio: 0.7578125\n",
      "Epoch [94], val_loss: 2.2722\n",
      "gradient norm: 6.883824184536934, minimum ratio: 0.7578125\n",
      "Epoch [95], val_loss: 2.3198\n",
      "gradient norm: 6.961805611848831, minimum ratio: 0.75390625\n",
      "Epoch [96], val_loss: 2.3681\n",
      "gradient norm: 7.039968430995941, minimum ratio: 0.76953125\n",
      "Epoch [97], val_loss: 2.4170\n",
      "gradient norm: 7.118344575166702, minimum ratio: 0.76953125\n",
      "Epoch [98], val_loss: 2.4665\n",
      "gradient norm: 7.1968682408332825, minimum ratio: 0.7578125\n",
      "Epoch [99], val_loss: 2.5165\n",
      "gradient norm: 7.275738641619682, minimum ratio: 0.7578125\n",
      "Epoch [100], val_loss: 2.5672\n",
      "gradient norm: 7.354835346341133, minimum ratio: 0.765625\n",
      "Epoch [101], val_loss: 2.6185\n",
      "gradient norm: 7.4341786950826645, minimum ratio: 0.77734375\n",
      "Epoch [102], val_loss: 2.6703\n",
      "gradient norm: 7.513653665781021, minimum ratio: 0.7734375\n",
      "Epoch [103], val_loss: 2.7228\n",
      "gradient norm: 7.593543991446495, minimum ratio: 0.76171875\n",
      "Epoch [104], val_loss: 2.7760\n",
      "gradient norm: 7.673623815178871, minimum ratio: 0.75390625\n",
      "Epoch [105], val_loss: 2.8297\n",
      "gradient norm: 7.754004195332527, minimum ratio: 0.77734375\n",
      "Epoch [106], val_loss: 2.8841\n",
      "gradient norm: 7.8346433490514755, minimum ratio: 0.77734375\n",
      "Epoch [107], val_loss: 2.9391\n",
      "gradient norm: 7.915520042181015, minimum ratio: 0.75\n",
      "Epoch [108], val_loss: 2.9947\n",
      "gradient norm: 7.996672257781029, minimum ratio: 0.78125\n",
      "Epoch [109], val_loss: 3.0510\n",
      "gradient norm: 8.078104808926582, minimum ratio: 0.78125\n",
      "Epoch [110], val_loss: 3.1079\n",
      "gradient norm: 8.159812152385712, minimum ratio: 0.76953125\n",
      "Epoch [111], val_loss: 3.1655\n",
      "gradient norm: 8.241850912570953, minimum ratio: 0.77734375\n",
      "Epoch [112], val_loss: 3.2237\n",
      "gradient norm: 8.324113488197327, minimum ratio: 0.7890625\n",
      "Epoch [113], val_loss: 3.2826\n",
      "gradient norm: 8.40665477514267, minimum ratio: 0.76171875\n",
      "Epoch [114], val_loss: 3.3421\n",
      "gradient norm: 8.48942843079567, minimum ratio: 0.7421875\n",
      "Epoch [115], val_loss: 3.4023\n",
      "gradient norm: 8.57265517115593, minimum ratio: 0.77734375\n",
      "Epoch [116], val_loss: 3.4631\n",
      "gradient norm: 8.656071946024895, minimum ratio: 0.76171875\n",
      "Epoch [117], val_loss: 3.5247\n",
      "gradient norm: 8.739738211035728, minimum ratio: 0.7578125\n",
      "Epoch [118], val_loss: 3.5869\n",
      "gradient norm: 8.823742970824242, minimum ratio: 0.75\n",
      "Epoch [119], val_loss: 3.6498\n",
      "gradient norm: 8.908215299248695, minimum ratio: 0.7890625\n",
      "Epoch [120], val_loss: 3.7134\n",
      "gradient norm: 8.99287286400795, minimum ratio: 0.77734375\n",
      "Epoch [121], val_loss: 3.7777\n",
      "gradient norm: 9.077823847532272, minimum ratio: 0.76953125\n",
      "Epoch [122], val_loss: 3.8427\n",
      "gradient norm: 9.163044422864914, minimum ratio: 0.76171875\n",
      "Epoch [123], val_loss: 3.9083\n",
      "gradient norm: 9.248678624629974, minimum ratio: 0.7578125\n",
      "Epoch [124], val_loss: 3.9747\n",
      "gradient norm: 9.334556698799133, minimum ratio: 0.76953125\n",
      "Epoch [125], val_loss: 4.0418\n",
      "gradient norm: 9.420774042606354, minimum ratio: 0.7578125\n",
      "Epoch [126], val_loss: 4.1095\n",
      "gradient norm: 9.507086127996445, minimum ratio: 0.76953125\n",
      "Epoch [127], val_loss: 4.1780\n",
      "gradient norm: 9.59396880865097, minimum ratio: 0.77734375\n",
      "Epoch [128], val_loss: 4.2472\n",
      "gradient norm: 9.681065499782562, minimum ratio: 0.796875\n",
      "Epoch [129], val_loss: 4.3172\n",
      "gradient norm: 9.768432199954987, minimum ratio: 0.75390625\n",
      "Epoch [130], val_loss: 4.3878\n",
      "gradient norm: 9.856180846691132, minimum ratio: 0.77734375\n",
      "Epoch [131], val_loss: 4.4592\n",
      "gradient norm: 9.944322139024734, minimum ratio: 0.77734375\n",
      "Epoch [132], val_loss: 4.5313\n",
      "gradient norm: 10.032695472240448, minimum ratio: 0.78515625\n",
      "Epoch [133], val_loss: 4.6042\n",
      "gradient norm: 10.121411710977554, minimum ratio: 0.76953125\n",
      "Epoch [134], val_loss: 4.6778\n",
      "gradient norm: 10.210413455963135, minimum ratio: 0.78125\n",
      "Epoch [135], val_loss: 4.7521\n",
      "gradient norm: 10.299745827913284, minimum ratio: 0.76171875\n",
      "Epoch [136], val_loss: 4.8272\n",
      "gradient norm: 10.389426171779633, minimum ratio: 0.7578125\n",
      "Epoch [137], val_loss: 4.9031\n",
      "gradient norm: 10.479498714208603, minimum ratio: 0.76171875\n",
      "Epoch [138], val_loss: 4.9797\n",
      "gradient norm: 10.569867074489594, minimum ratio: 0.7734375\n",
      "Epoch [139], val_loss: 5.0571\n",
      "gradient norm: 10.660697907209396, minimum ratio: 0.7734375\n",
      "Epoch [140], val_loss: 5.1353\n",
      "gradient norm: 10.751773566007614, minimum ratio: 0.765625\n",
      "Epoch [141], val_loss: 5.2142\n",
      "gradient norm: 10.84323900938034, minimum ratio: 0.77734375\n",
      "Epoch [142], val_loss: 5.2940\n",
      "gradient norm: 10.934924334287643, minimum ratio: 0.796875\n",
      "Epoch [143], val_loss: 5.3745\n",
      "gradient norm: 11.027116864919662, minimum ratio: 0.76953125\n",
      "Epoch [144], val_loss: 5.4558\n",
      "gradient norm: 11.11952629685402, minimum ratio: 0.7578125\n",
      "Epoch [145], val_loss: 5.5379\n",
      "gradient norm: 11.212225914001465, minimum ratio: 0.76171875\n",
      "Epoch [146], val_loss: 5.6208\n",
      "gradient norm: 11.305315881967545, minimum ratio: 0.765625\n",
      "Epoch [147], val_loss: 5.7045\n",
      "gradient norm: 11.398839294910431, minimum ratio: 0.76171875\n",
      "Epoch [148], val_loss: 5.7890\n",
      "gradient norm: 11.492722243070602, minimum ratio: 0.77734375\n",
      "Epoch [149], val_loss: 5.8743\n",
      "gradient norm: 11.58699581027031, minimum ratio: 0.76953125\n",
      "Epoch [150], val_loss: 5.9605\n",
      "gradient norm: 11.681494921445847, minimum ratio: 0.77734375\n",
      "Epoch [151], val_loss: 6.0476\n",
      "gradient norm: 11.776516914367676, minimum ratio: 0.765625\n",
      "Epoch [152], val_loss: 6.1354\n",
      "gradient norm: 11.872004240751266, minimum ratio: 0.77734375\n",
      "Epoch [153], val_loss: 6.2241\n",
      "gradient norm: 11.96747624874115, minimum ratio: 0.78515625\n",
      "Epoch [154], val_loss: 6.3137\n",
      "gradient norm: 12.063512951135635, minimum ratio: 0.79296875\n",
      "Epoch [155], val_loss: 6.4041\n",
      "gradient norm: 12.160088211297989, minimum ratio: 0.7734375\n",
      "Epoch [156], val_loss: 6.4954\n",
      "gradient norm: 12.256972879171371, minimum ratio: 0.78515625\n",
      "Epoch [157], val_loss: 6.5875\n",
      "gradient norm: 12.354355543851852, minimum ratio: 0.74609375\n",
      "Epoch [158], val_loss: 6.6805\n",
      "gradient norm: 12.451938480138779, minimum ratio: 0.78515625\n",
      "Epoch [159], val_loss: 6.7744\n",
      "gradient norm: 12.550048440694809, minimum ratio: 0.76953125\n",
      "Epoch [160], val_loss: 6.8691\n",
      "gradient norm: 12.648294001817703, minimum ratio: 0.7734375\n",
      "Epoch [161], val_loss: 6.9648\n",
      "gradient norm: 12.747056871652603, minimum ratio: 0.75390625\n",
      "Epoch [162], val_loss: 7.0613\n",
      "gradient norm: 12.846123069524765, minimum ratio: 0.78125\n",
      "Epoch [163], val_loss: 7.1587\n",
      "gradient norm: 12.94593670964241, minimum ratio: 0.77734375\n",
      "Epoch [164], val_loss: 7.2571\n",
      "gradient norm: 13.04587498307228, minimum ratio: 0.765625\n",
      "Epoch [165], val_loss: 7.3563\n",
      "gradient norm: 13.14609882235527, minimum ratio: 0.79296875\n",
      "Epoch [166], val_loss: 7.4565\n",
      "gradient norm: 13.24698743224144, minimum ratio: 0.7734375\n",
      "Epoch [167], val_loss: 7.5576\n",
      "gradient norm: 13.348338305950165, minimum ratio: 0.79296875\n",
      "Epoch [168], val_loss: 7.6596\n",
      "gradient norm: 13.449901819229126, minimum ratio: 0.78125\n",
      "Epoch [169], val_loss: 7.7626\n",
      "gradient norm: 13.552005767822266, minimum ratio: 0.75390625\n",
      "Epoch [170], val_loss: 7.8665\n",
      "gradient norm: 13.65447935461998, minimum ratio: 0.765625\n",
      "Epoch [171], val_loss: 7.9714\n",
      "gradient norm: 13.757465094327927, minimum ratio: 0.765625\n",
      "Epoch [172], val_loss: 8.0773\n",
      "gradient norm: 13.86077681183815, minimum ratio: 0.76953125\n",
      "Epoch [173], val_loss: 8.1841\n",
      "gradient norm: 13.96444508433342, minimum ratio: 0.78515625\n",
      "Epoch [174], val_loss: 8.2918\n",
      "gradient norm: 14.068779796361923, minimum ratio: 0.78515625\n",
      "Epoch [175], val_loss: 8.4006\n",
      "gradient norm: 14.173608273267746, minimum ratio: 0.77734375\n",
      "Epoch [176], val_loss: 8.5103\n",
      "gradient norm: 14.27859702706337, minimum ratio: 0.78125\n",
      "Epoch [177], val_loss: 8.6210\n",
      "gradient norm: 14.384081810712814, minimum ratio: 0.80078125\n",
      "Epoch [178], val_loss: 8.7327\n",
      "gradient norm: 14.489714711904526, minimum ratio: 0.80078125\n",
      "Epoch [179], val_loss: 8.8454\n",
      "gradient norm: 14.596275985240936, minimum ratio: 0.77734375\n",
      "Epoch [180], val_loss: 8.9591\n",
      "gradient norm: 14.702959775924683, minimum ratio: 0.7734375\n",
      "Epoch [181], val_loss: 9.0738\n",
      "gradient norm: 14.809945344924927, minimum ratio: 0.76953125\n",
      "Epoch [182], val_loss: 9.1895\n",
      "gradient norm: 14.917517006397247, minimum ratio: 0.77734375\n",
      "Epoch [183], val_loss: 9.3062\n",
      "gradient norm: 15.02567046880722, minimum ratio: 0.78515625\n",
      "Epoch [184], val_loss: 9.4240\n",
      "gradient norm: 15.134279906749725, minimum ratio: 0.76953125\n",
      "Epoch [185], val_loss: 9.5429\n",
      "gradient norm: 15.243325412273407, minimum ratio: 0.7734375\n",
      "Epoch [186], val_loss: 9.6628\n",
      "gradient norm: 15.352776765823364, minimum ratio: 0.7890625\n",
      "Epoch [187], val_loss: 9.7838\n",
      "gradient norm: 15.4631068110466, minimum ratio: 0.76953125\n",
      "Epoch [188], val_loss: 9.9058\n",
      "gradient norm: 15.573565125465393, minimum ratio: 0.77734375\n",
      "Epoch [189], val_loss: 10.0289\n",
      "gradient norm: 15.68432754278183, minimum ratio: 0.80859375\n",
      "Epoch [190], val_loss: 10.1531\n",
      "gradient norm: 15.795458197593689, minimum ratio: 0.77734375\n",
      "Epoch [191], val_loss: 10.2783\n",
      "gradient norm: 15.907231450080872, minimum ratio: 0.77734375\n",
      "Epoch [192], val_loss: 10.4046\n",
      "gradient norm: 16.019344449043274, minimum ratio: 0.7890625\n",
      "Epoch [193], val_loss: 10.5320\n",
      "gradient norm: 16.132106363773346, minimum ratio: 0.78515625\n",
      "Epoch [194], val_loss: 10.6606\n",
      "gradient norm: 16.245247185230255, minimum ratio: 0.7890625\n",
      "Epoch [195], val_loss: 10.7902\n",
      "gradient norm: 16.359030842781067, minimum ratio: 0.765625\n",
      "Epoch [196], val_loss: 10.9209\n",
      "gradient norm: 16.47320795059204, minimum ratio: 0.765625\n",
      "Epoch [197], val_loss: 11.0528\n",
      "gradient norm: 16.587705850601196, minimum ratio: 0.7890625\n",
      "Epoch [198], val_loss: 11.1858\n",
      "gradient norm: 16.702760457992554, minimum ratio: 0.7734375\n",
      "Epoch [199], val_loss: 11.3200\n",
      "gradient norm: 16.81817764043808, minimum ratio: 0.7890625\n",
      "Epoch [200], val_loss: 11.4553\n",
      "gradient norm: 16.9343404173851, minimum ratio: 0.7890625\n",
      "Epoch [201], val_loss: 11.5918\n",
      "gradient norm: 17.050927758216858, minimum ratio: 0.7890625\n",
      "Epoch [202], val_loss: 11.7294\n",
      "gradient norm: 17.167871117591858, minimum ratio: 0.78515625\n",
      "Epoch [203], val_loss: 11.8682\n",
      "gradient norm: 17.2851642370224, minimum ratio: 0.80859375\n",
      "Epoch [204], val_loss: 12.0081\n",
      "gradient norm: 17.40317314863205, minimum ratio: 0.78515625\n",
      "Epoch [205], val_loss: 12.1493\n",
      "gradient norm: 17.521783649921417, minimum ratio: 0.7890625\n",
      "Epoch [206], val_loss: 12.2916\n",
      "gradient norm: 17.640821397304535, minimum ratio: 0.78125\n",
      "Epoch [207], val_loss: 12.4352\n",
      "gradient norm: 17.76016789674759, minimum ratio: 0.765625\n",
      "Epoch [208], val_loss: 12.5799\n",
      "gradient norm: 17.88019359111786, minimum ratio: 0.76953125\n",
      "Epoch [209], val_loss: 12.7259\n",
      "gradient norm: 18.00052285194397, minimum ratio: 0.78515625\n",
      "Epoch [210], val_loss: 12.8731\n",
      "gradient norm: 18.121722280979156, minimum ratio: 0.7890625\n",
      "Epoch [211], val_loss: 13.0215\n",
      "gradient norm: 18.243246495723724, minimum ratio: 0.78515625\n",
      "Epoch [212], val_loss: 13.1712\n",
      "gradient norm: 18.365497052669525, minimum ratio: 0.80078125\n",
      "Epoch [213], val_loss: 13.3222\n",
      "gradient norm: 18.488178849220276, minimum ratio: 0.78515625\n",
      "Epoch [214], val_loss: 13.4744\n",
      "gradient norm: 18.61121916770935, minimum ratio: 0.7734375\n",
      "Epoch [215], val_loss: 13.6278\n",
      "gradient norm: 18.73475354909897, minimum ratio: 0.7890625\n",
      "Epoch [216], val_loss: 13.7825\n",
      "gradient norm: 18.859077990055084, minimum ratio: 0.78125\n",
      "Epoch [217], val_loss: 13.9385\n",
      "gradient norm: 18.98362970352173, minimum ratio: 0.80078125\n",
      "Epoch [218], val_loss: 14.0958\n",
      "gradient norm: 19.108748614788055, minimum ratio: 0.78125\n",
      "Epoch [219], val_loss: 14.2544\n",
      "gradient norm: 19.23454123735428, minimum ratio: 0.7734375\n",
      "Epoch [220], val_loss: 14.4142\n",
      "gradient norm: 19.360678136348724, minimum ratio: 0.80078125\n",
      "Epoch [221], val_loss: 14.5754\n",
      "gradient norm: 19.487628281116486, minimum ratio: 0.77734375\n",
      "Epoch [222], val_loss: 14.7379\n",
      "gradient norm: 19.61468207836151, minimum ratio: 0.78125\n",
      "Epoch [223], val_loss: 14.9018\n",
      "gradient norm: 19.742334067821503, minimum ratio: 0.8046875\n",
      "Epoch [224], val_loss: 15.0669\n",
      "gradient norm: 19.87104856967926, minimum ratio: 0.7890625\n",
      "Epoch [225], val_loss: 15.2334\n",
      "gradient norm: 20.000033259391785, minimum ratio: 0.75390625\n",
      "Epoch [226], val_loss: 15.4013\n",
      "gradient norm: 20.12926185131073, minimum ratio: 0.76953125\n",
      "Epoch [227], val_loss: 15.5705\n",
      "gradient norm: 20.258980572223663, minimum ratio: 0.78125\n",
      "Epoch [228], val_loss: 15.7410\n",
      "gradient norm: 20.389650344848633, minimum ratio: 0.765625\n",
      "Epoch [229], val_loss: 15.9130\n",
      "gradient norm: 20.520667254924774, minimum ratio: 0.76953125\n",
      "Epoch [230], val_loss: 16.0864\n",
      "gradient norm: 20.652153968811035, minimum ratio: 0.7890625\n",
      "Epoch [231], val_loss: 16.2612\n",
      "gradient norm: 20.784332871437073, minimum ratio: 0.7890625\n",
      "Epoch [232], val_loss: 16.4374\n",
      "gradient norm: 20.916797041893005, minimum ratio: 0.796875\n",
      "Epoch [233], val_loss: 16.6150\n",
      "gradient norm: 21.05013871192932, minimum ratio: 0.7734375\n",
      "Epoch [234], val_loss: 16.7941\n",
      "gradient norm: 21.184303641319275, minimum ratio: 0.80078125\n",
      "Epoch [235], val_loss: 16.9745\n",
      "gradient norm: 21.318386018276215, minimum ratio: 0.80859375\n",
      "Epoch [236], val_loss: 17.1564\n",
      "gradient norm: 21.453251838684082, minimum ratio: 0.78515625\n",
      "Epoch [237], val_loss: 17.3397\n",
      "gradient norm: 21.588607728481293, minimum ratio: 0.80078125\n",
      "Epoch [238], val_loss: 17.5244\n",
      "gradient norm: 21.72487962245941, minimum ratio: 0.77734375\n",
      "Epoch [239], val_loss: 17.7106\n",
      "gradient norm: 21.861485838890076, minimum ratio: 0.796875\n",
      "Epoch [240], val_loss: 17.8982\n",
      "gradient norm: 21.99835878610611, minimum ratio: 0.78515625\n",
      "Epoch [241], val_loss: 18.0873\n",
      "gradient norm: 22.1361044049263, minimum ratio: 0.78515625\n",
      "Epoch [242], val_loss: 18.2780\n",
      "gradient norm: 22.274724781513214, minimum ratio: 0.79296875\n",
      "Epoch [243], val_loss: 18.4701\n",
      "gradient norm: 22.413540482521057, minimum ratio: 0.78125\n",
      "Epoch [244], val_loss: 18.6637\n",
      "gradient norm: 22.55281662940979, minimum ratio: 0.78515625\n",
      "Epoch [245], val_loss: 18.8588\n",
      "gradient norm: 22.69246244430542, minimum ratio: 0.7890625\n",
      "Epoch [246], val_loss: 19.0555\n",
      "gradient norm: 22.833127737045288, minimum ratio: 0.7734375\n",
      "Epoch [247], val_loss: 19.2537\n",
      "gradient norm: 22.974510729312897, minimum ratio: 0.77734375\n",
      "Epoch [248], val_loss: 19.4534\n",
      "gradient norm: 23.116175651550293, minimum ratio: 0.76953125\n",
      "Epoch [249], val_loss: 19.6547\n",
      "gradient norm: 23.258371829986572, minimum ratio: 0.78515625\n",
      "Epoch [250], val_loss: 19.8575\n",
      "gradient norm: 23.40132212638855, minimum ratio: 0.76953125\n",
      "Epoch [251], val_loss: 20.0619\n",
      "gradient norm: 23.544822692871094, minimum ratio: 0.78125\n",
      "Epoch [252], val_loss: 20.2679\n",
      "gradient norm: 23.689165830612183, minimum ratio: 0.7890625\n",
      "Epoch [253], val_loss: 20.4755\n",
      "gradient norm: 23.833796858787537, minimum ratio: 0.76171875\n",
      "Epoch [254], val_loss: 20.6847\n",
      "gradient norm: 23.978774666786194, minimum ratio: 0.765625\n",
      "Epoch [255], val_loss: 20.8956\n",
      "gradient norm: 24.124648928642273, minimum ratio: 0.7734375\n",
      "Epoch [256], val_loss: 21.1081\n",
      "gradient norm: 24.271281242370605, minimum ratio: 0.8046875\n",
      "Epoch [257], val_loss: 21.3222\n",
      "gradient norm: 24.4185152053833, minimum ratio: 0.78515625\n",
      "Epoch [258], val_loss: 21.5379\n",
      "gradient norm: 24.566241145133972, minimum ratio: 0.78125\n",
      "Epoch [259], val_loss: 21.7554\n",
      "gradient norm: 24.7144877910614, minimum ratio: 0.78515625\n",
      "Epoch [260], val_loss: 21.9744\n",
      "gradient norm: 24.863710284233093, minimum ratio: 0.79296875\n",
      "Epoch [261], val_loss: 22.1953\n",
      "gradient norm: 25.013393998146057, minimum ratio: 0.80078125\n",
      "Epoch [262], val_loss: 22.4176\n",
      "gradient norm: 25.163777351379395, minimum ratio: 0.78515625\n",
      "Epoch [263], val_loss: 22.6417\n",
      "gradient norm: 25.314029693603516, minimum ratio: 0.78125\n",
      "Epoch [264], val_loss: 22.8675\n",
      "gradient norm: 25.465564131736755, minimum ratio: 0.78125\n",
      "Epoch [265], val_loss: 23.0950\n",
      "gradient norm: 25.617995500564575, minimum ratio: 0.7890625\n",
      "Epoch [266], val_loss: 23.3242\n",
      "gradient norm: 25.770911812782288, minimum ratio: 0.77734375\n",
      "Epoch [267], val_loss: 23.5552\n",
      "gradient norm: 25.924214959144592, minimum ratio: 0.80078125\n",
      "Epoch [268], val_loss: 23.7879\n",
      "gradient norm: 26.077919840812683, minimum ratio: 0.8046875\n",
      "Epoch [269], val_loss: 24.0222\n",
      "gradient norm: 26.232766151428223, minimum ratio: 0.8046875\n",
      "Epoch [270], val_loss: 24.2583\n",
      "gradient norm: 26.387635231018066, minimum ratio: 0.80078125\n",
      "Epoch [271], val_loss: 24.4962\n",
      "gradient norm: 26.543240547180176, minimum ratio: 0.79296875\n",
      "Epoch [272], val_loss: 24.7359\n",
      "gradient norm: 26.699783086776733, minimum ratio: 0.78125\n",
      "Epoch [273], val_loss: 24.9775\n",
      "gradient norm: 26.856603026390076, minimum ratio: 0.78125\n",
      "Epoch [274], val_loss: 25.2209\n",
      "gradient norm: 27.014711499214172, minimum ratio: 0.8125\n",
      "Epoch [275], val_loss: 25.4660\n",
      "gradient norm: 27.173380732536316, minimum ratio: 0.796875\n",
      "Epoch [276], val_loss: 25.7131\n",
      "gradient norm: 27.332038402557373, minimum ratio: 0.7890625\n",
      "Epoch [277], val_loss: 25.9619\n",
      "gradient norm: 27.491957306861877, minimum ratio: 0.796875\n",
      "Epoch [278], val_loss: 26.2126\n",
      "gradient norm: 27.65227496623993, minimum ratio: 0.8125\n",
      "Epoch [279], val_loss: 26.4651\n",
      "gradient norm: 27.813512206077576, minimum ratio: 0.8046875\n",
      "Epoch [280], val_loss: 26.7195\n",
      "gradient norm: 27.974791049957275, minimum ratio: 0.78125\n",
      "Epoch [281], val_loss: 26.9758\n",
      "gradient norm: 28.13680398464203, minimum ratio: 0.80078125\n",
      "Epoch [282], val_loss: 27.2339\n",
      "gradient norm: 28.299840331077576, minimum ratio: 0.796875\n",
      "Epoch [283], val_loss: 27.4940\n",
      "gradient norm: 28.464003920555115, minimum ratio: 0.765625\n",
      "Epoch [284], val_loss: 27.7560\n",
      "gradient norm: 28.62835204601288, minimum ratio: 0.78515625\n",
      "Epoch [285], val_loss: 28.0199\n",
      "gradient norm: 28.793337106704712, minimum ratio: 0.78515625\n",
      "Epoch [286], val_loss: 28.2857\n",
      "gradient norm: 28.958956718444824, minimum ratio: 0.79296875\n",
      "Epoch [287], val_loss: 28.5534\n",
      "gradient norm: 29.125217080116272, minimum ratio: 0.80078125\n",
      "Epoch [288], val_loss: 28.8231\n",
      "gradient norm: 29.292521595954895, minimum ratio: 0.76953125\n",
      "Epoch [289], val_loss: 29.0947\n",
      "gradient norm: 29.45994210243225, minimum ratio: 0.77734375\n",
      "Epoch [290], val_loss: 29.3683\n",
      "gradient norm: 29.62786030769348, minimum ratio: 0.78125\n",
      "Epoch [291], val_loss: 29.6438\n",
      "gradient norm: 29.796311736106873, minimum ratio: 0.78125\n",
      "Epoch [292], val_loss: 29.9214\n",
      "gradient norm: 29.965842962265015, minimum ratio: 0.76953125\n",
      "Epoch [293], val_loss: 30.2009\n",
      "gradient norm: 30.13609552383423, minimum ratio: 0.77734375\n",
      "Epoch [294], val_loss: 30.4824\n",
      "gradient norm: 30.306761384010315, minimum ratio: 0.79296875\n",
      "Epoch [295], val_loss: 30.7659\n",
      "gradient norm: 30.47876274585724, minimum ratio: 0.76953125\n",
      "Epoch [296], val_loss: 31.0515\n",
      "gradient norm: 30.65106976032257, minimum ratio: 0.796875\n",
      "Epoch [297], val_loss: 31.3390\n",
      "gradient norm: 30.82368803024292, minimum ratio: 0.79296875\n",
      "Epoch [298], val_loss: 31.6287\n",
      "gradient norm: 30.997902750968933, minimum ratio: 0.796875\n",
      "Epoch [299], val_loss: 31.9204\n",
      "gradient norm: 31.17157483100891, minimum ratio: 0.80078125\n",
      "Epoch [300], val_loss: 32.2142\n",
      "gradient norm: 31.346673011779785, minimum ratio: 0.7734375\n",
      "Epoch [301], val_loss: 32.5101\n",
      "gradient norm: 31.522297739982605, minimum ratio: 0.77734375\n",
      "Epoch [302], val_loss: 32.8081\n",
      "gradient norm: 31.69866693019867, minimum ratio: 0.78125\n",
      "Epoch [303], val_loss: 33.1082\n",
      "gradient norm: 31.875772356987, minimum ratio: 0.79296875\n",
      "Epoch [304], val_loss: 33.4105\n",
      "gradient norm: 32.05377781391144, minimum ratio: 0.765625\n",
      "Epoch [305], val_loss: 33.7149\n",
      "gradient norm: 32.232298493385315, minimum ratio: 0.80078125\n",
      "Epoch [306], val_loss: 34.0214\n",
      "gradient norm: 32.41074204444885, minimum ratio: 0.76953125\n",
      "Epoch [307], val_loss: 34.3300\n",
      "gradient norm: 32.59096705913544, minimum ratio: 0.80078125\n",
      "Epoch [308], val_loss: 34.6408\n",
      "gradient norm: 32.771209478378296, minimum ratio: 0.78515625\n",
      "Epoch [309], val_loss: 34.9538\n",
      "gradient norm: 32.95252573490143, minimum ratio: 0.796875\n",
      "Epoch [310], val_loss: 35.2691\n",
      "gradient norm: 33.1345249414444, minimum ratio: 0.796875\n",
      "Epoch [311], val_loss: 35.5866\n",
      "gradient norm: 33.316978335380554, minimum ratio: 0.796875\n",
      "Epoch [312], val_loss: 35.9064\n",
      "gradient norm: 33.50044822692871, minimum ratio: 0.81640625\n",
      "Epoch [313], val_loss: 36.2284\n",
      "gradient norm: 33.68448293209076, minimum ratio: 0.7734375\n",
      "Epoch [314], val_loss: 36.5526\n",
      "gradient norm: 33.869344830513, minimum ratio: 0.78515625\n",
      "Epoch [315], val_loss: 36.8790\n",
      "gradient norm: 34.055440187454224, minimum ratio: 0.79296875\n",
      "Epoch [316], val_loss: 37.2077\n",
      "gradient norm: 34.24182331562042, minimum ratio: 0.796875\n",
      "Epoch [317], val_loss: 37.5387\n",
      "gradient norm: 34.429022908210754, minimum ratio: 0.77734375\n",
      "Epoch [318], val_loss: 37.8720\n",
      "gradient norm: 34.61562430858612, minimum ratio: 0.7890625\n",
      "Epoch [319], val_loss: 38.2076\n",
      "gradient norm: 34.80438756942749, minimum ratio: 0.77734375\n",
      "Epoch [320], val_loss: 38.5455\n",
      "gradient norm: 34.99372434616089, minimum ratio: 0.80078125\n",
      "Epoch [321], val_loss: 38.8858\n",
      "gradient norm: 35.183679819107056, minimum ratio: 0.8046875\n",
      "Epoch [322], val_loss: 39.2283\n",
      "gradient norm: 35.37485122680664, minimum ratio: 0.78125\n",
      "Epoch [323], val_loss: 39.5733\n",
      "gradient norm: 35.5660719871521, minimum ratio: 0.80078125\n",
      "Epoch [324], val_loss: 39.9206\n",
      "gradient norm: 35.758116722106934, minimum ratio: 0.796875\n",
      "Epoch [325], val_loss: 40.2704\n",
      "gradient norm: 35.951019406318665, minimum ratio: 0.8125\n",
      "Epoch [326], val_loss: 40.6224\n",
      "gradient norm: 36.14460361003876, minimum ratio: 0.7890625\n",
      "Epoch [327], val_loss: 40.9769\n",
      "gradient norm: 36.339051604270935, minimum ratio: 0.8125\n",
      "Epoch [328], val_loss: 41.3337\n",
      "gradient norm: 36.53449988365173, minimum ratio: 0.79296875\n",
      "Epoch [329], val_loss: 41.6930\n",
      "gradient norm: 36.729586243629456, minimum ratio: 0.80078125\n",
      "Epoch [330], val_loss: 42.0547\n",
      "gradient norm: 36.92630231380463, minimum ratio: 0.7890625\n",
      "Epoch [331], val_loss: 42.4189\n",
      "gradient norm: 37.123679876327515, minimum ratio: 0.81640625\n",
      "Epoch [332], val_loss: 42.7854\n",
      "gradient norm: 37.32147812843323, minimum ratio: 0.7734375\n",
      "Epoch [333], val_loss: 43.1545\n",
      "gradient norm: 37.519999623298645, minimum ratio: 0.8046875\n",
      "Epoch [334], val_loss: 43.5261\n",
      "gradient norm: 37.719677805900574, minimum ratio: 0.79296875\n",
      "Epoch [335], val_loss: 43.9002\n",
      "gradient norm: 37.919978857040405, minimum ratio: 0.80078125\n",
      "Epoch [336], val_loss: 44.2768\n",
      "gradient norm: 38.12117350101471, minimum ratio: 0.765625\n",
      "Epoch [337], val_loss: 44.6558\n",
      "gradient norm: 38.323237657547, minimum ratio: 0.796875\n",
      "Epoch [338], val_loss: 45.0374\n",
      "gradient norm: 38.525880336761475, minimum ratio: 0.78125\n",
      "Epoch [339], val_loss: 45.4215\n",
      "gradient norm: 38.72966480255127, minimum ratio: 0.78515625\n",
      "Epoch [340], val_loss: 45.8083\n",
      "gradient norm: 38.93336272239685, minimum ratio: 0.7890625\n",
      "Epoch [341], val_loss: 46.1977\n",
      "gradient norm: 39.13803219795227, minimum ratio: 0.78515625\n",
      "Epoch [342], val_loss: 46.5897\n",
      "gradient norm: 39.34333920478821, minimum ratio: 0.7890625\n",
      "Epoch [343], val_loss: 46.9842\n",
      "gradient norm: 39.55007290840149, minimum ratio: 0.7890625\n",
      "Epoch [344], val_loss: 47.3814\n",
      "gradient norm: 39.75719213485718, minimum ratio: 0.78125\n",
      "Epoch [345], val_loss: 47.7812\n",
      "gradient norm: 39.96527314186096, minimum ratio: 0.79296875\n",
      "Epoch [346], val_loss: 48.1837\n",
      "gradient norm: 40.17442035675049, minimum ratio: 0.79296875\n",
      "Epoch [347], val_loss: 48.5887\n",
      "gradient norm: 40.38296461105347, minimum ratio: 0.76953125\n",
      "Epoch [348], val_loss: 48.9963\n",
      "gradient norm: 40.59327149391174, minimum ratio: 0.7734375\n",
      "Epoch [349], val_loss: 49.4066\n",
      "gradient norm: 40.80462908744812, minimum ratio: 0.78515625\n",
      "Epoch [350], val_loss: 49.8196\n",
      "gradient norm: 41.0167236328125, minimum ratio: 0.765625\n",
      "Epoch [351], val_loss: 50.2354\n",
      "gradient norm: 41.22838830947876, minimum ratio: 0.79296875\n",
      "Epoch [352], val_loss: 50.6538\n",
      "gradient norm: 41.4419584274292, minimum ratio: 0.78515625\n",
      "Epoch [353], val_loss: 51.0751\n",
      "gradient norm: 41.6563880443573, minimum ratio: 0.78125\n",
      "Epoch [354], val_loss: 51.4991\n",
      "gradient norm: 41.87197828292847, minimum ratio: 0.76953125\n",
      "Epoch [355], val_loss: 51.9260\n",
      "gradient norm: 42.08744716644287, minimum ratio: 0.78515625\n",
      "Epoch [356], val_loss: 52.3556\n",
      "gradient norm: 42.304038524627686, minimum ratio: 0.78515625\n",
      "Epoch [357], val_loss: 52.7881\n",
      "gradient norm: 42.5214467048645, minimum ratio: 0.78515625\n",
      "Epoch [358], val_loss: 53.2235\n",
      "gradient norm: 42.739027976989746, minimum ratio: 0.8203125\n",
      "Epoch [359], val_loss: 53.6617\n",
      "gradient norm: 42.958298683166504, minimum ratio: 0.765625\n",
      "Epoch [360], val_loss: 54.1026\n",
      "gradient norm: 43.178309202194214, minimum ratio: 0.80078125\n",
      "Epoch [361], val_loss: 54.5464\n",
      "gradient norm: 43.399495124816895, minimum ratio: 0.7890625\n",
      "Epoch [362], val_loss: 54.9931\n",
      "gradient norm: 43.620479345321655, minimum ratio: 0.7890625\n",
      "Epoch [363], val_loss: 55.4427\n",
      "gradient norm: 43.84312129020691, minimum ratio: 0.78515625\n",
      "Epoch [364], val_loss: 55.8949\n",
      "gradient norm: 44.06620478630066, minimum ratio: 0.78515625\n",
      "Epoch [365], val_loss: 56.3503\n",
      "gradient norm: 44.290284872055054, minimum ratio: 0.8125\n",
      "Epoch [366], val_loss: 56.8086\n",
      "gradient norm: 44.514989376068115, minimum ratio: 0.7890625\n",
      "Epoch [367], val_loss: 57.2699\n",
      "gradient norm: 44.741307973861694, minimum ratio: 0.7890625\n",
      "Epoch [368], val_loss: 57.7342\n",
      "gradient norm: 44.96802639961243, minimum ratio: 0.796875\n",
      "Epoch [369], val_loss: 58.2014\n",
      "gradient norm: 45.195592164993286, minimum ratio: 0.8046875\n",
      "Epoch [370], val_loss: 58.6717\n",
      "gradient norm: 45.42417120933533, minimum ratio: 0.78515625\n",
      "Epoch [371], val_loss: 59.1450\n",
      "gradient norm: 45.65339541435242, minimum ratio: 0.80859375\n",
      "Epoch [372], val_loss: 59.6211\n",
      "gradient norm: 45.88276028633118, minimum ratio: 0.78515625\n",
      "Epoch [373], val_loss: 60.1002\n",
      "gradient norm: 46.11378359794617, minimum ratio: 0.80859375\n",
      "Epoch [374], val_loss: 60.5825\n",
      "gradient norm: 46.344905614852905, minimum ratio: 0.7890625\n",
      "Epoch [375], val_loss: 61.0678\n",
      "gradient norm: 46.57682228088379, minimum ratio: 0.7890625\n",
      "Epoch [376], val_loss: 61.5561\n",
      "gradient norm: 46.80987524986267, minimum ratio: 0.81640625\n",
      "Epoch [377], val_loss: 62.0475\n",
      "gradient norm: 47.04327082633972, minimum ratio: 0.796875\n",
      "Epoch [378], val_loss: 62.5417\n",
      "gradient norm: 47.27831029891968, minimum ratio: 0.7890625\n",
      "Epoch [379], val_loss: 63.0391\n",
      "gradient norm: 47.514487981796265, minimum ratio: 0.8046875\n",
      "Epoch [380], val_loss: 63.5394\n",
      "gradient norm: 47.7497239112854, minimum ratio: 0.81640625\n",
      "Epoch [381], val_loss: 64.0428\n",
      "gradient norm: 47.98708915710449, minimum ratio: 0.81640625\n",
      "Epoch [382], val_loss: 64.5494\n",
      "gradient norm: 48.2252140045166, minimum ratio: 0.80078125\n",
      "Epoch [383], val_loss: 65.0590\n",
      "gradient norm: 48.4641695022583, minimum ratio: 0.796875\n",
      "Epoch [384], val_loss: 65.5718\n",
      "gradient norm: 48.70342803001404, minimum ratio: 0.79296875\n",
      "Epoch [385], val_loss: 66.0878\n",
      "gradient norm: 48.94404053688049, minimum ratio: 0.796875\n",
      "Epoch [386], val_loss: 66.6071\n",
      "gradient norm: 49.18424868583679, minimum ratio: 0.79296875\n",
      "Epoch [387], val_loss: 67.1294\n",
      "gradient norm: 49.426485776901245, minimum ratio: 0.796875\n",
      "Epoch [388], val_loss: 67.6550\n",
      "gradient norm: 49.669496297836304, minimum ratio: 0.80078125\n",
      "Epoch [389], val_loss: 68.1840\n",
      "gradient norm: 49.9136905670166, minimum ratio: 0.78125\n",
      "Epoch [390], val_loss: 68.7163\n",
      "gradient norm: 50.159008264541626, minimum ratio: 0.80078125\n",
      "Epoch [391], val_loss: 69.2518\n",
      "gradient norm: 50.404698848724365, minimum ratio: 0.79296875\n",
      "Epoch [392], val_loss: 69.7908\n",
      "gradient norm: 50.6513888835907, minimum ratio: 0.796875\n",
      "Epoch [393], val_loss: 70.3332\n",
      "gradient norm: 50.89918351173401, minimum ratio: 0.7890625\n",
      "Epoch [394], val_loss: 70.8788\n",
      "gradient norm: 51.146549701690674, minimum ratio: 0.8046875\n",
      "Epoch [395], val_loss: 71.4279\n",
      "gradient norm: 51.39583373069763, minimum ratio: 0.78125\n",
      "Epoch [396], val_loss: 71.9802\n",
      "gradient norm: 51.645753622055054, minimum ratio: 0.796875\n",
      "Epoch [397], val_loss: 72.5359\n",
      "gradient norm: 51.897642374038696, minimum ratio: 0.7734375\n",
      "Epoch [398], val_loss: 73.0949\n",
      "gradient norm: 52.150049686431885, minimum ratio: 0.78125\n",
      "Epoch [399], val_loss: 73.6575\n",
      "gradient norm: 52.402657985687256, minimum ratio: 0.796875\n",
      "Epoch [400], val_loss: 74.2235\n",
      "gradient norm: 52.65685558319092, minimum ratio: 0.78515625\n",
      "Epoch [401], val_loss: 74.7928\n",
      "gradient norm: 52.911887407302856, minimum ratio: 0.79296875\n",
      "Epoch [402], val_loss: 75.3654\n",
      "gradient norm: 53.1671142578125, minimum ratio: 0.8046875\n",
      "Epoch [403], val_loss: 75.9414\n",
      "gradient norm: 53.42296624183655, minimum ratio: 0.796875\n",
      "Epoch [404], val_loss: 76.5209\n",
      "gradient norm: 53.68019104003906, minimum ratio: 0.77734375\n",
      "Epoch [405], val_loss: 77.1039\n",
      "gradient norm: 53.93682146072388, minimum ratio: 0.8046875\n",
      "Epoch [406], val_loss: 77.6902\n",
      "gradient norm: 54.19573211669922, minimum ratio: 0.796875\n",
      "Epoch [407], val_loss: 78.2802\n",
      "gradient norm: 54.4550666809082, minimum ratio: 0.7890625\n",
      "Epoch [408], val_loss: 78.8737\n",
      "gradient norm: 54.71548271179199, minimum ratio: 0.80078125\n",
      "Epoch [409], val_loss: 79.4705\n",
      "gradient norm: 54.977296113967896, minimum ratio: 0.796875\n",
      "Epoch [410], val_loss: 80.0709\n",
      "gradient norm: 55.23910331726074, minimum ratio: 0.78515625\n",
      "Epoch [411], val_loss: 80.6750\n",
      "gradient norm: 55.50237512588501, minimum ratio: 0.78515625\n",
      "Epoch [412], val_loss: 81.2827\n",
      "gradient norm: 55.7657425403595, minimum ratio: 0.796875\n",
      "Epoch [413], val_loss: 81.8939\n",
      "gradient norm: 56.03025770187378, minimum ratio: 0.796875\n",
      "Epoch [414], val_loss: 82.5088\n",
      "gradient norm: 56.29656744003296, minimum ratio: 0.78125\n",
      "Epoch [415], val_loss: 83.1274\n",
      "gradient norm: 56.562758445739746, minimum ratio: 0.80859375\n",
      "Epoch [416], val_loss: 83.7496\n",
      "gradient norm: 56.83084034919739, minimum ratio: 0.8046875\n",
      "Epoch [417], val_loss: 84.3755\n",
      "gradient norm: 57.09891629219055, minimum ratio: 0.79296875\n",
      "Epoch [418], val_loss: 85.0051\n",
      "gradient norm: 57.36914944648743, minimum ratio: 0.80859375\n",
      "Epoch [419], val_loss: 85.6383\n",
      "gradient norm: 57.63912749290466, minimum ratio: 0.78125\n",
      "Epoch [420], val_loss: 86.2752\n",
      "gradient norm: 57.90994906425476, minimum ratio: 0.78125\n",
      "Epoch [421], val_loss: 86.9155\n",
      "gradient norm: 58.18233871459961, minimum ratio: 0.8046875\n",
      "Epoch [422], val_loss: 87.5597\n",
      "gradient norm: 58.45580458641052, minimum ratio: 0.78515625\n",
      "Epoch [423], val_loss: 88.2079\n",
      "gradient norm: 58.72980546951294, minimum ratio: 0.78515625\n",
      "Epoch [424], val_loss: 88.8597\n",
      "gradient norm: 59.00456738471985, minimum ratio: 0.7890625\n",
      "Epoch [425], val_loss: 89.5154\n",
      "gradient norm: 59.28049564361572, minimum ratio: 0.7890625\n",
      "Epoch [426], val_loss: 90.1749\n",
      "gradient norm: 59.55701780319214, minimum ratio: 0.8125\n",
      "Epoch [427], val_loss: 90.8383\n",
      "gradient norm: 59.83396124839783, minimum ratio: 0.78515625\n",
      "Epoch [428], val_loss: 91.5057\n",
      "gradient norm: 60.11321949958801, minimum ratio: 0.79296875\n",
      "Epoch [429], val_loss: 92.1771\n",
      "gradient norm: 60.392802238464355, minimum ratio: 0.78125\n",
      "Epoch [430], val_loss: 92.8524\n",
      "gradient norm: 60.67274284362793, minimum ratio: 0.8046875\n",
      "Epoch [431], val_loss: 93.5314\n",
      "gradient norm: 60.954845666885376, minimum ratio: 0.80078125\n",
      "Epoch [432], val_loss: 94.2143\n",
      "gradient norm: 61.23627328872681, minimum ratio: 0.78515625\n",
      "Epoch [433], val_loss: 94.9011\n",
      "gradient norm: 61.519469022750854, minimum ratio: 0.7734375\n",
      "Epoch [434], val_loss: 95.5920\n",
      "gradient norm: 61.80342102050781, minimum ratio: 0.79296875\n",
      "Epoch [435], val_loss: 96.2868\n",
      "gradient norm: 62.08969759941101, minimum ratio: 0.77734375\n",
      "Epoch [436], val_loss: 96.9855\n",
      "gradient norm: 62.376338720321655, minimum ratio: 0.76171875\n",
      "Epoch [437], val_loss: 97.6883\n",
      "gradient norm: 62.66303253173828, minimum ratio: 0.76953125\n",
      "Epoch [438], val_loss: 98.3951\n",
      "gradient norm: 62.95138430595398, minimum ratio: 0.78515625\n",
      "Epoch [439], val_loss: 99.1058\n",
      "gradient norm: 63.24058771133423, minimum ratio: 0.796875\n",
      "Epoch [440], val_loss: 99.8208\n",
      "gradient norm: 63.53021478652954, minimum ratio: 0.76953125\n",
      "Epoch [441], val_loss: 100.5400\n",
      "gradient norm: 63.82154989242554, minimum ratio: 0.78515625\n",
      "Epoch [442], val_loss: 101.2634\n",
      "gradient norm: 64.11332869529724, minimum ratio: 0.7890625\n",
      "Epoch [443], val_loss: 101.9909\n",
      "gradient norm: 64.40676140785217, minimum ratio: 0.78515625\n",
      "Epoch [444], val_loss: 102.7225\n",
      "gradient norm: 64.70092535018921, minimum ratio: 0.765625\n",
      "Epoch [445], val_loss: 103.4581\n",
      "gradient norm: 64.9945821762085, minimum ratio: 0.8046875\n",
      "Epoch [446], val_loss: 104.1981\n",
      "gradient norm: 65.29071474075317, minimum ratio: 0.8046875\n",
      "Epoch [447], val_loss: 104.9421\n",
      "gradient norm: 65.58737516403198, minimum ratio: 0.78515625\n",
      "Epoch [448], val_loss: 105.6902\n",
      "gradient norm: 65.88523435592651, minimum ratio: 0.7734375\n",
      "Epoch [449], val_loss: 106.4425\n",
      "gradient norm: 66.18385553359985, minimum ratio: 0.79296875\n",
      "Epoch [450], val_loss: 107.1989\n",
      "gradient norm: 66.48347568511963, minimum ratio: 0.78515625\n",
      "Epoch [451], val_loss: 107.9595\n",
      "gradient norm: 66.78359508514404, minimum ratio: 0.8203125\n",
      "Epoch [452], val_loss: 108.7243\n",
      "gradient norm: 67.08518028259277, minimum ratio: 0.8046875\n",
      "Epoch [453], val_loss: 109.4933\n",
      "gradient norm: 67.38820171356201, minimum ratio: 0.78515625\n",
      "Epoch [454], val_loss: 110.2667\n",
      "gradient norm: 67.6920337677002, minimum ratio: 0.796875\n",
      "Epoch [455], val_loss: 111.0443\n",
      "gradient norm: 67.9970293045044, minimum ratio: 0.79296875\n",
      "Epoch [456], val_loss: 111.8265\n",
      "gradient norm: 68.30296421051025, minimum ratio: 0.796875\n",
      "Epoch [457], val_loss: 112.6129\n",
      "gradient norm: 68.60947370529175, minimum ratio: 0.78125\n",
      "Epoch [458], val_loss: 113.4037\n",
      "gradient norm: 68.91696739196777, minimum ratio: 0.8046875\n",
      "Epoch [459], val_loss: 114.1990\n",
      "gradient norm: 69.22545957565308, minimum ratio: 0.80078125\n",
      "Epoch [460], val_loss: 114.9987\n",
      "gradient norm: 69.53527498245239, minimum ratio: 0.796875\n",
      "Epoch [461], val_loss: 115.8029\n",
      "gradient norm: 69.84615564346313, minimum ratio: 0.8125\n",
      "Epoch [462], val_loss: 116.6114\n",
      "gradient norm: 70.15693473815918, minimum ratio: 0.80078125\n",
      "Epoch [463], val_loss: 117.4246\n",
      "gradient norm: 70.46906423568726, minimum ratio: 0.8125\n",
      "Epoch [464], val_loss: 118.2422\n",
      "gradient norm: 70.78300619125366, minimum ratio: 0.796875\n",
      "Epoch [465], val_loss: 119.0643\n",
      "gradient norm: 71.09758472442627, minimum ratio: 0.796875\n",
      "Epoch [466], val_loss: 119.8909\n",
      "gradient norm: 71.4134292602539, minimum ratio: 0.7890625\n",
      "Epoch [467], val_loss: 120.7221\n",
      "gradient norm: 71.7300124168396, minimum ratio: 0.8046875\n",
      "Epoch [468], val_loss: 121.5579\n",
      "gradient norm: 72.04635906219482, minimum ratio: 0.80078125\n",
      "Epoch [469], val_loss: 122.3982\n",
      "gradient norm: 72.36480569839478, minimum ratio: 0.796875\n",
      "Epoch [470], val_loss: 123.2429\n",
      "gradient norm: 72.68477249145508, minimum ratio: 0.7890625\n",
      "Epoch [471], val_loss: 124.0923\n",
      "gradient norm: 73.00512886047363, minimum ratio: 0.7890625\n",
      "Epoch [472], val_loss: 124.9461\n",
      "gradient norm: 73.32743120193481, minimum ratio: 0.79296875\n",
      "Epoch [473], val_loss: 125.8044\n",
      "gradient norm: 73.65093994140625, minimum ratio: 0.80078125\n",
      "Epoch [474], val_loss: 126.6671\n",
      "gradient norm: 73.97405529022217, minimum ratio: 0.796875\n",
      "Epoch [475], val_loss: 127.5343\n",
      "gradient norm: 74.29684734344482, minimum ratio: 0.78515625\n",
      "Epoch [476], val_loss: 128.4062\n",
      "gradient norm: 74.62197971343994, minimum ratio: 0.7890625\n",
      "Epoch [477], val_loss: 129.2827\n",
      "gradient norm: 74.94723129272461, minimum ratio: 0.796875\n",
      "Epoch [478], val_loss: 130.1640\n",
      "gradient norm: 75.27503728866577, minimum ratio: 0.78515625\n",
      "Epoch [479], val_loss: 131.0500\n",
      "gradient norm: 75.60373830795288, minimum ratio: 0.7890625\n",
      "Epoch [480], val_loss: 131.9407\n",
      "gradient norm: 75.9330883026123, minimum ratio: 0.8046875\n",
      "Epoch [481], val_loss: 132.8360\n",
      "gradient norm: 76.26353883743286, minimum ratio: 0.80078125\n",
      "Epoch [482], val_loss: 133.7363\n",
      "gradient norm: 76.59563779830933, minimum ratio: 0.8203125\n",
      "Epoch [483], val_loss: 134.6412\n",
      "gradient norm: 76.92787837982178, minimum ratio: 0.7734375\n",
      "Epoch [484], val_loss: 135.5507\n",
      "gradient norm: 77.26139974594116, minimum ratio: 0.8046875\n",
      "Epoch [485], val_loss: 136.4649\n",
      "gradient norm: 77.59587383270264, minimum ratio: 0.796875\n",
      "Epoch [486], val_loss: 137.3842\n",
      "gradient norm: 77.9293704032898, minimum ratio: 0.796875\n",
      "Epoch [487], val_loss: 138.3082\n",
      "gradient norm: 78.266197681427, minimum ratio: 0.80078125\n",
      "Epoch [488], val_loss: 139.2370\n",
      "gradient norm: 78.60364770889282, minimum ratio: 0.78125\n",
      "Epoch [489], val_loss: 140.1710\n",
      "gradient norm: 78.94192409515381, minimum ratio: 0.7890625\n",
      "Epoch [490], val_loss: 141.1098\n",
      "gradient norm: 79.28240585327148, minimum ratio: 0.8125\n",
      "Epoch [491], val_loss: 142.0538\n",
      "gradient norm: 79.62365388870239, minimum ratio: 0.81640625\n",
      "Epoch [492], val_loss: 143.0027\n",
      "gradient norm: 79.96476984024048, minimum ratio: 0.8046875\n",
      "Epoch [493], val_loss: 143.9564\n",
      "gradient norm: 80.30628776550293, minimum ratio: 0.7890625\n",
      "Epoch [494], val_loss: 144.9150\n",
      "gradient norm: 80.64985513687134, minimum ratio: 0.80859375\n",
      "Epoch [495], val_loss: 145.8785\n",
      "gradient norm: 80.99414682388306, minimum ratio: 0.796875\n",
      "Epoch [496], val_loss: 146.8468\n",
      "gradient norm: 81.33891296386719, minimum ratio: 0.8125\n",
      "Epoch [497], val_loss: 147.8202\n",
      "gradient norm: 81.68627786636353, minimum ratio: 0.78515625\n",
      "Epoch [498], val_loss: 148.7988\n",
      "gradient norm: 82.0340256690979, minimum ratio: 0.78515625\n",
      "Epoch [499], val_loss: 149.7825\n",
      "gradient norm: 82.38363313674927, minimum ratio: 0.8046875\n",
      "Epoch [500], val_loss: 150.7712\n",
      "gradient norm: 82.73314762115479, minimum ratio: 0.8046875\n",
      "Epoch [501], val_loss: 151.7650\n",
      "gradient norm: 83.08523082733154, minimum ratio: 0.80859375\n",
      "Epoch [502], val_loss: 152.7638\n",
      "gradient norm: 83.43777132034302, minimum ratio: 0.79296875\n",
      "Epoch [503], val_loss: 153.7680\n",
      "gradient norm: 83.79150295257568, minimum ratio: 0.80859375\n",
      "Epoch [504], val_loss: 154.7775\n",
      "gradient norm: 84.14541864395142, minimum ratio: 0.81640625\n",
      "Epoch [505], val_loss: 155.7922\n",
      "gradient norm: 84.4995379447937, minimum ratio: 0.765625\n",
      "Epoch [506], val_loss: 156.8119\n",
      "gradient norm: 84.85531425476074, minimum ratio: 0.796875\n",
      "Epoch [507], val_loss: 157.8369\n",
      "gradient norm: 85.21168899536133, minimum ratio: 0.8046875\n",
      "Epoch [508], val_loss: 158.8673\n",
      "gradient norm: 85.5702953338623, minimum ratio: 0.77734375\n",
      "Epoch [509], val_loss: 159.9030\n",
      "gradient norm: 85.93018436431885, minimum ratio: 0.796875\n",
      "Epoch [510], val_loss: 160.9438\n",
      "gradient norm: 86.29103899002075, minimum ratio: 0.76953125\n",
      "Epoch [511], val_loss: 161.9898\n",
      "gradient norm: 86.65367221832275, minimum ratio: 0.7890625\n",
      "Epoch [512], val_loss: 163.0411\n",
      "gradient norm: 87.01628160476685, minimum ratio: 0.7890625\n",
      "Epoch [513], val_loss: 164.0978\n",
      "gradient norm: 87.37852382659912, minimum ratio: 0.78125\n",
      "Epoch [514], val_loss: 165.1599\n",
      "gradient norm: 87.74422550201416, minimum ratio: 0.77734375\n",
      "Epoch [515], val_loss: 166.2274\n",
      "gradient norm: 88.11065149307251, minimum ratio: 0.796875\n",
      "Epoch [516], val_loss: 167.3003\n",
      "gradient norm: 88.47853565216064, minimum ratio: 0.78515625\n",
      "Epoch [517], val_loss: 168.3785\n",
      "gradient norm: 88.84627676010132, minimum ratio: 0.7890625\n",
      "Epoch [518], val_loss: 169.4617\n",
      "gradient norm: 89.21548748016357, minimum ratio: 0.796875\n",
      "Epoch [519], val_loss: 170.5504\n",
      "gradient norm: 89.58581113815308, minimum ratio: 0.7890625\n",
      "Epoch [520], val_loss: 171.6447\n",
      "gradient norm: 89.95655012130737, minimum ratio: 0.7734375\n",
      "Epoch [521], val_loss: 172.7444\n",
      "gradient norm: 90.32955694198608, minimum ratio: 0.7890625\n",
      "Epoch [522], val_loss: 173.8494\n",
      "gradient norm: 90.70185613632202, minimum ratio: 0.78125\n",
      "Epoch [523], val_loss: 174.9603\n",
      "gradient norm: 91.07527542114258, minimum ratio: 0.796875\n",
      "Epoch [524], val_loss: 176.0765\n",
      "gradient norm: 91.4507646560669, minimum ratio: 0.80859375\n",
      "Epoch [525], val_loss: 177.1984\n",
      "gradient norm: 91.82654666900635, minimum ratio: 0.78125\n",
      "Epoch [526], val_loss: 178.3260\n",
      "gradient norm: 92.20462846755981, minimum ratio: 0.80859375\n",
      "Epoch [527], val_loss: 179.4591\n",
      "gradient norm: 92.58323431015015, minimum ratio: 0.77734375\n",
      "Epoch [528], val_loss: 180.5979\n",
      "gradient norm: 92.96259307861328, minimum ratio: 0.76953125\n",
      "Epoch [529], val_loss: 181.7422\n",
      "gradient norm: 93.34493780136108, minimum ratio: 0.78515625\n",
      "Epoch [530], val_loss: 182.8922\n",
      "gradient norm: 93.72719669342041, minimum ratio: 0.7890625\n",
      "Epoch [531], val_loss: 184.0478\n",
      "gradient norm: 94.1113715171814, minimum ratio: 0.8046875\n",
      "Epoch [532], val_loss: 185.2090\n",
      "gradient norm: 94.4954743385315, minimum ratio: 0.76171875\n",
      "Epoch [533], val_loss: 186.3761\n",
      "gradient norm: 94.87906265258789, minimum ratio: 0.7890625\n",
      "Epoch [534], val_loss: 187.5489\n",
      "gradient norm: 95.26595258712769, minimum ratio: 0.78515625\n",
      "Epoch [535], val_loss: 188.7273\n",
      "gradient norm: 95.65416860580444, minimum ratio: 0.8125\n",
      "Epoch [536], val_loss: 189.9114\n",
      "gradient norm: 96.04165697097778, minimum ratio: 0.8046875\n",
      "Epoch [537], val_loss: 191.1014\n",
      "gradient norm: 96.4327392578125, minimum ratio: 0.77734375\n",
      "Epoch [538], val_loss: 192.2971\n",
      "gradient norm: 96.82310724258423, minimum ratio: 0.79296875\n",
      "Epoch [539], val_loss: 193.4987\n",
      "gradient norm: 97.21459579467773, minimum ratio: 0.7890625\n",
      "Epoch [540], val_loss: 194.7059\n",
      "gradient norm: 97.60831880569458, minimum ratio: 0.7890625\n",
      "Epoch [541], val_loss: 195.9191\n",
      "gradient norm: 98.00282907485962, minimum ratio: 0.8046875\n",
      "Epoch [542], val_loss: 197.1384\n",
      "gradient norm: 98.39654541015625, minimum ratio: 0.79296875\n",
      "Epoch [543], val_loss: 198.3636\n",
      "gradient norm: 98.79363489151001, minimum ratio: 0.77734375\n",
      "Epoch [544], val_loss: 199.5948\n",
      "gradient norm: 99.19160175323486, minimum ratio: 0.77734375\n",
      "Epoch [545], val_loss: 200.8316\n",
      "gradient norm: 99.59184980392456, minimum ratio: 0.79296875\n",
      "Epoch [546], val_loss: 202.0745\n",
      "gradient norm: 99.99293088912964, minimum ratio: 0.765625\n",
      "Epoch [547], val_loss: 203.3232\n",
      "gradient norm: 100.39248180389404, minimum ratio: 0.78125\n",
      "Epoch [548], val_loss: 204.5778\n",
      "gradient norm: 100.79537868499756, minimum ratio: 0.78125\n",
      "Epoch [549], val_loss: 205.8382\n",
      "gradient norm: 101.19682836532593, minimum ratio: 0.765625\n",
      "Epoch [550], val_loss: 207.1047\n",
      "gradient norm: 101.60061693191528, minimum ratio: 0.80078125\n",
      "Epoch [551], val_loss: 208.3773\n",
      "gradient norm: 102.00423192977905, minimum ratio: 0.7890625\n",
      "Epoch [552], val_loss: 209.6558\n",
      "gradient norm: 102.41238355636597, minimum ratio: 0.79296875\n",
      "Epoch [553], val_loss: 210.9403\n",
      "gradient norm: 102.82027673721313, minimum ratio: 0.80078125\n",
      "Epoch [554], val_loss: 212.2308\n",
      "gradient norm: 103.22910022735596, minimum ratio: 0.7890625\n",
      "Epoch [555], val_loss: 213.5276\n",
      "gradient norm: 103.63550186157227, minimum ratio: 0.7734375\n",
      "Epoch [556], val_loss: 214.8304\n",
      "gradient norm: 104.04722785949707, minimum ratio: 0.79296875\n",
      "Epoch [557], val_loss: 216.1394\n",
      "gradient norm: 104.45963907241821, minimum ratio: 0.78125\n",
      "Epoch [558], val_loss: 217.4544\n",
      "gradient norm: 104.87307643890381, minimum ratio: 0.7890625\n",
      "Epoch [559], val_loss: 218.7754\n",
      "gradient norm: 105.28829860687256, minimum ratio: 0.7890625\n",
      "Epoch [560], val_loss: 220.1020\n",
      "gradient norm: 105.70346355438232, minimum ratio: 0.7734375\n",
      "Epoch [561], val_loss: 221.4350\n",
      "gradient norm: 106.11804533004761, minimum ratio: 0.77734375\n",
      "Epoch [562], val_loss: 222.7742\n",
      "gradient norm: 106.53458499908447, minimum ratio: 0.78125\n",
      "Epoch [563], val_loss: 224.1195\n",
      "gradient norm: 106.95378303527832, minimum ratio: 0.7890625\n",
      "Epoch [564], val_loss: 225.4712\n",
      "gradient norm: 107.37422704696655, minimum ratio: 0.77734375\n",
      "Epoch [565], val_loss: 226.8293\n",
      "gradient norm: 107.79560947418213, minimum ratio: 0.796875\n",
      "Epoch [566], val_loss: 228.1942\n",
      "gradient norm: 108.21783256530762, minimum ratio: 0.78515625\n",
      "Epoch [567], val_loss: 229.5655\n",
      "gradient norm: 108.6421971321106, minimum ratio: 0.79296875\n",
      "Epoch [568], val_loss: 230.9432\n",
      "gradient norm: 109.06486797332764, minimum ratio: 0.8046875\n",
      "Epoch [569], val_loss: 232.3270\n",
      "gradient norm: 109.49130153656006, minimum ratio: 0.7890625\n",
      "Epoch [570], val_loss: 233.7174\n",
      "gradient norm: 109.91688299179077, minimum ratio: 0.7890625\n",
      "Epoch [571], val_loss: 235.1137\n",
      "gradient norm: 110.34334802627563, minimum ratio: 0.7890625\n",
      "Epoch [572], val_loss: 236.5167\n",
      "gradient norm: 110.76966381072998, minimum ratio: 0.76953125\n",
      "Epoch [573], val_loss: 237.9259\n",
      "gradient norm: 111.20028400421143, minimum ratio: 0.78125\n",
      "Epoch [574], val_loss: 239.3413\n",
      "gradient norm: 111.63281154632568, minimum ratio: 0.78125\n",
      "Epoch [575], val_loss: 240.7633\n",
      "gradient norm: 112.06524085998535, minimum ratio: 0.76953125\n",
      "Epoch [576], val_loss: 242.1917\n",
      "gradient norm: 112.49803733825684, minimum ratio: 0.79296875\n",
      "Epoch [577], val_loss: 243.6269\n",
      "gradient norm: 112.93128395080566, minimum ratio: 0.78125\n",
      "Epoch [578], val_loss: 245.0685\n",
      "gradient norm: 113.36825370788574, minimum ratio: 0.76953125\n",
      "Epoch [579], val_loss: 246.5171\n",
      "gradient norm: 113.80703449249268, minimum ratio: 0.77734375\n",
      "Epoch [580], val_loss: 247.9722\n",
      "gradient norm: 114.24319458007812, minimum ratio: 0.7890625\n",
      "Epoch [581], val_loss: 249.4337\n",
      "gradient norm: 114.68292808532715, minimum ratio: 0.79296875\n",
      "Epoch [582], val_loss: 250.9013\n",
      "gradient norm: 115.12251853942871, minimum ratio: 0.79296875\n",
      "Epoch [583], val_loss: 252.3757\n",
      "gradient norm: 115.56279468536377, minimum ratio: 0.796875\n",
      "Epoch [584], val_loss: 253.8569\n",
      "gradient norm: 116.0052375793457, minimum ratio: 0.78125\n",
      "Epoch [585], val_loss: 255.3449\n",
      "gradient norm: 116.44813060760498, minimum ratio: 0.7890625\n",
      "Epoch [586], val_loss: 256.8398\n",
      "gradient norm: 116.89457511901855, minimum ratio: 0.78515625\n",
      "Epoch [587], val_loss: 258.3409\n",
      "gradient norm: 117.34040355682373, minimum ratio: 0.796875\n",
      "Epoch [588], val_loss: 259.8487\n",
      "gradient norm: 117.78848171234131, minimum ratio: 0.7734375\n",
      "Epoch [589], val_loss: 261.3633\n",
      "gradient norm: 118.2357988357544, minimum ratio: 0.77734375\n",
      "Epoch [590], val_loss: 262.8844\n",
      "gradient norm: 118.68449687957764, minimum ratio: 0.8046875\n",
      "Epoch [591], val_loss: 264.4126\n",
      "gradient norm: 119.13445281982422, minimum ratio: 0.80078125\n",
      "Epoch [592], val_loss: 265.9474\n",
      "gradient norm: 119.58486557006836, minimum ratio: 0.76953125\n",
      "Epoch [593], val_loss: 267.4886\n",
      "gradient norm: 120.03816986083984, minimum ratio: 0.78515625\n",
      "Epoch [594], val_loss: 269.0365\n",
      "gradient norm: 120.49219989776611, minimum ratio: 0.80078125\n",
      "Epoch [595], val_loss: 270.5918\n",
      "gradient norm: 120.94523620605469, minimum ratio: 0.78515625\n",
      "Epoch [596], val_loss: 272.1539\n",
      "gradient norm: 121.40263175964355, minimum ratio: 0.8125\n",
      "Epoch [597], val_loss: 273.7223\n",
      "gradient norm: 121.85889339447021, minimum ratio: 0.796875\n",
      "Epoch [598], val_loss: 275.2979\n",
      "gradient norm: 122.31909465789795, minimum ratio: 0.78125\n",
      "Epoch [599], val_loss: 276.8804\n",
      "gradient norm: 122.7786455154419, minimum ratio: 0.80078125\n",
      "Epoch [600], val_loss: 278.4699\n",
      "gradient norm: 123.23907089233398, minimum ratio: 0.79296875\n",
      "Epoch [601], val_loss: 280.0665\n",
      "gradient norm: 123.69989967346191, minimum ratio: 0.78515625\n",
      "Epoch [602], val_loss: 281.6703\n",
      "gradient norm: 124.16410541534424, minimum ratio: 0.78515625\n",
      "Epoch [603], val_loss: 283.2807\n",
      "gradient norm: 124.62820243835449, minimum ratio: 0.7890625\n",
      "Epoch [604], val_loss: 284.8987\n",
      "gradient norm: 125.09370136260986, minimum ratio: 0.8125\n",
      "Epoch [605], val_loss: 286.5239\n",
      "gradient norm: 125.56095600128174, minimum ratio: 0.79296875\n",
      "Epoch [606], val_loss: 288.1560\n",
      "gradient norm: 126.02993106842041, minimum ratio: 0.796875\n",
      "Epoch [607], val_loss: 289.7948\n",
      "gradient norm: 126.4980297088623, minimum ratio: 0.8046875\n",
      "Epoch [608], val_loss: 291.4408\n",
      "gradient norm: 126.96859645843506, minimum ratio: 0.77734375\n",
      "Epoch [609], val_loss: 293.0942\n",
      "gradient norm: 127.43973445892334, minimum ratio: 0.7890625\n",
      "Epoch [610], val_loss: 294.7549\n",
      "gradient norm: 127.91395854949951, minimum ratio: 0.7734375\n",
      "Epoch [611], val_loss: 296.4228\n",
      "gradient norm: 128.38992977142334, minimum ratio: 0.80859375\n",
      "Epoch [612], val_loss: 298.0980\n",
      "gradient norm: 128.86613273620605, minimum ratio: 0.7890625\n",
      "Epoch [613], val_loss: 299.7805\n",
      "gradient norm: 129.3434066772461, minimum ratio: 0.7890625\n",
      "Epoch [614], val_loss: 301.4700\n",
      "gradient norm: 129.81986904144287, minimum ratio: 0.79296875\n",
      "Epoch [615], val_loss: 303.1670\n",
      "gradient norm: 130.2998275756836, minimum ratio: 0.8046875\n",
      "Epoch [616], val_loss: 304.8714\n",
      "gradient norm: 130.77801895141602, minimum ratio: 0.78515625\n",
      "Epoch [617], val_loss: 306.5830\n",
      "gradient norm: 131.2581386566162, minimum ratio: 0.79296875\n",
      "Epoch [618], val_loss: 308.3023\n",
      "gradient norm: 131.7411642074585, minimum ratio: 0.80078125\n",
      "Epoch [619], val_loss: 310.0291\n",
      "gradient norm: 132.2240514755249, minimum ratio: 0.80078125\n",
      "Epoch [620], val_loss: 311.7626\n",
      "gradient norm: 132.70625400543213, minimum ratio: 0.7734375\n",
      "Epoch [621], val_loss: 313.5038\n",
      "gradient norm: 133.19324684143066, minimum ratio: 0.81640625\n",
      "Epoch [622], val_loss: 315.2520\n",
      "gradient norm: 133.67858695983887, minimum ratio: 0.7890625\n",
      "Epoch [623], val_loss: 317.0071\n",
      "gradient norm: 134.16562461853027, minimum ratio: 0.796875\n",
      "Epoch [624], val_loss: 318.7699\n",
      "gradient norm: 134.65437984466553, minimum ratio: 0.80078125\n",
      "Epoch [625], val_loss: 320.5402\n",
      "gradient norm: 135.14440727233887, minimum ratio: 0.80078125\n",
      "Epoch [626], val_loss: 322.3179\n",
      "gradient norm: 135.6366138458252, minimum ratio: 0.7890625\n",
      "Epoch [627], val_loss: 324.1030\n",
      "gradient norm: 136.1300172805786, minimum ratio: 0.80859375\n",
      "Epoch [628], val_loss: 325.8953\n",
      "gradient norm: 136.6239528656006, minimum ratio: 0.79296875\n",
      "Epoch [629], val_loss: 327.6954\n",
      "gradient norm: 137.12032413482666, minimum ratio: 0.77734375\n",
      "Epoch [630], val_loss: 329.5027\n",
      "gradient norm: 137.61719036102295, minimum ratio: 0.7890625\n",
      "Epoch [631], val_loss: 331.3174\n",
      "gradient norm: 138.11321544647217, minimum ratio: 0.80859375\n",
      "Epoch [632], val_loss: 333.1402\n",
      "gradient norm: 138.61034965515137, minimum ratio: 0.77734375\n",
      "Epoch [633], val_loss: 334.9705\n",
      "gradient norm: 139.1096305847168, minimum ratio: 0.8125\n",
      "Epoch [634], val_loss: 336.8085\n",
      "gradient norm: 139.6123561859131, minimum ratio: 0.78515625\n",
      "Epoch [635], val_loss: 338.6542\n",
      "gradient norm: 140.1151294708252, minimum ratio: 0.80859375\n",
      "Epoch [636], val_loss: 340.5078\n",
      "gradient norm: 140.61916828155518, minimum ratio: 0.7890625\n",
      "Epoch [637], val_loss: 342.3686\n",
      "gradient norm: 141.12052822113037, minimum ratio: 0.796875\n",
      "Epoch [638], val_loss: 344.2372\n",
      "gradient norm: 141.62646293640137, minimum ratio: 0.80078125\n",
      "Epoch [639], val_loss: 346.1136\n",
      "gradient norm: 142.13383197784424, minimum ratio: 0.765625\n",
      "Epoch [640], val_loss: 347.9981\n",
      "gradient norm: 142.64362335205078, minimum ratio: 0.7890625\n",
      "Epoch [641], val_loss: 349.8910\n",
      "gradient norm: 143.1548204421997, minimum ratio: 0.7890625\n",
      "Epoch [642], val_loss: 351.7915\n",
      "gradient norm: 143.66447257995605, minimum ratio: 0.796875\n",
      "Epoch [643], val_loss: 353.7000\n",
      "gradient norm: 144.17649364471436, minimum ratio: 0.7890625\n",
      "Epoch [644], val_loss: 355.6158\n",
      "gradient norm: 144.68971920013428, minimum ratio: 0.78515625\n",
      "Epoch [645], val_loss: 357.5392\n",
      "gradient norm: 145.20233249664307, minimum ratio: 0.8203125\n",
      "Epoch [646], val_loss: 359.4702\n",
      "gradient norm: 145.71825885772705, minimum ratio: 0.80078125\n",
      "Epoch [647], val_loss: 361.4093\n",
      "gradient norm: 146.23435497283936, minimum ratio: 0.78125\n",
      "Epoch [648], val_loss: 363.3562\n",
      "gradient norm: 146.75327682495117, minimum ratio: 0.76171875\n",
      "Epoch [649], val_loss: 365.3112\n",
      "gradient norm: 147.2737216949463, minimum ratio: 0.7890625\n",
      "Epoch [650], val_loss: 367.2738\n",
      "gradient norm: 147.79595279693604, minimum ratio: 0.79296875\n",
      "Epoch [651], val_loss: 369.2446\n",
      "gradient norm: 148.31740283966064, minimum ratio: 0.79296875\n",
      "Epoch [652], val_loss: 371.2235\n",
      "gradient norm: 148.83904838562012, minimum ratio: 0.8046875\n",
      "Epoch [653], val_loss: 373.2104\n",
      "gradient norm: 149.36083221435547, minimum ratio: 0.796875\n",
      "Epoch [654], val_loss: 375.2054\n",
      "gradient norm: 149.88436698913574, minimum ratio: 0.79296875\n",
      "Epoch [655], val_loss: 377.2079\n",
      "gradient norm: 150.40906715393066, minimum ratio: 0.796875\n",
      "Epoch [656], val_loss: 379.2180\n",
      "gradient norm: 150.93733596801758, minimum ratio: 0.78515625\n",
      "Epoch [657], val_loss: 381.2363\n",
      "gradient norm: 151.46503162384033, minimum ratio: 0.80078125\n",
      "Epoch [658], val_loss: 383.2628\n",
      "gradient norm: 151.9952907562256, minimum ratio: 0.796875\n",
      "Epoch [659], val_loss: 385.2974\n",
      "gradient norm: 152.52776527404785, minimum ratio: 0.78125\n",
      "Epoch [660], val_loss: 387.3400\n",
      "gradient norm: 153.0592679977417, minimum ratio: 0.8125\n",
      "Epoch [661], val_loss: 389.3911\n",
      "gradient norm: 153.5905466079712, minimum ratio: 0.80078125\n",
      "Epoch [662], val_loss: 391.4498\n",
      "gradient norm: 154.12636280059814, minimum ratio: 0.78125\n",
      "Epoch [663], val_loss: 393.5171\n",
      "gradient norm: 154.6618423461914, minimum ratio: 0.8046875\n",
      "Epoch [664], val_loss: 395.5931\n",
      "gradient norm: 155.19808673858643, minimum ratio: 0.78515625\n",
      "Epoch [665], val_loss: 397.6771\n",
      "gradient norm: 155.738263130188, minimum ratio: 0.78125\n",
      "Epoch [666], val_loss: 399.7693\n",
      "gradient norm: 156.27626419067383, minimum ratio: 0.7890625\n",
      "Epoch [667], val_loss: 401.8697\n",
      "gradient norm: 156.81522369384766, minimum ratio: 0.77734375\n",
      "Epoch [668], val_loss: 403.9785\n",
      "gradient norm: 157.35802364349365, minimum ratio: 0.796875\n",
      "Epoch [669], val_loss: 406.0952\n",
      "gradient norm: 157.8990879058838, minimum ratio: 0.76953125\n",
      "Epoch [670], val_loss: 408.2211\n",
      "gradient norm: 158.44212341308594, minimum ratio: 0.80078125\n",
      "Epoch [671], val_loss: 410.3549\n",
      "gradient norm: 158.98743534088135, minimum ratio: 0.7890625\n",
      "Epoch [672], val_loss: 412.4975\n",
      "gradient norm: 159.53524494171143, minimum ratio: 0.77734375\n",
      "Epoch [673], val_loss: 414.6487\n",
      "gradient norm: 160.08265209197998, minimum ratio: 0.7890625\n",
      "Epoch [674], val_loss: 416.8084\n",
      "gradient norm: 160.6333465576172, minimum ratio: 0.796875\n",
      "Epoch [675], val_loss: 418.9769\n",
      "gradient norm: 161.18320751190186, minimum ratio: 0.79296875\n",
      "Epoch [676], val_loss: 421.1534\n",
      "gradient norm: 161.73729419708252, minimum ratio: 0.78515625\n",
      "Epoch [677], val_loss: 423.3381\n",
      "gradient norm: 162.29271602630615, minimum ratio: 0.78125\n",
      "Epoch [678], val_loss: 425.5314\n",
      "gradient norm: 162.84836769104004, minimum ratio: 0.7890625\n",
      "Epoch [679], val_loss: 427.7331\n",
      "gradient norm: 163.40308284759521, minimum ratio: 0.7890625\n",
      "Epoch [680], val_loss: 429.9432\n",
      "gradient norm: 163.95997619628906, minimum ratio: 0.7578125\n",
      "Epoch [681], val_loss: 432.1617\n",
      "gradient norm: 164.51569747924805, minimum ratio: 0.78515625\n",
      "Epoch [682], val_loss: 434.3889\n",
      "gradient norm: 165.07336044311523, minimum ratio: 0.78125\n",
      "Epoch [683], val_loss: 436.6251\n",
      "gradient norm: 165.63357734680176, minimum ratio: 0.78515625\n",
      "Epoch [684], val_loss: 438.8699\n",
      "gradient norm: 166.195294380188, minimum ratio: 0.76953125\n",
      "Epoch [685], val_loss: 441.1232\n",
      "gradient norm: 166.75782680511475, minimum ratio: 0.79296875\n",
      "Epoch [686], val_loss: 443.3855\n",
      "gradient norm: 167.32309246063232, minimum ratio: 0.78515625\n",
      "Epoch [687], val_loss: 445.6562\n",
      "gradient norm: 167.88693809509277, minimum ratio: 0.78125\n",
      "Epoch [688], val_loss: 447.9353\n",
      "gradient norm: 168.45370864868164, minimum ratio: 0.78515625\n",
      "Epoch [689], val_loss: 450.2231\n",
      "gradient norm: 169.02066135406494, minimum ratio: 0.796875\n",
      "Epoch [690], val_loss: 452.5197\n",
      "gradient norm: 169.58750438690186, minimum ratio: 0.78125\n",
      "Epoch [691], val_loss: 454.8253\n",
      "gradient norm: 170.15798473358154, minimum ratio: 0.77734375\n",
      "Epoch [692], val_loss: 457.1395\n",
      "gradient norm: 170.7302303314209, minimum ratio: 0.77734375\n",
      "Epoch [693], val_loss: 459.4624\n",
      "gradient norm: 171.30447483062744, minimum ratio: 0.79296875\n",
      "Epoch [694], val_loss: 461.7942\n",
      "gradient norm: 171.8761968612671, minimum ratio: 0.7890625\n",
      "Epoch [695], val_loss: 464.1352\n",
      "gradient norm: 172.4536418914795, minimum ratio: 0.78125\n",
      "Epoch [696], val_loss: 466.4853\n",
      "gradient norm: 173.0283088684082, minimum ratio: 0.80078125\n",
      "Epoch [697], val_loss: 468.8441\n",
      "gradient norm: 173.60735607147217, minimum ratio: 0.7734375\n",
      "Epoch [698], val_loss: 471.2119\n",
      "gradient norm: 174.18745136260986, minimum ratio: 0.77734375\n",
      "Epoch [699], val_loss: 473.5890\n",
      "gradient norm: 174.76840019226074, minimum ratio: 0.78515625\n",
      "Epoch [700], val_loss: 475.9754\n",
      "gradient norm: 175.35150146484375, minimum ratio: 0.79296875\n",
      "Epoch [701], val_loss: 478.3707\n",
      "gradient norm: 175.93572807312012, minimum ratio: 0.78515625\n",
      "Epoch [702], val_loss: 480.7751\n",
      "gradient norm: 176.51913833618164, minimum ratio: 0.78515625\n",
      "Epoch [703], val_loss: 483.1885\n",
      "gradient norm: 177.1008653640747, minimum ratio: 0.79296875\n",
      "Epoch [704], val_loss: 485.6111\n",
      "gradient norm: 177.6887788772583, minimum ratio: 0.80078125\n",
      "Epoch [705], val_loss: 488.0424\n",
      "gradient norm: 178.2776107788086, minimum ratio: 0.79296875\n",
      "Epoch [706], val_loss: 490.4831\n",
      "gradient norm: 178.86817073822021, minimum ratio: 0.80078125\n",
      "Epoch [707], val_loss: 492.9329\n",
      "gradient norm: 179.46063327789307, minimum ratio: 0.79296875\n",
      "Epoch [708], val_loss: 495.3922\n",
      "gradient norm: 180.0522928237915, minimum ratio: 0.80078125\n",
      "Epoch [709], val_loss: 497.8605\n",
      "gradient norm: 180.64337539672852, minimum ratio: 0.78125\n",
      "Epoch [710], val_loss: 500.3377\n",
      "gradient norm: 181.2358913421631, minimum ratio: 0.76953125\n",
      "Epoch [711], val_loss: 502.8246\n",
      "gradient norm: 181.83368587493896, minimum ratio: 0.80078125\n",
      "Epoch [712], val_loss: 505.3210\n",
      "gradient norm: 182.4270486831665, minimum ratio: 0.78515625\n",
      "Epoch [713], val_loss: 507.8269\n",
      "gradient norm: 183.02570629119873, minimum ratio: 0.78125\n",
      "Epoch [714], val_loss: 510.3417\n",
      "gradient norm: 183.6274175643921, minimum ratio: 0.77734375\n",
      "Epoch [715], val_loss: 512.8652\n",
      "gradient norm: 184.22868061065674, minimum ratio: 0.77734375\n",
      "Epoch [716], val_loss: 515.3980\n",
      "gradient norm: 184.83141994476318, minimum ratio: 0.77734375\n",
      "Epoch [717], val_loss: 517.9402\n",
      "gradient norm: 185.43634796142578, minimum ratio: 0.796875\n",
      "Epoch [718], val_loss: 520.4923\n",
      "gradient norm: 186.04096794128418, minimum ratio: 0.7890625\n",
      "Epoch [719], val_loss: 523.0535\n",
      "gradient norm: 186.64611530303955, minimum ratio: 0.7734375\n",
      "Epoch [720], val_loss: 525.6245\n",
      "gradient norm: 187.2531499862671, minimum ratio: 0.79296875\n",
      "Epoch [721], val_loss: 528.2059\n",
      "gradient norm: 187.86514568328857, minimum ratio: 0.76953125\n",
      "Epoch [722], val_loss: 530.7971\n",
      "gradient norm: 188.47056198120117, minimum ratio: 0.79296875\n",
      "Epoch [723], val_loss: 533.3980\n",
      "gradient norm: 189.08054637908936, minimum ratio: 0.78125\n",
      "Epoch [724], val_loss: 536.0081\n",
      "gradient norm: 189.69469165802002, minimum ratio: 0.78125\n",
      "Epoch [725], val_loss: 538.6284\n",
      "gradient norm: 190.31220626831055, minimum ratio: 0.7734375\n",
      "Epoch [726], val_loss: 541.2579\n",
      "gradient norm: 190.92687892913818, minimum ratio: 0.79296875\n",
      "Epoch [727], val_loss: 543.8971\n",
      "gradient norm: 191.5469627380371, minimum ratio: 0.8125\n",
      "Epoch [728], val_loss: 546.5460\n",
      "gradient norm: 192.16517734527588, minimum ratio: 0.78515625\n",
      "Epoch [729], val_loss: 549.2050\n",
      "gradient norm: 192.78556728363037, minimum ratio: 0.7890625\n",
      "Epoch [730], val_loss: 551.8734\n",
      "gradient norm: 193.40776348114014, minimum ratio: 0.7734375\n",
      "Epoch [731], val_loss: 554.5516\n",
      "gradient norm: 194.03141117095947, minimum ratio: 0.7890625\n",
      "Epoch [732], val_loss: 557.2391\n",
      "gradient norm: 194.65592670440674, minimum ratio: 0.79296875\n",
      "Epoch [733], val_loss: 559.9366\n",
      "gradient norm: 195.28311157226562, minimum ratio: 0.7890625\n",
      "Epoch [734], val_loss: 562.6436\n",
      "gradient norm: 195.90942573547363, minimum ratio: 0.796875\n",
      "Epoch [735], val_loss: 565.3603\n",
      "gradient norm: 196.53236770629883, minimum ratio: 0.796875\n",
      "Epoch [736], val_loss: 568.0870\n",
      "gradient norm: 197.16063404083252, minimum ratio: 0.80078125\n",
      "Epoch [737], val_loss: 570.8231\n",
      "gradient norm: 197.78659343719482, minimum ratio: 0.78125\n",
      "Epoch [738], val_loss: 573.5688\n",
      "gradient norm: 198.41778564453125, minimum ratio: 0.7890625\n",
      "Epoch [739], val_loss: 576.3229\n",
      "gradient norm: 199.0506591796875, minimum ratio: 0.78515625\n",
      "Epoch [740], val_loss: 579.0865\n",
      "gradient norm: 199.68473434448242, minimum ratio: 0.78125\n",
      "Epoch [741], val_loss: 581.8597\n",
      "gradient norm: 200.3186264038086, minimum ratio: 0.796875\n",
      "Epoch [742], val_loss: 584.6427\n",
      "gradient norm: 200.9572238922119, minimum ratio: 0.79296875\n",
      "Epoch [743], val_loss: 587.4349\n",
      "gradient norm: 201.59499549865723, minimum ratio: 0.7890625\n",
      "Epoch [744], val_loss: 590.2368\n",
      "gradient norm: 202.23428344726562, minimum ratio: 0.78515625\n",
      "Epoch [745], val_loss: 593.0485\n",
      "gradient norm: 202.8691120147705, minimum ratio: 0.796875\n",
      "Epoch [746], val_loss: 595.8702\n",
      "gradient norm: 203.51021766662598, minimum ratio: 0.78515625\n",
      "Epoch [747], val_loss: 598.7021\n",
      "gradient norm: 204.15159797668457, minimum ratio: 0.8046875\n",
      "Epoch [748], val_loss: 601.5445\n",
      "gradient norm: 204.79407691955566, minimum ratio: 0.76953125\n",
      "Epoch [749], val_loss: 604.3972\n",
      "gradient norm: 205.4380989074707, minimum ratio: 0.8046875\n",
      "Epoch [750], val_loss: 607.2595\n",
      "gradient norm: 206.08557319641113, minimum ratio: 0.80078125\n",
      "Epoch [751], val_loss: 610.1321\n",
      "gradient norm: 206.7285041809082, minimum ratio: 0.8046875\n",
      "Epoch [752], val_loss: 613.0147\n",
      "gradient norm: 207.37965965270996, minimum ratio: 0.7890625\n",
      "Epoch [753], val_loss: 615.9078\n",
      "gradient norm: 208.02808380126953, minimum ratio: 0.78125\n",
      "Epoch [754], val_loss: 618.8119\n",
      "gradient norm: 208.6777400970459, minimum ratio: 0.80078125\n",
      "Epoch [755], val_loss: 621.7252\n",
      "gradient norm: 209.33323287963867, minimum ratio: 0.77734375\n",
      "Epoch [756], val_loss: 624.6489\n",
      "gradient norm: 209.99016761779785, minimum ratio: 0.796875\n",
      "Epoch [757], val_loss: 627.5825\n",
      "gradient norm: 210.64282989501953, minimum ratio: 0.7890625\n",
      "Epoch [758], val_loss: 630.5270\n",
      "gradient norm: 211.30267715454102, minimum ratio: 0.78515625\n",
      "Epoch [759], val_loss: 633.4819\n",
      "gradient norm: 211.96023178100586, minimum ratio: 0.80078125\n",
      "Epoch [760], val_loss: 636.4475\n",
      "gradient norm: 212.61606407165527, minimum ratio: 0.78515625\n",
      "Epoch [761], val_loss: 639.4235\n",
      "gradient norm: 213.27881050109863, minimum ratio: 0.77734375\n",
      "Epoch [762], val_loss: 642.4094\n",
      "gradient norm: 213.94165992736816, minimum ratio: 0.7734375\n",
      "Epoch [763], val_loss: 645.4060\n",
      "gradient norm: 214.60516357421875, minimum ratio: 0.78515625\n",
      "Epoch [764], val_loss: 648.4128\n",
      "gradient norm: 215.27099800109863, minimum ratio: 0.796875\n",
      "Epoch [765], val_loss: 651.4299\n",
      "gradient norm: 215.9347438812256, minimum ratio: 0.7890625\n",
      "Epoch [766], val_loss: 654.4581\n",
      "gradient norm: 216.60284996032715, minimum ratio: 0.7734375\n",
      "Epoch [767], val_loss: 657.4966\n",
      "gradient norm: 217.2707691192627, minimum ratio: 0.77734375\n",
      "Epoch [768], val_loss: 660.5460\n",
      "gradient norm: 217.93989944458008, minimum ratio: 0.78515625\n",
      "Epoch [769], val_loss: 663.6060\n",
      "gradient norm: 218.6137466430664, minimum ratio: 0.78125\n",
      "Epoch [770], val_loss: 666.6767\n",
      "gradient norm: 219.28478813171387, minimum ratio: 0.79296875\n",
      "Epoch [771], val_loss: 669.7580\n",
      "gradient norm: 219.95139122009277, minimum ratio: 0.78125\n",
      "Epoch [772], val_loss: 672.8495\n",
      "gradient norm: 220.62628555297852, minimum ratio: 0.77734375\n",
      "Epoch [773], val_loss: 675.9520\n",
      "gradient norm: 221.30594444274902, minimum ratio: 0.7734375\n",
      "Epoch [774], val_loss: 679.0652\n",
      "gradient norm: 221.9868450164795, minimum ratio: 0.78125\n",
      "Epoch [775], val_loss: 682.1884\n",
      "gradient norm: 222.66898345947266, minimum ratio: 0.796875\n",
      "Epoch [776], val_loss: 685.3221\n",
      "gradient norm: 223.3513126373291, minimum ratio: 0.796875\n",
      "Epoch [777], val_loss: 688.4666\n",
      "gradient norm: 224.03538703918457, minimum ratio: 0.78515625\n",
      "Epoch [778], val_loss: 691.6226\n",
      "gradient norm: 224.7227382659912, minimum ratio: 0.8203125\n",
      "Epoch [779], val_loss: 694.7902\n",
      "gradient norm: 225.40608978271484, minimum ratio: 0.796875\n",
      "Epoch [780], val_loss: 697.9681\n",
      "gradient norm: 226.09263610839844, minimum ratio: 0.79296875\n",
      "Epoch [781], val_loss: 701.1571\n",
      "gradient norm: 226.78243827819824, minimum ratio: 0.79296875\n",
      "Epoch [782], val_loss: 704.3567\n",
      "gradient norm: 227.4708709716797, minimum ratio: 0.79296875\n",
      "Epoch [783], val_loss: 707.5671\n",
      "gradient norm: 228.1553497314453, minimum ratio: 0.80859375\n",
      "Epoch [784], val_loss: 710.7878\n",
      "gradient norm: 228.84864234924316, minimum ratio: 0.796875\n",
      "Epoch [785], val_loss: 714.0199\n",
      "gradient norm: 229.54251289367676, minimum ratio: 0.78125\n",
      "Epoch [786], val_loss: 717.2631\n",
      "gradient norm: 230.23706817626953, minimum ratio: 0.78515625\n",
      "Epoch [787], val_loss: 720.5173\n",
      "gradient norm: 230.9355354309082, minimum ratio: 0.796875\n",
      "Epoch [788], val_loss: 723.7820\n",
      "gradient norm: 231.6333465576172, minimum ratio: 0.78125\n",
      "Epoch [789], val_loss: 727.0572\n",
      "gradient norm: 232.33070945739746, minimum ratio: 0.7890625\n",
      "Epoch [790], val_loss: 730.3436\n",
      "gradient norm: 233.03229904174805, minimum ratio: 0.78125\n",
      "Epoch [791], val_loss: 733.6411\n",
      "gradient norm: 233.73604011535645, minimum ratio: 0.77734375\n",
      "Epoch [792], val_loss: 736.9494\n",
      "gradient norm: 234.43867111206055, minimum ratio: 0.78125\n",
      "Epoch [793], val_loss: 740.2686\n",
      "gradient norm: 235.13493156433105, minimum ratio: 0.796875\n",
      "Epoch [794], val_loss: 743.5993\n",
      "gradient norm: 235.84171676635742, minimum ratio: 0.79296875\n",
      "Epoch [795], val_loss: 746.9417\n",
      "gradient norm: 236.5489959716797, minimum ratio: 0.7890625\n",
      "Epoch [796], val_loss: 750.2947\n",
      "gradient norm: 237.2560329437256, minimum ratio: 0.78125\n",
      "Epoch [797], val_loss: 753.6589\n",
      "gradient norm: 237.96455192565918, minimum ratio: 0.78125\n",
      "Epoch [798], val_loss: 757.0338\n",
      "gradient norm: 238.67646026611328, minimum ratio: 0.79296875\n",
      "Epoch [799], val_loss: 760.4196\n",
      "gradient norm: 239.3919734954834, minimum ratio: 0.79296875\n",
      "Epoch [800], val_loss: 763.8168\n",
      "gradient norm: 240.10597038269043, minimum ratio: 0.796875\n",
      "Epoch [801], val_loss: 767.2250\n",
      "gradient norm: 240.8224639892578, minimum ratio: 0.77734375\n",
      "Epoch [802], val_loss: 770.6445\n",
      "gradient norm: 241.5370635986328, minimum ratio: 0.77734375\n",
      "Epoch [803], val_loss: 774.0757\n",
      "gradient norm: 242.25397491455078, minimum ratio: 0.7890625\n",
      "Epoch [804], val_loss: 777.5187\n",
      "gradient norm: 242.9749412536621, minimum ratio: 0.78515625\n",
      "Epoch [805], val_loss: 780.9738\n",
      "gradient norm: 243.6978244781494, minimum ratio: 0.7890625\n",
      "Epoch [806], val_loss: 784.4401\n",
      "gradient norm: 244.42389488220215, minimum ratio: 0.76953125\n",
      "Epoch [807], val_loss: 787.9177\n",
      "gradient norm: 245.14505577087402, minimum ratio: 0.78515625\n",
      "Epoch [808], val_loss: 791.4064\n",
      "gradient norm: 245.86775970458984, minimum ratio: 0.828125\n",
      "Epoch [809], val_loss: 794.9070\n",
      "gradient norm: 246.59641456604004, minimum ratio: 0.8125\n",
      "Epoch [810], val_loss: 798.4193\n",
      "gradient norm: 247.32160758972168, minimum ratio: 0.79296875\n",
      "Epoch [811], val_loss: 801.9432\n",
      "gradient norm: 248.04918479919434, minimum ratio: 0.77734375\n",
      "Epoch [812], val_loss: 805.4788\n",
      "gradient norm: 248.7812671661377, minimum ratio: 0.78125\n",
      "Epoch [813], val_loss: 809.0264\n",
      "gradient norm: 249.50482559204102, minimum ratio: 0.78125\n",
      "Epoch [814], val_loss: 812.5852\n",
      "gradient norm: 250.23649978637695, minimum ratio: 0.76953125\n",
      "Epoch [815], val_loss: 816.1567\n",
      "gradient norm: 250.96864700317383, minimum ratio: 0.8046875\n",
      "Epoch [816], val_loss: 819.7399\n",
      "gradient norm: 251.70865058898926, minimum ratio: 0.79296875\n",
      "Epoch [817], val_loss: 823.3348\n",
      "gradient norm: 252.44992446899414, minimum ratio: 0.78515625\n",
      "Epoch [818], val_loss: 826.9406\n",
      "gradient norm: 253.19183349609375, minimum ratio: 0.77734375\n",
      "Epoch [819], val_loss: 830.5577\n",
      "gradient norm: 253.9361515045166, minimum ratio: 0.7890625\n",
      "Epoch [820], val_loss: 834.1859\n",
      "gradient norm: 254.67974853515625, minimum ratio: 0.7734375\n",
      "Epoch [821], val_loss: 837.8262\n",
      "gradient norm: 255.41827201843262, minimum ratio: 0.7890625\n",
      "Epoch [822], val_loss: 841.4774\n",
      "gradient norm: 256.1608142852783, minimum ratio: 0.76953125\n",
      "Epoch [823], val_loss: 845.1408\n",
      "gradient norm: 256.9082508087158, minimum ratio: 0.80078125\n",
      "Epoch [824], val_loss: 848.8165\n",
      "gradient norm: 257.6588897705078, minimum ratio: 0.78515625\n",
      "Epoch [825], val_loss: 852.5047\n",
      "gradient norm: 258.4062252044678, minimum ratio: 0.7734375\n",
      "Epoch [826], val_loss: 856.2048\n",
      "gradient norm: 259.16026306152344, minimum ratio: 0.80078125\n",
      "Epoch [827], val_loss: 859.9167\n",
      "gradient norm: 259.9049987792969, minimum ratio: 0.79296875\n",
      "Epoch [828], val_loss: 863.6401\n",
      "gradient norm: 260.6600704193115, minimum ratio: 0.8125\n",
      "Epoch [829], val_loss: 867.3753\n",
      "gradient norm: 261.4143524169922, minimum ratio: 0.79296875\n",
      "Epoch [830], val_loss: 871.1216\n",
      "gradient norm: 262.1705741882324, minimum ratio: 0.7890625\n",
      "Epoch [831], val_loss: 874.8802\n",
      "gradient norm: 262.9303321838379, minimum ratio: 0.796875\n",
      "Epoch [832], val_loss: 878.6517\n",
      "gradient norm: 263.6922721862793, minimum ratio: 0.78125\n",
      "Epoch [833], val_loss: 882.4349\n",
      "gradient norm: 264.4471035003662, minimum ratio: 0.8046875\n",
      "Epoch [834], val_loss: 886.2304\n",
      "gradient norm: 265.2086715698242, minimum ratio: 0.81640625\n",
      "Epoch [835], val_loss: 890.0379\n",
      "gradient norm: 265.9681053161621, minimum ratio: 0.76171875\n",
      "Epoch [836], val_loss: 893.8573\n",
      "gradient norm: 266.7294273376465, minimum ratio: 0.79296875\n",
      "Epoch [837], val_loss: 897.6896\n",
      "gradient norm: 267.49798583984375, minimum ratio: 0.77734375\n",
      "Epoch [838], val_loss: 901.5342\n",
      "gradient norm: 268.267183303833, minimum ratio: 0.7890625\n",
      "Epoch [839], val_loss: 905.3898\n",
      "gradient norm: 269.0314254760742, minimum ratio: 0.80859375\n",
      "Epoch [840], val_loss: 909.2578\n",
      "gradient norm: 269.7978286743164, minimum ratio: 0.78515625\n",
      "Epoch [841], val_loss: 913.1383\n",
      "gradient norm: 270.57130432128906, minimum ratio: 0.80078125\n",
      "Epoch [842], val_loss: 917.0307\n",
      "gradient norm: 271.3451328277588, minimum ratio: 0.8046875\n",
      "Epoch [843], val_loss: 920.9345\n",
      "gradient norm: 272.11861991882324, minimum ratio: 0.78125\n",
      "Epoch [844], val_loss: 924.8503\n",
      "gradient norm: 272.89024353027344, minimum ratio: 0.796875\n",
      "Epoch [845], val_loss: 928.7783\n",
      "gradient norm: 273.67082595825195, minimum ratio: 0.80859375\n",
      "Epoch [846], val_loss: 932.7188\n",
      "gradient norm: 274.44739723205566, minimum ratio: 0.8046875\n",
      "Epoch [847], val_loss: 936.6713\n",
      "gradient norm: 275.22621154785156, minimum ratio: 0.79296875\n",
      "Epoch [848], val_loss: 940.6358\n",
      "gradient norm: 276.0056629180908, minimum ratio: 0.78515625\n",
      "Epoch [849], val_loss: 944.6125\n",
      "gradient norm: 276.7864685058594, minimum ratio: 0.7890625\n",
      "Epoch [850], val_loss: 948.6020\n",
      "gradient norm: 277.5695266723633, minimum ratio: 0.7734375\n",
      "Epoch [851], val_loss: 952.6044\n",
      "gradient norm: 278.3537292480469, minimum ratio: 0.80078125\n",
      "Epoch [852], val_loss: 956.6204\n",
      "gradient norm: 279.1410388946533, minimum ratio: 0.78515625\n",
      "Epoch [853], val_loss: 960.6487\n",
      "gradient norm: 279.9266662597656, minimum ratio: 0.80078125\n",
      "Epoch [854], val_loss: 964.6887\n",
      "gradient norm: 280.7204532623291, minimum ratio: 0.796875\n",
      "Epoch [855], val_loss: 968.7404\n",
      "gradient norm: 281.50682640075684, minimum ratio: 0.78125\n",
      "Epoch [856], val_loss: 972.8048\n",
      "gradient norm: 282.3039798736572, minimum ratio: 0.78515625\n",
      "Epoch [857], val_loss: 976.8823\n",
      "gradient norm: 283.0989761352539, minimum ratio: 0.78515625\n",
      "Epoch [858], val_loss: 980.9725\n",
      "gradient norm: 283.89227294921875, minimum ratio: 0.8046875\n",
      "Epoch [859], val_loss: 985.0756\n",
      "gradient norm: 284.6839351654053, minimum ratio: 0.80078125\n",
      "Epoch [860], val_loss: 989.1912\n",
      "gradient norm: 285.4823474884033, minimum ratio: 0.77734375\n",
      "Epoch [861], val_loss: 993.3193\n",
      "gradient norm: 286.28182792663574, minimum ratio: 0.80859375\n",
      "Epoch [862], val_loss: 997.4592\n",
      "gradient norm: 287.0804691314697, minimum ratio: 0.77734375\n",
      "Epoch [863], val_loss: 1001.6123\n",
      "gradient norm: 287.8806896209717, minimum ratio: 0.796875\n",
      "Epoch [864], val_loss: 1005.7789\n",
      "gradient norm: 288.6858940124512, minimum ratio: 0.79296875\n",
      "Epoch [865], val_loss: 1009.9583\n",
      "gradient norm: 289.4889831542969, minimum ratio: 0.796875\n",
      "Epoch [866], val_loss: 1014.1514\n",
      "gradient norm: 290.2963695526123, minimum ratio: 0.8046875\n",
      "Epoch [867], val_loss: 1018.3570\n",
      "gradient norm: 291.1075248718262, minimum ratio: 0.78125\n",
      "Epoch [868], val_loss: 1022.5744\n",
      "gradient norm: 291.9111957550049, minimum ratio: 0.78515625\n",
      "Epoch [869], val_loss: 1026.8038\n",
      "gradient norm: 292.71890449523926, minimum ratio: 0.80078125\n",
      "Epoch [870], val_loss: 1031.0458\n",
      "gradient norm: 293.52556800842285, minimum ratio: 0.80078125\n",
      "Epoch [871], val_loss: 1035.3003\n",
      "gradient norm: 294.3419380187988, minimum ratio: 0.8046875\n",
      "Epoch [872], val_loss: 1039.5679\n",
      "gradient norm: 295.1606330871582, minimum ratio: 0.78515625\n",
      "Epoch [873], val_loss: 1043.8488\n",
      "gradient norm: 295.9789733886719, minimum ratio: 0.8046875\n",
      "Epoch [874], val_loss: 1048.1418\n",
      "gradient norm: 296.79787254333496, minimum ratio: 0.79296875\n",
      "Epoch [875], val_loss: 1052.4489\n",
      "gradient norm: 297.6129398345947, minimum ratio: 0.7890625\n",
      "Epoch [876], val_loss: 1056.7694\n",
      "gradient norm: 298.43342781066895, minimum ratio: 0.796875\n",
      "Epoch [877], val_loss: 1061.1036\n",
      "gradient norm: 299.25523567199707, minimum ratio: 0.79296875\n",
      "Epoch [878], val_loss: 1065.4512\n",
      "gradient norm: 300.07525062561035, minimum ratio: 0.78515625\n",
      "Epoch [879], val_loss: 1069.8129\n",
      "gradient norm: 300.8999710083008, minimum ratio: 0.8046875\n",
      "Epoch [880], val_loss: 1074.1873\n",
      "gradient norm: 301.72943115234375, minimum ratio: 0.7890625\n",
      "Epoch [881], val_loss: 1078.5748\n",
      "gradient norm: 302.5561103820801, minimum ratio: 0.79296875\n",
      "Epoch [882], val_loss: 1082.9761\n",
      "gradient norm: 303.3826541900635, minimum ratio: 0.77734375\n",
      "Epoch [883], val_loss: 1087.3903\n",
      "gradient norm: 304.2118320465088, minimum ratio: 0.796875\n",
      "Epoch [884], val_loss: 1091.8185\n",
      "gradient norm: 305.04931259155273, minimum ratio: 0.7890625\n",
      "Epoch [885], val_loss: 1096.2593\n",
      "gradient norm: 305.8838920593262, minimum ratio: 0.79296875\n",
      "Epoch [886], val_loss: 1100.7129\n",
      "gradient norm: 306.7079734802246, minimum ratio: 0.7734375\n",
      "Epoch [887], val_loss: 1105.1796\n",
      "gradient norm: 307.5449962615967, minimum ratio: 0.79296875\n",
      "Epoch [888], val_loss: 1109.6594\n",
      "gradient norm: 308.3841609954834, minimum ratio: 0.76953125\n",
      "Epoch [889], val_loss: 1114.1533\n",
      "gradient norm: 309.2183647155762, minimum ratio: 0.80078125\n",
      "Epoch [890], val_loss: 1118.6610\n",
      "gradient norm: 310.06170654296875, minimum ratio: 0.78515625\n",
      "Epoch [891], val_loss: 1123.1829\n",
      "gradient norm: 310.9072742462158, minimum ratio: 0.8046875\n",
      "Epoch [892], val_loss: 1127.7179\n",
      "gradient norm: 311.7539920806885, minimum ratio: 0.796875\n",
      "Epoch [893], val_loss: 1132.2676\n",
      "gradient norm: 312.6004219055176, minimum ratio: 0.796875\n",
      "Epoch [894], val_loss: 1136.8296\n",
      "gradient norm: 313.44470977783203, minimum ratio: 0.80078125\n",
      "Epoch [895], val_loss: 1141.4052\n",
      "gradient norm: 314.2982006072998, minimum ratio: 0.8125\n",
      "Epoch [896], val_loss: 1145.9937\n",
      "gradient norm: 315.14678382873535, minimum ratio: 0.796875\n",
      "Epoch [897], val_loss: 1150.5961\n",
      "gradient norm: 315.99964332580566, minimum ratio: 0.78125\n",
      "Epoch [898], val_loss: 1155.2117\n",
      "gradient norm: 316.84066581726074, minimum ratio: 0.79296875\n",
      "Epoch [899], val_loss: 1159.8411\n",
      "gradient norm: 317.69474601745605, minimum ratio: 0.8046875\n",
      "Epoch [900], val_loss: 1164.4833\n",
      "gradient norm: 318.55389404296875, minimum ratio: 0.80078125\n",
      "Epoch [901], val_loss: 1169.1388\n",
      "gradient norm: 319.40885162353516, minimum ratio: 0.78515625\n",
      "Epoch [902], val_loss: 1173.8079\n",
      "gradient norm: 320.2637424468994, minimum ratio: 0.78125\n",
      "Epoch [903], val_loss: 1178.4904\n",
      "gradient norm: 321.12074089050293, minimum ratio: 0.80859375\n",
      "Epoch [904], val_loss: 1183.1879\n",
      "gradient norm: 321.9853763580322, minimum ratio: 0.78125\n",
      "Epoch [905], val_loss: 1187.8998\n",
      "gradient norm: 322.849063873291, minimum ratio: 0.78125\n",
      "Epoch [906], val_loss: 1192.6263\n",
      "gradient norm: 323.7130813598633, minimum ratio: 0.78515625\n",
      "Epoch [907], val_loss: 1197.3672\n",
      "gradient norm: 324.5718631744385, minimum ratio: 0.80078125\n",
      "Epoch [908], val_loss: 1202.1218\n",
      "gradient norm: 325.4400939941406, minimum ratio: 0.8046875\n",
      "Epoch [909], val_loss: 1206.8914\n",
      "gradient norm: 326.314884185791, minimum ratio: 0.78125\n",
      "Epoch [910], val_loss: 1211.6749\n",
      "gradient norm: 327.18323135375977, minimum ratio: 0.796875\n",
      "Epoch [911], val_loss: 1216.4723\n",
      "gradient norm: 328.059513092041, minimum ratio: 0.80078125\n",
      "Epoch [912], val_loss: 1221.2826\n",
      "gradient norm: 328.9294128417969, minimum ratio: 0.80859375\n",
      "Epoch [913], val_loss: 1226.1062\n",
      "gradient norm: 329.8018856048584, minimum ratio: 0.78515625\n",
      "Epoch [914], val_loss: 1230.9438\n",
      "gradient norm: 330.67643547058105, minimum ratio: 0.796875\n",
      "Epoch [915], val_loss: 1235.7933\n",
      "gradient norm: 331.5493640899658, minimum ratio: 0.78515625\n",
      "Epoch [916], val_loss: 1240.6586\n",
      "gradient norm: 332.4254207611084, minimum ratio: 0.77734375\n",
      "Epoch [917], val_loss: 1245.5380\n",
      "gradient norm: 333.30660247802734, minimum ratio: 0.796875\n",
      "Epoch [918], val_loss: 1250.4309\n",
      "gradient norm: 334.18836975097656, minimum ratio: 0.78515625\n",
      "Epoch [919], val_loss: 1255.3375\n",
      "gradient norm: 335.0719909667969, minimum ratio: 0.7890625\n",
      "Epoch [920], val_loss: 1260.2574\n",
      "gradient norm: 335.9584655761719, minimum ratio: 0.7890625\n",
      "Epoch [921], val_loss: 1265.1921\n",
      "gradient norm: 336.8412170410156, minimum ratio: 0.7890625\n",
      "Epoch [922], val_loss: 1270.1400\n",
      "gradient norm: 337.73385429382324, minimum ratio: 0.77734375\n",
      "Epoch [923], val_loss: 1275.1021\n",
      "gradient norm: 338.62182807922363, minimum ratio: 0.8046875\n",
      "Epoch [924], val_loss: 1280.0797\n",
      "gradient norm: 339.50796699523926, minimum ratio: 0.79296875\n",
      "Epoch [925], val_loss: 1285.0714\n",
      "gradient norm: 340.40389251708984, minimum ratio: 0.79296875\n",
      "Epoch [926], val_loss: 1290.0778\n",
      "gradient norm: 341.29989433288574, minimum ratio: 0.80859375\n",
      "Epoch [927], val_loss: 1295.0977\n",
      "gradient norm: 342.1861000061035, minimum ratio: 0.78515625\n",
      "Epoch [928], val_loss: 1300.1316\n",
      "gradient norm: 343.0802803039551, minimum ratio: 0.8046875\n",
      "Epoch [929], val_loss: 1305.1804\n",
      "gradient norm: 343.96914863586426, minimum ratio: 0.79296875\n",
      "Epoch [930], val_loss: 1310.2439\n",
      "gradient norm: 344.86845207214355, minimum ratio: 0.78515625\n",
      "Epoch [931], val_loss: 1315.3203\n",
      "gradient norm: 345.76843070983887, minimum ratio: 0.80078125\n",
      "Epoch [932], val_loss: 1320.4124\n",
      "gradient norm: 346.67375564575195, minimum ratio: 0.79296875\n",
      "Epoch [933], val_loss: 1325.5184\n",
      "gradient norm: 347.5704765319824, minimum ratio: 0.7890625\n",
      "Epoch [934], val_loss: 1330.6396\n",
      "gradient norm: 348.47873878479004, minimum ratio: 0.80078125\n",
      "Epoch [935], val_loss: 1335.7753\n",
      "gradient norm: 349.37855529785156, minimum ratio: 0.80859375\n",
      "Epoch [936], val_loss: 1340.9258\n",
      "gradient norm: 350.2897415161133, minimum ratio: 0.7890625\n",
      "Epoch [937], val_loss: 1346.0901\n",
      "gradient norm: 351.20518684387207, minimum ratio: 0.78515625\n",
      "Epoch [938], val_loss: 1351.2710\n",
      "gradient norm: 352.1177177429199, minimum ratio: 0.76953125\n",
      "Epoch [939], val_loss: 1356.4669\n",
      "gradient norm: 353.0358352661133, minimum ratio: 0.76953125\n",
      "Epoch [940], val_loss: 1361.6772\n",
      "gradient norm: 353.94046211242676, minimum ratio: 0.80078125\n",
      "Epoch [941], val_loss: 1366.9016\n",
      "gradient norm: 354.8575038909912, minimum ratio: 0.7890625\n",
      "Epoch [942], val_loss: 1372.1400\n",
      "gradient norm: 355.77513122558594, minimum ratio: 0.79296875\n",
      "Epoch [943], val_loss: 1377.3931\n",
      "gradient norm: 356.6923770904541, minimum ratio: 0.7734375\n",
      "Epoch [944], val_loss: 1382.6600\n",
      "gradient norm: 357.6113815307617, minimum ratio: 0.80078125\n",
      "Epoch [945], val_loss: 1387.9414\n",
      "gradient norm: 358.5333023071289, minimum ratio: 0.8125\n",
      "Epoch [946], val_loss: 1393.2378\n",
      "gradient norm: 359.45910263061523, minimum ratio: 0.7734375\n",
      "Epoch [947], val_loss: 1398.5491\n",
      "gradient norm: 360.38233184814453, minimum ratio: 0.796875\n",
      "Epoch [948], val_loss: 1403.8744\n",
      "gradient norm: 361.30014991760254, minimum ratio: 0.78515625\n",
      "Epoch [949], val_loss: 1409.2135\n",
      "gradient norm: 362.23072242736816, minimum ratio: 0.81640625\n",
      "Epoch [950], val_loss: 1414.5684\n",
      "gradient norm: 363.1605052947998, minimum ratio: 0.80078125\n",
      "Epoch [951], val_loss: 1419.9379\n",
      "gradient norm: 364.08577728271484, minimum ratio: 0.8125\n",
      "Epoch [952], val_loss: 1425.3224\n",
      "gradient norm: 365.0171241760254, minimum ratio: 0.78515625\n",
      "Epoch [953], val_loss: 1430.7220\n",
      "gradient norm: 365.955078125, minimum ratio: 0.79296875\n",
      "Epoch [954], val_loss: 1436.1389\n",
      "gradient norm: 366.88351249694824, minimum ratio: 0.78515625\n",
      "Epoch [955], val_loss: 1441.5707\n",
      "gradient norm: 367.8200225830078, minimum ratio: 0.765625\n",
      "Epoch [956], val_loss: 1447.0175\n",
      "gradient norm: 368.75003814697266, minimum ratio: 0.77734375\n",
      "Epoch [957], val_loss: 1452.4797\n",
      "gradient norm: 369.6947937011719, minimum ratio: 0.8203125\n",
      "Epoch [958], val_loss: 1457.9569\n",
      "gradient norm: 370.63373947143555, minimum ratio: 0.8046875\n",
      "Epoch [959], val_loss: 1463.4492\n",
      "gradient norm: 371.58285903930664, minimum ratio: 0.76953125\n",
      "Epoch [960], val_loss: 1468.9565\n",
      "gradient norm: 372.5336265563965, minimum ratio: 0.81640625\n",
      "Epoch [961], val_loss: 1474.4792\n",
      "gradient norm: 373.4596252441406, minimum ratio: 0.80859375\n",
      "Epoch [962], val_loss: 1480.0164\n",
      "gradient norm: 374.4080123901367, minimum ratio: 0.8046875\n",
      "Epoch [963], val_loss: 1485.5682\n",
      "gradient norm: 375.3562812805176, minimum ratio: 0.8046875\n",
      "Epoch [964], val_loss: 1491.1360\n",
      "gradient norm: 376.30715560913086, minimum ratio: 0.78125\n",
      "Epoch [965], val_loss: 1496.7190\n",
      "gradient norm: 377.2630729675293, minimum ratio: 0.7890625\n",
      "Epoch [966], val_loss: 1502.3171\n",
      "gradient norm: 378.2150993347168, minimum ratio: 0.80078125\n",
      "Epoch [967], val_loss: 1507.9304\n",
      "gradient norm: 379.1697006225586, minimum ratio: 0.79296875\n",
      "Epoch [968], val_loss: 1513.5601\n",
      "gradient norm: 380.1185607910156, minimum ratio: 0.78515625\n",
      "Epoch [969], val_loss: 1519.2048\n",
      "gradient norm: 381.0809898376465, minimum ratio: 0.79296875\n",
      "Epoch [970], val_loss: 1524.8654\n",
      "gradient norm: 382.0348815917969, minimum ratio: 0.8046875\n",
      "Epoch [971], val_loss: 1530.5427\n",
      "gradient norm: 383.00089263916016, minimum ratio: 0.796875\n",
      "Epoch [972], val_loss: 1536.2347\n",
      "gradient norm: 383.9621047973633, minimum ratio: 0.80859375\n",
      "Epoch [973], val_loss: 1541.9426\n",
      "gradient norm: 384.92004013061523, minimum ratio: 0.8046875\n",
      "Epoch [974], val_loss: 1547.6665\n",
      "gradient norm: 385.88585662841797, minimum ratio: 0.796875\n",
      "Epoch [975], val_loss: 1553.4059\n",
      "gradient norm: 386.85120391845703, minimum ratio: 0.796875\n",
      "Epoch [976], val_loss: 1559.1611\n",
      "gradient norm: 387.82714462280273, minimum ratio: 0.79296875\n",
      "Epoch [977], val_loss: 1564.9303\n",
      "gradient norm: 388.7968940734863, minimum ratio: 0.78125\n",
      "Epoch [978], val_loss: 1570.7163\n",
      "gradient norm: 389.76326751708984, minimum ratio: 0.78515625\n",
      "Epoch [979], val_loss: 1576.5181\n",
      "gradient norm: 390.73996353149414, minimum ratio: 0.79296875\n",
      "Epoch [980], val_loss: 1582.3337\n",
      "gradient norm: 391.7123489379883, minimum ratio: 0.78125\n",
      "Epoch [981], val_loss: 1588.1654\n",
      "gradient norm: 392.68806076049805, minimum ratio: 0.7890625\n",
      "Epoch [982], val_loss: 1594.0127\n",
      "gradient norm: 393.662540435791, minimum ratio: 0.78515625\n",
      "Epoch [983], val_loss: 1599.8762\n",
      "gradient norm: 394.648681640625, minimum ratio: 0.79296875\n",
      "Epoch [984], val_loss: 1605.7568\n",
      "gradient norm: 395.6277770996094, minimum ratio: 0.78125\n",
      "Epoch [985], val_loss: 1611.6528\n",
      "gradient norm: 396.6123161315918, minimum ratio: 0.80859375\n",
      "Epoch [986], val_loss: 1617.5664\n",
      "gradient norm: 397.5926284790039, minimum ratio: 0.80078125\n",
      "Epoch [987], val_loss: 1623.4961\n",
      "gradient norm: 398.57362365722656, minimum ratio: 0.77734375\n",
      "Epoch [988], val_loss: 1629.4393\n",
      "gradient norm: 399.5634765625, minimum ratio: 0.8046875\n",
      "Epoch [989], val_loss: 1635.3983\n",
      "gradient norm: 400.5473098754883, minimum ratio: 0.76953125\n",
      "Epoch [990], val_loss: 1641.3724\n",
      "gradient norm: 401.5359916687012, minimum ratio: 0.80078125\n",
      "Epoch [991], val_loss: 1647.3600\n",
      "gradient norm: 402.53027725219727, minimum ratio: 0.7890625\n",
      "Epoch [992], val_loss: 1653.3635\n",
      "gradient norm: 403.51707458496094, minimum ratio: 0.796875\n",
      "Epoch [993], val_loss: 1659.3831\n",
      "gradient norm: 404.5145606994629, minimum ratio: 0.78125\n",
      "Epoch [994], val_loss: 1665.4185\n",
      "gradient norm: 405.50288009643555, minimum ratio: 0.8046875\n",
      "Epoch [995], val_loss: 1671.4684\n",
      "gradient norm: 406.500732421875, minimum ratio: 0.8046875\n",
      "Epoch [996], val_loss: 1677.5342\n",
      "gradient norm: 407.4978790283203, minimum ratio: 0.79296875\n",
      "Epoch [997], val_loss: 1683.6168\n",
      "gradient norm: 408.48776626586914, minimum ratio: 0.796875\n",
      "Epoch [998], val_loss: 1689.7157\n",
      "gradient norm: 409.4937515258789, minimum ratio: 0.78125\n",
      "Epoch [999], val_loss: 1695.8301\n",
      "gradient norm: 410.49602127075195, minimum ratio: 0.81640625\n",
      "Epoch [1000], val_loss: 1701.9619\n",
      "gradient norm: 411.49925994873047, minimum ratio: 0.79296875\n",
      "Epoch [1001], val_loss: 1708.1101\n",
      "gradient norm: 412.5063056945801, minimum ratio: 0.78125\n",
      "Epoch [1002], val_loss: 1714.2753\n",
      "gradient norm: 413.5128364562988, minimum ratio: 0.8046875\n",
      "Epoch [1003], val_loss: 1720.4572\n",
      "gradient norm: 414.52936935424805, minimum ratio: 0.80078125\n",
      "Epoch [1004], val_loss: 1726.6539\n",
      "gradient norm: 415.535099029541, minimum ratio: 0.80859375\n",
      "Epoch [1005], val_loss: 1732.8689\n",
      "gradient norm: 416.543701171875, minimum ratio: 0.79296875\n",
      "Epoch [1006], val_loss: 1739.1000\n",
      "gradient norm: 417.5497131347656, minimum ratio: 0.7734375\n",
      "Epoch [1007], val_loss: 1745.3469\n",
      "gradient norm: 418.5703659057617, minimum ratio: 0.8046875\n",
      "Epoch [1008], val_loss: 1751.6111\n",
      "gradient norm: 419.58094787597656, minimum ratio: 0.80078125\n",
      "Epoch [1009], val_loss: 1757.8927\n",
      "gradient norm: 420.60205459594727, minimum ratio: 0.78515625\n",
      "Epoch [1010], val_loss: 1764.1913\n",
      "gradient norm: 421.61964416503906, minimum ratio: 0.8046875\n",
      "Epoch [1011], val_loss: 1770.5056\n",
      "gradient norm: 422.6410598754883, minimum ratio: 0.80859375\n",
      "Epoch [1012], val_loss: 1776.8363\n",
      "gradient norm: 423.6658630371094, minimum ratio: 0.7890625\n",
      "Epoch [1013], val_loss: 1783.1816\n",
      "gradient norm: 424.6944122314453, minimum ratio: 0.7734375\n",
      "Epoch [1014], val_loss: 1789.5442\n",
      "gradient norm: 425.7205276489258, minimum ratio: 0.7890625\n",
      "Epoch [1015], val_loss: 1795.9248\n",
      "gradient norm: 426.74828338623047, minimum ratio: 0.78515625\n",
      "Epoch [1016], val_loss: 1802.3229\n",
      "gradient norm: 427.7815361022949, minimum ratio: 0.79296875\n",
      "Epoch [1017], val_loss: 1808.7397\n",
      "gradient norm: 428.7983589172363, minimum ratio: 0.8125\n",
      "Epoch [1018], val_loss: 1815.1727\n",
      "gradient norm: 429.83300018310547, minimum ratio: 0.79296875\n",
      "Epoch [1019], val_loss: 1821.6226\n",
      "gradient norm: 430.86389923095703, minimum ratio: 0.80859375\n",
      "Epoch [1020], val_loss: 1828.0896\n",
      "gradient norm: 431.9088935852051, minimum ratio: 0.80078125\n",
      "Epoch [1021], val_loss: 1834.5750\n",
      "gradient norm: 432.9433059692383, minimum ratio: 0.78515625\n",
      "Epoch [1022], val_loss: 1841.0767\n",
      "gradient norm: 433.9873924255371, minimum ratio: 0.78515625\n",
      "Epoch [1023], val_loss: 1847.5938\n",
      "gradient norm: 435.0349006652832, minimum ratio: 0.80078125\n",
      "Epoch [1024], val_loss: 1854.1273\n",
      "gradient norm: 436.0859375, minimum ratio: 0.78515625\n",
      "Epoch [1025], val_loss: 1860.6801\n",
      "gradient norm: 437.129581451416, minimum ratio: 0.78515625\n",
      "Epoch [1026], val_loss: 1867.2488\n",
      "gradient norm: 438.1648826599121, minimum ratio: 0.8203125\n",
      "Epoch [1027], val_loss: 1873.8328\n",
      "gradient norm: 439.2062530517578, minimum ratio: 0.796875\n",
      "Epoch [1028], val_loss: 1880.4313\n",
      "gradient norm: 440.2626533508301, minimum ratio: 0.77734375\n",
      "Epoch [1029], val_loss: 1887.0477\n",
      "gradient norm: 441.3066291809082, minimum ratio: 0.7890625\n",
      "Epoch [1030], val_loss: 1893.6815\n",
      "gradient norm: 442.3468780517578, minimum ratio: 0.78515625\n",
      "Epoch [1031], val_loss: 1900.3329\n",
      "gradient norm: 443.40294647216797, minimum ratio: 0.796875\n",
      "Epoch [1032], val_loss: 1907.0023\n",
      "gradient norm: 444.4589385986328, minimum ratio: 0.8125\n",
      "Epoch [1033], val_loss: 1913.6892\n",
      "gradient norm: 445.5163803100586, minimum ratio: 0.7890625\n",
      "Epoch [1034], val_loss: 1920.3925\n",
      "gradient norm: 446.5791816711426, minimum ratio: 0.8125\n",
      "Epoch [1035], val_loss: 1927.1117\n",
      "gradient norm: 447.6402778625488, minimum ratio: 0.78515625\n",
      "Epoch [1036], val_loss: 1933.8470\n",
      "gradient norm: 448.7074737548828, minimum ratio: 0.8046875\n",
      "Epoch [1037], val_loss: 1940.5970\n",
      "gradient norm: 449.76954650878906, minimum ratio: 0.78515625\n",
      "Epoch [1038], val_loss: 1947.3633\n",
      "gradient norm: 450.8322982788086, minimum ratio: 0.7890625\n",
      "Epoch [1039], val_loss: 1954.1456\n",
      "gradient norm: 451.8893623352051, minimum ratio: 0.79296875\n",
      "Epoch [1040], val_loss: 1960.9463\n",
      "gradient norm: 452.9594268798828, minimum ratio: 0.796875\n",
      "Epoch [1041], val_loss: 1967.7627\n",
      "gradient norm: 454.02123641967773, minimum ratio: 0.76953125\n",
      "Epoch [1042], val_loss: 1974.5984\n",
      "gradient norm: 455.0833740234375, minimum ratio: 0.796875\n",
      "Epoch [1043], val_loss: 1981.4529\n",
      "gradient norm: 456.15697860717773, minimum ratio: 0.796875\n",
      "Epoch [1044], val_loss: 1988.3264\n",
      "gradient norm: 457.216796875, minimum ratio: 0.8046875\n",
      "Epoch [1045], val_loss: 1995.2173\n",
      "gradient norm: 458.2921142578125, minimum ratio: 0.80078125\n",
      "Epoch [1046], val_loss: 2002.1259\n",
      "gradient norm: 459.374698638916, minimum ratio: 0.79296875\n",
      "Epoch [1047], val_loss: 2009.0516\n",
      "gradient norm: 460.4455757141113, minimum ratio: 0.78125\n",
      "Epoch [1048], val_loss: 2015.9941\n",
      "gradient norm: 461.523868560791, minimum ratio: 0.79296875\n",
      "Epoch [1049], val_loss: 2022.9552\n",
      "gradient norm: 462.591064453125, minimum ratio: 0.77734375\n",
      "Epoch [1050], val_loss: 2029.9329\n",
      "gradient norm: 463.6764221191406, minimum ratio: 0.82421875\n",
      "Epoch [1051], val_loss: 2036.9304\n",
      "gradient norm: 464.7702293395996, minimum ratio: 0.79296875\n",
      "Epoch [1052], val_loss: 2043.9435\n",
      "gradient norm: 465.8589324951172, minimum ratio: 0.796875\n",
      "Epoch [1053], val_loss: 2050.9736\n",
      "gradient norm: 466.9474334716797, minimum ratio: 0.80078125\n",
      "Epoch [1054], val_loss: 2058.0217\n",
      "gradient norm: 468.0398826599121, minimum ratio: 0.7734375\n",
      "Epoch [1055], val_loss: 2065.0867\n",
      "gradient norm: 469.13415145874023, minimum ratio: 0.80078125\n",
      "Epoch [1056], val_loss: 2072.1685\n",
      "gradient norm: 470.22986602783203, minimum ratio: 0.78515625\n",
      "Epoch [1057], val_loss: 2079.2688\n",
      "gradient norm: 471.31962966918945, minimum ratio: 0.7890625\n",
      "Epoch [1058], val_loss: 2086.3887\n",
      "gradient norm: 472.4132270812988, minimum ratio: 0.796875\n",
      "Epoch [1059], val_loss: 2093.5273\n",
      "gradient norm: 473.5199737548828, minimum ratio: 0.7734375\n",
      "Epoch [1060], val_loss: 2100.6838\n",
      "gradient norm: 474.6186828613281, minimum ratio: 0.7890625\n",
      "Epoch [1061], val_loss: 2107.8586\n",
      "gradient norm: 475.7290344238281, minimum ratio: 0.80078125\n",
      "Epoch [1062], val_loss: 2115.0513\n",
      "gradient norm: 476.82139587402344, minimum ratio: 0.7890625\n",
      "Epoch [1063], val_loss: 2122.2617\n",
      "gradient norm: 477.9231300354004, minimum ratio: 0.83203125\n",
      "Epoch [1064], val_loss: 2129.4888\n",
      "gradient norm: 479.00975036621094, minimum ratio: 0.78515625\n",
      "Epoch [1065], val_loss: 2136.7349\n",
      "gradient norm: 480.11606216430664, minimum ratio: 0.78515625\n",
      "Epoch [1066], val_loss: 2143.9971\n",
      "gradient norm: 481.2293891906738, minimum ratio: 0.7890625\n",
      "Epoch [1067], val_loss: 2151.2791\n",
      "gradient norm: 482.33752822875977, minimum ratio: 0.8046875\n",
      "Epoch [1068], val_loss: 2158.5801\n",
      "gradient norm: 483.4538345336914, minimum ratio: 0.8125\n",
      "Epoch [1069], val_loss: 2165.8992\n",
      "gradient norm: 484.5727729797363, minimum ratio: 0.7734375\n",
      "Epoch [1070], val_loss: 2173.2356\n",
      "gradient norm: 485.68932723999023, minimum ratio: 0.796875\n",
      "Epoch [1071], val_loss: 2180.5896\n",
      "gradient norm: 486.80542755126953, minimum ratio: 0.79296875\n",
      "Epoch [1072], val_loss: 2187.9614\n",
      "gradient norm: 487.92394638061523, minimum ratio: 0.78125\n",
      "Epoch [1073], val_loss: 2195.3496\n",
      "gradient norm: 489.0413475036621, minimum ratio: 0.80859375\n",
      "Epoch [1074], val_loss: 2202.7563\n",
      "gradient norm: 490.16121673583984, minimum ratio: 0.796875\n",
      "Epoch [1075], val_loss: 2210.1816\n",
      "gradient norm: 491.2807502746582, minimum ratio: 0.80078125\n",
      "Epoch [1076], val_loss: 2217.6248\n",
      "gradient norm: 492.4119758605957, minimum ratio: 0.796875\n",
      "Epoch [1077], val_loss: 2225.0850\n",
      "gradient norm: 493.5435447692871, minimum ratio: 0.79296875\n",
      "Epoch [1078], val_loss: 2232.5627\n",
      "gradient norm: 494.671329498291, minimum ratio: 0.7890625\n",
      "Epoch [1079], val_loss: 2240.0591\n",
      "gradient norm: 495.794620513916, minimum ratio: 0.765625\n",
      "Epoch [1080], val_loss: 2247.5745\n",
      "gradient norm: 496.9243392944336, minimum ratio: 0.7890625\n",
      "Epoch [1081], val_loss: 2255.1079\n",
      "gradient norm: 498.0500259399414, minimum ratio: 0.78125\n",
      "Epoch [1082], val_loss: 2262.6584\n",
      "gradient norm: 499.1876640319824, minimum ratio: 0.82421875\n",
      "Epoch [1083], val_loss: 2270.2271\n",
      "gradient norm: 500.32071685791016, minimum ratio: 0.796875\n",
      "Epoch [1084], val_loss: 2277.8118\n",
      "gradient norm: 501.4453315734863, minimum ratio: 0.8046875\n",
      "Epoch [1085], val_loss: 2285.4138\n",
      "gradient norm: 502.5781593322754, minimum ratio: 0.78515625\n",
      "Epoch [1086], val_loss: 2293.0349\n",
      "gradient norm: 503.7056427001953, minimum ratio: 0.8046875\n",
      "Epoch [1087], val_loss: 2300.6743\n",
      "gradient norm: 504.8484420776367, minimum ratio: 0.78125\n",
      "Epoch [1088], val_loss: 2308.3325\n",
      "gradient norm: 505.9934997558594, minimum ratio: 0.78515625\n",
      "Epoch [1089], val_loss: 2316.0129\n",
      "gradient norm: 507.13956451416016, minimum ratio: 0.80078125\n",
      "Epoch [1090], val_loss: 2323.7124\n",
      "gradient norm: 508.27979278564453, minimum ratio: 0.78125\n",
      "Epoch [1091], val_loss: 2331.4302\n",
      "gradient norm: 509.4330596923828, minimum ratio: 0.79296875\n",
      "Epoch [1092], val_loss: 2339.1675\n",
      "gradient norm: 510.5860023498535, minimum ratio: 0.8046875\n",
      "Epoch [1093], val_loss: 2346.9229\n",
      "gradient norm: 511.7370910644531, minimum ratio: 0.79296875\n",
      "Epoch [1094], val_loss: 2354.6968\n",
      "gradient norm: 512.8994979858398, minimum ratio: 0.796875\n",
      "Epoch [1095], val_loss: 2362.4907\n",
      "gradient norm: 514.0436134338379, minimum ratio: 0.8046875\n",
      "Epoch [1096], val_loss: 2370.3035\n",
      "gradient norm: 515.2097244262695, minimum ratio: 0.78515625\n",
      "Epoch [1097], val_loss: 2378.1333\n",
      "gradient norm: 516.3719100952148, minimum ratio: 0.78515625\n",
      "Epoch [1098], val_loss: 2385.9824\n",
      "gradient norm: 517.5305023193359, minimum ratio: 0.78515625\n",
      "Epoch [1099], val_loss: 2393.8477\n",
      "gradient norm: 518.6886558532715, minimum ratio: 0.78515625\n",
      "Epoch [1100], val_loss: 2401.7310\n",
      "gradient norm: 519.8537635803223, minimum ratio: 0.79296875\n",
      "Epoch [1101], val_loss: 2409.6343\n",
      "gradient norm: 521.0187950134277, minimum ratio: 0.80859375\n",
      "Epoch [1102], val_loss: 2417.5566\n",
      "gradient norm: 522.1863403320312, minimum ratio: 0.796875\n",
      "Epoch [1103], val_loss: 2425.4978\n",
      "gradient norm: 523.3571701049805, minimum ratio: 0.78125\n",
      "Epoch [1104], val_loss: 2433.4595\n",
      "gradient norm: 524.5244140625, minimum ratio: 0.7890625\n",
      "Epoch [1105], val_loss: 2441.4412\n",
      "gradient norm: 525.6796073913574, minimum ratio: 0.7890625\n",
      "Epoch [1106], val_loss: 2449.4421\n",
      "gradient norm: 526.8530311584473, minimum ratio: 0.796875\n",
      "Epoch [1107], val_loss: 2457.4624\n",
      "gradient norm: 528.0251541137695, minimum ratio: 0.78515625\n",
      "Epoch [1108], val_loss: 2465.5029\n",
      "gradient norm: 529.2112121582031, minimum ratio: 0.7890625\n",
      "Epoch [1109], val_loss: 2473.5596\n",
      "gradient norm: 530.3872451782227, minimum ratio: 0.7890625\n",
      "Epoch [1110], val_loss: 2481.6372\n",
      "gradient norm: 531.5723419189453, minimum ratio: 0.7890625\n",
      "Epoch [1111], val_loss: 2489.7339\n",
      "gradient norm: 532.7298278808594, minimum ratio: 0.7734375\n",
      "Epoch [1112], val_loss: 2497.8499\n",
      "gradient norm: 533.9123725891113, minimum ratio: 0.80078125\n",
      "Epoch [1113], val_loss: 2505.9839\n",
      "gradient norm: 535.1059150695801, minimum ratio: 0.80078125\n",
      "Epoch [1114], val_loss: 2514.1345\n",
      "gradient norm: 536.2879753112793, minimum ratio: 0.79296875\n",
      "Epoch [1115], val_loss: 2522.3037\n",
      "gradient norm: 537.473503112793, minimum ratio: 0.78515625\n",
      "Epoch [1116], val_loss: 2530.4937\n",
      "gradient norm: 538.6601104736328, minimum ratio: 0.78125\n",
      "Epoch [1117], val_loss: 2538.7019\n",
      "gradient norm: 539.8512268066406, minimum ratio: 0.80078125\n",
      "Epoch [1118], val_loss: 2546.9312\n",
      "gradient norm: 541.0529518127441, minimum ratio: 0.79296875\n",
      "Epoch [1119], val_loss: 2555.1812\n",
      "gradient norm: 542.247932434082, minimum ratio: 0.80859375\n",
      "Epoch [1120], val_loss: 2563.4482\n",
      "gradient norm: 543.4345855712891, minimum ratio: 0.79296875\n",
      "Epoch [1121], val_loss: 2571.7358\n",
      "gradient norm: 544.6337013244629, minimum ratio: 0.8125\n",
      "Epoch [1122], val_loss: 2580.0432\n",
      "gradient norm: 545.8164215087891, minimum ratio: 0.78125\n",
      "Epoch [1123], val_loss: 2588.3665\n",
      "gradient norm: 547.0072364807129, minimum ratio: 0.7890625\n",
      "Epoch [1124], val_loss: 2596.7109\n",
      "gradient norm: 548.2129135131836, minimum ratio: 0.78125\n",
      "Epoch [1125], val_loss: 2605.0759\n",
      "gradient norm: 549.411205291748, minimum ratio: 0.7734375\n",
      "Epoch [1126], val_loss: 2613.4629\n",
      "gradient norm: 550.6224517822266, minimum ratio: 0.79296875\n",
      "Epoch [1127], val_loss: 2621.8662\n",
      "gradient norm: 551.8288078308105, minimum ratio: 0.78125\n",
      "Epoch [1128], val_loss: 2630.2888\n",
      "gradient norm: 553.0365142822266, minimum ratio: 0.796875\n",
      "Epoch [1129], val_loss: 2638.7329\n",
      "gradient norm: 554.2524871826172, minimum ratio: 0.7890625\n",
      "Epoch [1130], val_loss: 2647.1970\n",
      "gradient norm: 555.4680595397949, minimum ratio: 0.765625\n",
      "Epoch [1131], val_loss: 2655.6812\n",
      "gradient norm: 556.6681823730469, minimum ratio: 0.80078125\n",
      "Epoch [1132], val_loss: 2664.1838\n",
      "gradient norm: 557.8818244934082, minimum ratio: 0.796875\n",
      "Epoch [1133], val_loss: 2672.7056\n",
      "gradient norm: 559.1025276184082, minimum ratio: 0.78515625\n",
      "Epoch [1134], val_loss: 2681.2466\n",
      "gradient norm: 560.3236656188965, minimum ratio: 0.78515625\n",
      "Epoch [1135], val_loss: 2689.8101\n",
      "gradient norm: 561.5503997802734, minimum ratio: 0.796875\n",
      "Epoch [1136], val_loss: 2698.3928\n",
      "gradient norm: 562.7601013183594, minimum ratio: 0.7890625\n",
      "Epoch [1137], val_loss: 2706.9932\n",
      "gradient norm: 563.9858283996582, minimum ratio: 0.7890625\n",
      "Epoch [1138], val_loss: 2715.6143\n",
      "gradient norm: 565.2182121276855, minimum ratio: 0.7734375\n",
      "Epoch [1139], val_loss: 2724.2549\n",
      "gradient norm: 566.4347724914551, minimum ratio: 0.7890625\n",
      "Epoch [1140], val_loss: 2732.9177\n",
      "gradient norm: 567.6604385375977, minimum ratio: 0.77734375\n",
      "Epoch [1141], val_loss: 2741.6008\n",
      "gradient norm: 568.888011932373, minimum ratio: 0.78515625\n",
      "Epoch [1142], val_loss: 2750.3032\n",
      "gradient norm: 570.1297454833984, minimum ratio: 0.79296875\n",
      "Epoch [1143], val_loss: 2759.0247\n",
      "gradient norm: 571.3728332519531, minimum ratio: 0.78515625\n",
      "Epoch [1144], val_loss: 2767.7656\n",
      "gradient norm: 572.5860633850098, minimum ratio: 0.796875\n",
      "Epoch [1145], val_loss: 2776.5276\n",
      "gradient norm: 573.8238372802734, minimum ratio: 0.77734375\n",
      "Epoch [1146], val_loss: 2785.3088\n",
      "gradient norm: 575.0559272766113, minimum ratio: 0.78125\n",
      "Epoch [1147], val_loss: 2794.1116\n",
      "gradient norm: 576.2849273681641, minimum ratio: 0.76953125\n",
      "Epoch [1148], val_loss: 2802.9333\n",
      "gradient norm: 577.5238227844238, minimum ratio: 0.78125\n",
      "Epoch [1149], val_loss: 2811.7749\n",
      "gradient norm: 578.7707366943359, minimum ratio: 0.78125\n",
      "Epoch [1150], val_loss: 2820.6377\n",
      "gradient norm: 580.0217666625977, minimum ratio: 0.7890625\n",
      "Epoch [1151], val_loss: 2829.5210\n",
      "gradient norm: 581.2676887512207, minimum ratio: 0.80078125\n",
      "Epoch [1152], val_loss: 2838.4253\n",
      "gradient norm: 582.5262451171875, minimum ratio: 0.7890625\n",
      "Epoch [1153], val_loss: 2847.3501\n",
      "gradient norm: 583.7634391784668, minimum ratio: 0.796875\n",
      "Epoch [1154], val_loss: 2856.2954\n",
      "gradient norm: 585.0165214538574, minimum ratio: 0.78125\n",
      "Epoch [1155], val_loss: 2865.2607\n",
      "gradient norm: 586.2672805786133, minimum ratio: 0.7890625\n",
      "Epoch [1156], val_loss: 2874.2488\n",
      "gradient norm: 587.5169525146484, minimum ratio: 0.80078125\n",
      "Epoch [1157], val_loss: 2883.2563\n",
      "gradient norm: 588.7642440795898, minimum ratio: 0.7890625\n",
      "Epoch [1158], val_loss: 2892.2852\n",
      "gradient norm: 590.0300636291504, minimum ratio: 0.79296875\n",
      "Epoch [1159], val_loss: 2901.3330\n",
      "gradient norm: 591.3008155822754, minimum ratio: 0.76953125\n",
      "Epoch [1160], val_loss: 2910.4011\n",
      "gradient norm: 592.5648918151855, minimum ratio: 0.78125\n",
      "Epoch [1161], val_loss: 2919.4890\n",
      "gradient norm: 593.8179779052734, minimum ratio: 0.78515625\n",
      "Epoch [1162], val_loss: 2928.5959\n",
      "gradient norm: 595.0811424255371, minimum ratio: 0.796875\n",
      "Epoch [1163], val_loss: 2937.7251\n",
      "gradient norm: 596.3393211364746, minimum ratio: 0.76953125\n",
      "Epoch [1164], val_loss: 2946.8743\n",
      "gradient norm: 597.6020927429199, minimum ratio: 0.78125\n",
      "Epoch [1165], val_loss: 2956.0430\n",
      "gradient norm: 598.8689422607422, minimum ratio: 0.79296875\n",
      "Epoch [1166], val_loss: 2965.2307\n",
      "gradient norm: 600.1500968933105, minimum ratio: 0.77734375\n",
      "Epoch [1167], val_loss: 2974.4387\n",
      "gradient norm: 601.4170227050781, minimum ratio: 0.8046875\n",
      "Epoch [1168], val_loss: 2983.6689\n",
      "gradient norm: 602.6872825622559, minimum ratio: 0.7890625\n",
      "Epoch [1169], val_loss: 2992.9197\n",
      "gradient norm: 603.973747253418, minimum ratio: 0.80859375\n",
      "Epoch [1170], val_loss: 3002.1909\n",
      "gradient norm: 605.2467765808105, minimum ratio: 0.796875\n",
      "Epoch [1171], val_loss: 3011.4829\n",
      "gradient norm: 606.527229309082, minimum ratio: 0.796875\n",
      "Epoch [1172], val_loss: 3020.7949\n",
      "gradient norm: 607.8184700012207, minimum ratio: 0.76953125\n",
      "Epoch [1173], val_loss: 3030.1277\n",
      "gradient norm: 609.087272644043, minimum ratio: 0.78125\n",
      "Epoch [1174], val_loss: 3039.4827\n",
      "gradient norm: 610.3713989257812, minimum ratio: 0.80078125\n",
      "Epoch [1175], val_loss: 3048.8555\n",
      "gradient norm: 611.6457481384277, minimum ratio: 0.7890625\n",
      "Epoch [1176], val_loss: 3058.2498\n",
      "gradient norm: 612.9275970458984, minimum ratio: 0.79296875\n",
      "Epoch [1177], val_loss: 3067.6663\n",
      "gradient norm: 614.2208976745605, minimum ratio: 0.78515625\n",
      "Epoch [1178], val_loss: 3077.1064\n",
      "gradient norm: 615.5068511962891, minimum ratio: 0.79296875\n",
      "Epoch [1179], val_loss: 3086.5642\n",
      "gradient norm: 616.8045082092285, minimum ratio: 0.796875\n",
      "Epoch [1180], val_loss: 3096.0417\n",
      "gradient norm: 618.0959777832031, minimum ratio: 0.80078125\n",
      "Epoch [1181], val_loss: 3105.5432\n",
      "gradient norm: 619.3902816772461, minimum ratio: 0.8203125\n",
      "Epoch [1182], val_loss: 3115.0667\n",
      "gradient norm: 620.665412902832, minimum ratio: 0.765625\n",
      "Epoch [1183], val_loss: 3124.6125\n",
      "gradient norm: 621.9761123657227, minimum ratio: 0.80859375\n",
      "Epoch [1184], val_loss: 3134.1797\n",
      "gradient norm: 623.2885208129883, minimum ratio: 0.78125\n",
      "Epoch [1185], val_loss: 3143.7664\n",
      "gradient norm: 624.5886383056641, minimum ratio: 0.77734375\n",
      "Epoch [1186], val_loss: 3153.3772\n",
      "gradient norm: 625.8973541259766, minimum ratio: 0.7734375\n",
      "Epoch [1187], val_loss: 3163.0112\n",
      "gradient norm: 627.1895637512207, minimum ratio: 0.7734375\n",
      "Epoch [1188], val_loss: 3172.6682\n",
      "gradient norm: 628.501537322998, minimum ratio: 0.76953125\n",
      "Epoch [1189], val_loss: 3182.3459\n",
      "gradient norm: 629.8114547729492, minimum ratio: 0.7734375\n",
      "Epoch [1190], val_loss: 3192.0466\n",
      "gradient norm: 631.1360511779785, minimum ratio: 0.77734375\n",
      "Epoch [1191], val_loss: 3201.7678\n",
      "gradient norm: 632.4506988525391, minimum ratio: 0.77734375\n",
      "Epoch [1192], val_loss: 3211.5103\n",
      "gradient norm: 633.7626113891602, minimum ratio: 0.796875\n",
      "Epoch [1193], val_loss: 3221.2705\n",
      "gradient norm: 635.0908813476562, minimum ratio: 0.765625\n",
      "Epoch [1194], val_loss: 3231.0505\n",
      "gradient norm: 636.4068908691406, minimum ratio: 0.78125\n",
      "Epoch [1195], val_loss: 3240.8525\n",
      "gradient norm: 637.7161483764648, minimum ratio: 0.8046875\n",
      "Epoch [1196], val_loss: 3250.6763\n",
      "gradient norm: 639.0390968322754, minimum ratio: 0.8046875\n",
      "Epoch [1197], val_loss: 3260.5227\n",
      "gradient norm: 640.3515243530273, minimum ratio: 0.77734375\n",
      "Epoch [1198], val_loss: 3270.3909\n",
      "gradient norm: 641.6883773803711, minimum ratio: 0.76953125\n",
      "Epoch [1199], val_loss: 3280.2786\n",
      "gradient norm: 643.0045013427734, minimum ratio: 0.80859375\n",
      "Epoch [1200], val_loss: 3290.1885\n",
      "gradient norm: 644.3142700195312, minimum ratio: 0.76953125\n",
      "Epoch [1201], val_loss: 3300.1191\n",
      "gradient norm: 645.6484184265137, minimum ratio: 0.79296875\n",
      "Epoch [1202], val_loss: 3310.0728\n",
      "gradient norm: 646.9835472106934, minimum ratio: 0.76171875\n",
      "Epoch [1203], val_loss: 3320.0476\n",
      "gradient norm: 648.3210906982422, minimum ratio: 0.80859375\n",
      "Epoch [1204], val_loss: 3330.0425\n",
      "gradient norm: 649.6626930236816, minimum ratio: 0.76953125\n",
      "Epoch [1205], val_loss: 3340.0603\n",
      "gradient norm: 650.985294342041, minimum ratio: 0.80078125\n",
      "Epoch [1206], val_loss: 3350.0989\n",
      "gradient norm: 652.2998542785645, minimum ratio: 0.78515625\n",
      "Epoch [1207], val_loss: 3360.1582\n",
      "gradient norm: 653.650806427002, minimum ratio: 0.77734375\n",
      "Epoch [1208], val_loss: 3370.2402\n",
      "gradient norm: 655.0038833618164, minimum ratio: 0.76953125\n",
      "Epoch [1209], val_loss: 3380.3433\n",
      "gradient norm: 656.3476600646973, minimum ratio: 0.79296875\n",
      "Epoch [1210], val_loss: 3390.4675\n",
      "gradient norm: 657.6916275024414, minimum ratio: 0.78515625\n",
      "Epoch [1211], val_loss: 3400.6162\n",
      "gradient norm: 659.0393295288086, minimum ratio: 0.78515625\n",
      "Epoch [1212], val_loss: 3410.7852\n",
      "gradient norm: 660.3742027282715, minimum ratio: 0.796875\n",
      "Epoch [1213], val_loss: 3420.9761\n",
      "gradient norm: 661.7353401184082, minimum ratio: 0.78515625\n",
      "Epoch [1214], val_loss: 3431.1875\n",
      "gradient norm: 663.0726356506348, minimum ratio: 0.78515625\n",
      "Epoch [1215], val_loss: 3441.4224\n",
      "gradient norm: 664.4334564208984, minimum ratio: 0.77734375\n",
      "Epoch [1216], val_loss: 3451.6799\n",
      "gradient norm: 665.7846870422363, minimum ratio: 0.80078125\n",
      "Epoch [1217], val_loss: 3461.9612\n",
      "gradient norm: 667.1475410461426, minimum ratio: 0.7734375\n",
      "Epoch [1218], val_loss: 3472.2607\n",
      "gradient norm: 668.5000839233398, minimum ratio: 0.7734375\n",
      "Epoch [1219], val_loss: 3482.5835\n",
      "gradient norm: 669.8656387329102, minimum ratio: 0.79296875\n",
      "Epoch [1220], val_loss: 3492.9253\n",
      "gradient norm: 671.2235794067383, minimum ratio: 0.76953125\n",
      "Epoch [1221], val_loss: 3503.2896\n",
      "gradient norm: 672.5857162475586, minimum ratio: 0.7890625\n",
      "Epoch [1222], val_loss: 3513.6792\n",
      "gradient norm: 673.9425964355469, minimum ratio: 0.79296875\n",
      "Epoch [1223], val_loss: 3524.0916\n",
      "gradient norm: 675.2883682250977, minimum ratio: 0.8046875\n",
      "Epoch [1224], val_loss: 3534.5247\n",
      "gradient norm: 676.6422653198242, minimum ratio: 0.79296875\n",
      "Epoch [1225], val_loss: 3544.9800\n",
      "gradient norm: 678.0237197875977, minimum ratio: 0.796875\n",
      "Epoch [1226], val_loss: 3555.4570\n",
      "gradient norm: 679.3909912109375, minimum ratio: 0.77734375\n",
      "Epoch [1227], val_loss: 3565.9546\n",
      "gradient norm: 680.7753639221191, minimum ratio: 0.77734375\n",
      "Epoch [1228], val_loss: 3576.4775\n",
      "gradient norm: 682.1408729553223, minimum ratio: 0.76953125\n",
      "Epoch [1229], val_loss: 3587.0242\n",
      "gradient norm: 683.5172348022461, minimum ratio: 0.77734375\n",
      "Epoch [1230], val_loss: 3597.5916\n",
      "gradient norm: 684.8857231140137, minimum ratio: 0.765625\n",
      "Epoch [1231], val_loss: 3608.1816\n",
      "gradient norm: 686.2689247131348, minimum ratio: 0.76171875\n",
      "Epoch [1232], val_loss: 3618.7949\n",
      "gradient norm: 687.632625579834, minimum ratio: 0.78125\n",
      "Epoch [1233], val_loss: 3629.4321\n",
      "gradient norm: 689.0105400085449, minimum ratio: 0.7890625\n",
      "Epoch [1234], val_loss: 3640.0903\n",
      "gradient norm: 690.408088684082, minimum ratio: 0.78125\n",
      "Epoch [1235], val_loss: 3650.7725\n",
      "gradient norm: 691.8048553466797, minimum ratio: 0.7890625\n",
      "Epoch [1236], val_loss: 3661.4751\n",
      "gradient norm: 693.2057037353516, minimum ratio: 0.79296875\n",
      "Epoch [1237], val_loss: 3672.2004\n",
      "gradient norm: 694.5840682983398, minimum ratio: 0.78515625\n",
      "Epoch [1238], val_loss: 3682.9492\n",
      "gradient norm: 695.9850387573242, minimum ratio: 0.7578125\n",
      "Epoch [1239], val_loss: 3693.7205\n",
      "gradient norm: 697.3794822692871, minimum ratio: 0.79296875\n",
      "Epoch [1240], val_loss: 3704.5142\n",
      "gradient norm: 698.7872505187988, minimum ratio: 0.7890625\n",
      "Epoch [1241], val_loss: 3715.3320\n",
      "gradient norm: 700.1811828613281, minimum ratio: 0.7890625\n",
      "Epoch [1242], val_loss: 3726.1726\n",
      "gradient norm: 701.5808601379395, minimum ratio: 0.80078125\n",
      "Epoch [1243], val_loss: 3737.0361\n",
      "gradient norm: 702.9858894348145, minimum ratio: 0.76171875\n",
      "Epoch [1244], val_loss: 3747.9238\n",
      "gradient norm: 704.3948516845703, minimum ratio: 0.77734375\n",
      "Epoch [1245], val_loss: 3758.8354\n",
      "gradient norm: 705.7978248596191, minimum ratio: 0.76171875\n",
      "Epoch [1246], val_loss: 3769.7700\n",
      "gradient norm: 707.200065612793, minimum ratio: 0.765625\n",
      "Epoch [1247], val_loss: 3780.7266\n",
      "gradient norm: 708.6209030151367, minimum ratio: 0.78125\n",
      "Epoch [1248], val_loss: 3791.7068\n",
      "gradient norm: 710.0302505493164, minimum ratio: 0.78125\n",
      "Epoch [1249], val_loss: 3802.7083\n",
      "gradient norm: 711.4274482727051, minimum ratio: 0.7734375\n",
      "Epoch [1250], val_loss: 3813.7324\n",
      "gradient norm: 712.8387680053711, minimum ratio: 0.76953125\n",
      "Epoch [1251], val_loss: 3824.7759\n",
      "gradient norm: 714.2392349243164, minimum ratio: 0.796875\n",
      "Epoch [1252], val_loss: 3835.8445\n",
      "gradient norm: 715.6530838012695, minimum ratio: 0.765625\n",
      "Epoch [1253], val_loss: 3846.9353\n",
      "gradient norm: 717.0714263916016, minimum ratio: 0.78515625\n",
      "Epoch [1254], val_loss: 3858.0525\n",
      "gradient norm: 718.4588928222656, minimum ratio: 0.80078125\n",
      "Epoch [1255], val_loss: 3869.1909\n",
      "gradient norm: 719.8760070800781, minimum ratio: 0.77734375\n",
      "Epoch [1256], val_loss: 3880.3533\n",
      "gradient norm: 721.2796478271484, minimum ratio: 0.79296875\n",
      "Epoch [1257], val_loss: 3891.5388\n",
      "gradient norm: 722.6791915893555, minimum ratio: 0.77734375\n",
      "Epoch [1258], val_loss: 3902.7478\n",
      "gradient norm: 724.1098480224609, minimum ratio: 0.7734375\n",
      "Epoch [1259], val_loss: 3913.9766\n",
      "gradient norm: 725.5429229736328, minimum ratio: 0.7578125\n",
      "Epoch [1260], val_loss: 3925.2307\n",
      "gradient norm: 726.981689453125, minimum ratio: 0.78125\n",
      "Epoch [1261], val_loss: 3936.5100\n",
      "gradient norm: 728.3968200683594, minimum ratio: 0.77734375\n",
      "Epoch [1262], val_loss: 3947.8113\n",
      "gradient norm: 729.8426513671875, minimum ratio: 0.8046875\n",
      "Epoch [1263], val_loss: 3959.1355\n",
      "gradient norm: 731.2754669189453, minimum ratio: 0.79296875\n",
      "Epoch [1264], val_loss: 3970.4800\n",
      "gradient norm: 732.7238998413086, minimum ratio: 0.76953125\n",
      "Epoch [1265], val_loss: 3981.8472\n",
      "gradient norm: 734.1679000854492, minimum ratio: 0.80078125\n",
      "Epoch [1266], val_loss: 3993.2407\n",
      "gradient norm: 735.6205902099609, minimum ratio: 0.77734375\n",
      "Epoch [1267], val_loss: 4004.6584\n",
      "gradient norm: 737.0509338378906, minimum ratio: 0.78515625\n",
      "Epoch [1268], val_loss: 4016.1001\n",
      "gradient norm: 738.5018997192383, minimum ratio: 0.7734375\n",
      "Epoch [1269], val_loss: 4027.5667\n",
      "gradient norm: 739.9606628417969, minimum ratio: 0.796875\n",
      "Epoch [1270], val_loss: 4039.0591\n",
      "gradient norm: 741.3931655883789, minimum ratio: 0.79296875\n",
      "Epoch [1271], val_loss: 4050.5730\n",
      "gradient norm: 742.8383407592773, minimum ratio: 0.8046875\n",
      "Epoch [1272], val_loss: 4062.1074\n",
      "gradient norm: 744.2826080322266, minimum ratio: 0.7890625\n",
      "Epoch [1273], val_loss: 4073.6682\n",
      "gradient norm: 745.7406616210938, minimum ratio: 0.7890625\n",
      "Epoch [1274], val_loss: 4085.2542\n",
      "gradient norm: 747.1840286254883, minimum ratio: 0.8046875\n",
      "Epoch [1275], val_loss: 4096.8647\n",
      "gradient norm: 748.6358795166016, minimum ratio: 0.8046875\n",
      "Epoch [1276], val_loss: 4108.5015\n",
      "gradient norm: 750.0941925048828, minimum ratio: 0.77734375\n",
      "Epoch [1277], val_loss: 4120.1621\n",
      "gradient norm: 751.5612945556641, minimum ratio: 0.77734375\n",
      "Epoch [1278], val_loss: 4131.8472\n",
      "gradient norm: 753.0371322631836, minimum ratio: 0.79296875\n",
      "Epoch [1279], val_loss: 4143.5557\n",
      "gradient norm: 754.5003814697266, minimum ratio: 0.80078125\n",
      "Epoch [1280], val_loss: 4155.2905\n",
      "gradient norm: 755.9590072631836, minimum ratio: 0.78515625\n",
      "Epoch [1281], val_loss: 4167.0483\n",
      "gradient norm: 757.4117431640625, minimum ratio: 0.77734375\n",
      "Epoch [1282], val_loss: 4178.8330\n",
      "gradient norm: 758.8768081665039, minimum ratio: 0.80078125\n",
      "Epoch [1283], val_loss: 4190.6401\n",
      "gradient norm: 760.3622207641602, minimum ratio: 0.76171875\n",
      "Epoch [1284], val_loss: 4202.4712\n",
      "gradient norm: 761.8311767578125, minimum ratio: 0.796875\n",
      "Epoch [1285], val_loss: 4214.3262\n",
      "gradient norm: 763.3089447021484, minimum ratio: 0.76171875\n",
      "Epoch [1286], val_loss: 4226.2041\n",
      "gradient norm: 764.7819213867188, minimum ratio: 0.77734375\n",
      "Epoch [1287], val_loss: 4238.1050\n",
      "gradient norm: 766.2481460571289, minimum ratio: 0.765625\n",
      "Epoch [1288], val_loss: 4250.0273\n",
      "gradient norm: 767.740852355957, minimum ratio: 0.78515625\n",
      "Epoch [1289], val_loss: 4261.9761\n",
      "gradient norm: 769.2179336547852, minimum ratio: 0.79296875\n",
      "Epoch [1290], val_loss: 4273.9482\n",
      "gradient norm: 770.6974563598633, minimum ratio: 0.77734375\n",
      "Epoch [1291], val_loss: 4285.9458\n",
      "gradient norm: 772.1820602416992, minimum ratio: 0.80078125\n",
      "Epoch [1292], val_loss: 4297.9688\n",
      "gradient norm: 773.6548004150391, minimum ratio: 0.79296875\n",
      "Epoch [1293], val_loss: 4310.0171\n",
      "gradient norm: 775.1498336791992, minimum ratio: 0.796875\n",
      "Epoch [1294], val_loss: 4322.0908\n",
      "gradient norm: 776.625129699707, minimum ratio: 0.79296875\n",
      "Epoch [1295], val_loss: 4334.1880\n",
      "gradient norm: 778.1152496337891, minimum ratio: 0.7734375\n",
      "Epoch [1296], val_loss: 4346.3110\n",
      "gradient norm: 779.6178512573242, minimum ratio: 0.7890625\n",
      "Epoch [1297], val_loss: 4358.4561\n",
      "gradient norm: 781.1102905273438, minimum ratio: 0.78125\n",
      "Epoch [1298], val_loss: 4370.6260\n",
      "gradient norm: 782.6152191162109, minimum ratio: 0.76953125\n",
      "Epoch [1299], val_loss: 4382.8213\n",
      "gradient norm: 784.11572265625, minimum ratio: 0.80078125\n",
      "Epoch [1300], val_loss: 4395.0381\n",
      "gradient norm: 785.5999984741211, minimum ratio: 0.76953125\n",
      "Epoch [1301], val_loss: 4407.2793\n",
      "gradient norm: 787.0860290527344, minimum ratio: 0.80078125\n",
      "Epoch [1302], val_loss: 4419.5449\n",
      "gradient norm: 788.5832595825195, minimum ratio: 0.76171875\n",
      "Epoch [1303], val_loss: 4431.8330\n",
      "gradient norm: 790.0872497558594, minimum ratio: 0.79296875\n",
      "Epoch [1304], val_loss: 4444.1465\n",
      "gradient norm: 791.5960159301758, minimum ratio: 0.78125\n",
      "Epoch [1305], val_loss: 4456.4854\n",
      "gradient norm: 793.1196441650391, minimum ratio: 0.77734375\n",
      "Epoch [1306], val_loss: 4468.8511\n",
      "gradient norm: 794.6454849243164, minimum ratio: 0.76953125\n",
      "Epoch [1307], val_loss: 4481.2427\n",
      "gradient norm: 796.1576309204102, minimum ratio: 0.78125\n",
      "Epoch [1308], val_loss: 4493.6616\n",
      "gradient norm: 797.6775588989258, minimum ratio: 0.8046875\n",
      "Epoch [1309], val_loss: 4506.1050\n",
      "gradient norm: 799.2099609375, minimum ratio: 0.7890625\n",
      "Epoch [1310], val_loss: 4518.5737\n",
      "gradient norm: 800.7146453857422, minimum ratio: 0.79296875\n",
      "Epoch [1311], val_loss: 4531.0654\n",
      "gradient norm: 802.2104110717773, minimum ratio: 0.8125\n",
      "Epoch [1312], val_loss: 4543.5811\n",
      "gradient norm: 803.717414855957, minimum ratio: 0.80078125\n",
      "Epoch [1313], val_loss: 4556.1211\n",
      "gradient norm: 805.2490615844727, minimum ratio: 0.80078125\n",
      "Epoch [1314], val_loss: 4568.6870\n",
      "gradient norm: 806.7899322509766, minimum ratio: 0.7734375\n",
      "Epoch [1315], val_loss: 4581.2769\n",
      "gradient norm: 808.3063735961914, minimum ratio: 0.7890625\n",
      "Epoch [1316], val_loss: 4593.8940\n",
      "gradient norm: 809.8413238525391, minimum ratio: 0.7890625\n",
      "Epoch [1317], val_loss: 4606.5381\n",
      "gradient norm: 811.3800430297852, minimum ratio: 0.77734375\n",
      "Epoch [1318], val_loss: 4619.2061\n",
      "gradient norm: 812.9181900024414, minimum ratio: 0.80078125\n",
      "Epoch [1319], val_loss: 4631.8970\n",
      "gradient norm: 814.4520111083984, minimum ratio: 0.78125\n",
      "Epoch [1320], val_loss: 4644.6152\n",
      "gradient norm: 815.9952163696289, minimum ratio: 0.77734375\n",
      "Epoch [1321], val_loss: 4657.3560\n",
      "gradient norm: 817.5056686401367, minimum ratio: 0.77734375\n",
      "Epoch [1322], val_loss: 4670.1245\n",
      "gradient norm: 819.0512619018555, minimum ratio: 0.76953125\n",
      "Epoch [1323], val_loss: 4682.9189\n",
      "gradient norm: 820.6089172363281, minimum ratio: 0.78125\n",
      "Epoch [1324], val_loss: 4695.7383\n",
      "gradient norm: 822.1572875976562, minimum ratio: 0.79296875\n",
      "Epoch [1325], val_loss: 4708.5811\n",
      "gradient norm: 823.7180938720703, minimum ratio: 0.7890625\n",
      "Epoch [1326], val_loss: 4721.4458\n",
      "gradient norm: 825.2574615478516, minimum ratio: 0.78125\n",
      "Epoch [1327], val_loss: 4734.3364\n",
      "gradient norm: 826.7755355834961, minimum ratio: 0.78515625\n",
      "Epoch [1328], val_loss: 4747.2505\n",
      "gradient norm: 828.3094635009766, minimum ratio: 0.796875\n",
      "Epoch [1329], val_loss: 4760.1919\n",
      "gradient norm: 829.867301940918, minimum ratio: 0.7578125\n",
      "Epoch [1330], val_loss: 4773.1558\n",
      "gradient norm: 831.4020690917969, minimum ratio: 0.76953125\n",
      "Epoch [1331], val_loss: 4786.1436\n",
      "gradient norm: 832.9396438598633, minimum ratio: 0.7890625\n",
      "Epoch [1332], val_loss: 4799.1553\n",
      "gradient norm: 834.5108337402344, minimum ratio: 0.7734375\n",
      "Epoch [1333], val_loss: 4812.1924\n",
      "gradient norm: 836.0758666992188, minimum ratio: 0.8125\n",
      "Epoch [1334], val_loss: 4825.2534\n",
      "gradient norm: 837.6560287475586, minimum ratio: 0.77734375\n",
      "Epoch [1335], val_loss: 4838.3394\n",
      "gradient norm: 839.2028350830078, minimum ratio: 0.76953125\n",
      "Epoch [1336], val_loss: 4851.4541\n",
      "gradient norm: 840.7533569335938, minimum ratio: 0.7734375\n",
      "Epoch [1337], val_loss: 4864.5957\n",
      "gradient norm: 842.3339614868164, minimum ratio: 0.7890625\n",
      "Epoch [1338], val_loss: 4877.7632\n",
      "gradient norm: 843.8999786376953, minimum ratio: 0.796875\n",
      "Epoch [1339], val_loss: 4890.9561\n",
      "gradient norm: 845.4716796875, minimum ratio: 0.796875\n",
      "Epoch [1340], val_loss: 4904.1748\n",
      "gradient norm: 847.0204696655273, minimum ratio: 0.77734375\n",
      "Epoch [1341], val_loss: 4917.4189\n",
      "gradient norm: 848.5941543579102, minimum ratio: 0.7734375\n",
      "Epoch [1342], val_loss: 4930.6885\n",
      "gradient norm: 850.1615295410156, minimum ratio: 0.77734375\n",
      "Epoch [1343], val_loss: 4943.9839\n",
      "gradient norm: 851.7346115112305, minimum ratio: 0.7734375\n",
      "Epoch [1344], val_loss: 4957.3047\n",
      "gradient norm: 853.319221496582, minimum ratio: 0.796875\n",
      "Epoch [1345], val_loss: 4970.6548\n",
      "gradient norm: 854.9037475585938, minimum ratio: 0.7890625\n",
      "Epoch [1346], val_loss: 4984.0303\n",
      "gradient norm: 856.4628982543945, minimum ratio: 0.7734375\n",
      "Epoch [1347], val_loss: 4997.4326\n",
      "gradient norm: 858.0504684448242, minimum ratio: 0.76953125\n",
      "Epoch [1348], val_loss: 5010.8623\n",
      "gradient norm: 859.6340179443359, minimum ratio: 0.7890625\n",
      "Epoch [1349], val_loss: 5024.3169\n",
      "gradient norm: 861.2373275756836, minimum ratio: 0.7734375\n",
      "Epoch [1350], val_loss: 5037.7979\n",
      "gradient norm: 862.849235534668, minimum ratio: 0.78515625\n",
      "Epoch [1351], val_loss: 5051.3042\n",
      "gradient norm: 864.4239654541016, minimum ratio: 0.78515625\n",
      "Epoch [1352], val_loss: 5064.8369\n",
      "gradient norm: 866.0026245117188, minimum ratio: 0.77734375\n",
      "Epoch [1353], val_loss: 5078.3955\n",
      "gradient norm: 867.59326171875, minimum ratio: 0.79296875\n",
      "Epoch [1354], val_loss: 5091.9810\n",
      "gradient norm: 869.2056045532227, minimum ratio: 0.78515625\n",
      "Epoch [1355], val_loss: 5105.5923\n",
      "gradient norm: 870.8082962036133, minimum ratio: 0.796875\n",
      "Epoch [1356], val_loss: 5119.2275\n",
      "gradient norm: 872.3945770263672, minimum ratio: 0.77734375\n",
      "Epoch [1357], val_loss: 5132.8906\n",
      "gradient norm: 874.0123825073242, minimum ratio: 0.7734375\n",
      "Epoch [1358], val_loss: 5146.5786\n",
      "gradient norm: 875.619514465332, minimum ratio: 0.7890625\n",
      "Epoch [1359], val_loss: 5160.2930\n",
      "gradient norm: 877.2126541137695, minimum ratio: 0.78125\n",
      "Epoch [1360], val_loss: 5174.0322\n",
      "gradient norm: 878.837890625, minimum ratio: 0.7734375\n",
      "Epoch [1361], val_loss: 5187.7979\n",
      "gradient norm: 880.4411773681641, minimum ratio: 0.78515625\n",
      "Epoch [1362], val_loss: 5201.5938\n",
      "gradient norm: 882.0683364868164, minimum ratio: 0.79296875\n",
      "Epoch [1363], val_loss: 5215.4116\n",
      "gradient norm: 883.6698913574219, minimum ratio: 0.78125\n",
      "Epoch [1364], val_loss: 5229.2583\n",
      "gradient norm: 885.2512130737305, minimum ratio: 0.78125\n",
      "Epoch [1365], val_loss: 5243.1318\n",
      "gradient norm: 886.8913345336914, minimum ratio: 0.76953125\n",
      "Epoch [1366], val_loss: 5257.0332\n",
      "gradient norm: 888.5135650634766, minimum ratio: 0.78125\n",
      "Epoch [1367], val_loss: 5270.9600\n",
      "gradient norm: 890.1309280395508, minimum ratio: 0.77734375\n",
      "Epoch [1368], val_loss: 5284.9136\n",
      "gradient norm: 891.7689743041992, minimum ratio: 0.7734375\n",
      "Epoch [1369], val_loss: 5298.8950\n",
      "gradient norm: 893.3823852539062, minimum ratio: 0.79296875\n",
      "Epoch [1370], val_loss: 5312.9048\n",
      "gradient norm: 894.969123840332, minimum ratio: 0.7890625\n",
      "Epoch [1371], val_loss: 5326.9424\n",
      "gradient norm: 896.5802612304688, minimum ratio: 0.77734375\n",
      "Epoch [1372], val_loss: 5341.0049\n",
      "gradient norm: 898.2036666870117, minimum ratio: 0.7890625\n",
      "Epoch [1373], val_loss: 5355.0933\n",
      "gradient norm: 899.8314514160156, minimum ratio: 0.78515625\n",
      "Epoch [1374], val_loss: 5369.2090\n",
      "gradient norm: 901.4546127319336, minimum ratio: 0.78125\n",
      "Epoch [1375], val_loss: 5383.3511\n",
      "gradient norm: 903.1060409545898, minimum ratio: 0.79296875\n",
      "Epoch [1376], val_loss: 5397.5200\n",
      "gradient norm: 904.7592544555664, minimum ratio: 0.79296875\n",
      "Epoch [1377], val_loss: 5411.7144\n",
      "gradient norm: 906.3944702148438, minimum ratio: 0.78125\n",
      "Epoch [1378], val_loss: 5425.9370\n",
      "gradient norm: 908.0400009155273, minimum ratio: 0.796875\n",
      "Epoch [1379], val_loss: 5440.1826\n",
      "gradient norm: 909.6887893676758, minimum ratio: 0.79296875\n",
      "Epoch [1380], val_loss: 5454.4561\n",
      "gradient norm: 911.3461303710938, minimum ratio: 0.78515625\n",
      "Epoch [1381], val_loss: 5468.7607\n",
      "gradient norm: 912.9859237670898, minimum ratio: 0.7890625\n",
      "Epoch [1382], val_loss: 5483.0947\n",
      "gradient norm: 914.6358337402344, minimum ratio: 0.77734375\n",
      "Epoch [1383], val_loss: 5497.4570\n",
      "gradient norm: 916.2800216674805, minimum ratio: 0.7890625\n",
      "Epoch [1384], val_loss: 5511.8457\n",
      "gradient norm: 917.9259490966797, minimum ratio: 0.78515625\n",
      "Epoch [1385], val_loss: 5526.2607\n",
      "gradient norm: 919.5727157592773, minimum ratio: 0.765625\n",
      "Epoch [1386], val_loss: 5540.7002\n",
      "gradient norm: 921.2181091308594, minimum ratio: 0.79296875\n",
      "Epoch [1387], val_loss: 5555.1650\n",
      "gradient norm: 922.8807754516602, minimum ratio: 0.77734375\n",
      "Epoch [1388], val_loss: 5569.6582\n",
      "gradient norm: 924.5475845336914, minimum ratio: 0.7890625\n",
      "Epoch [1389], val_loss: 5584.1787\n",
      "gradient norm: 926.2031707763672, minimum ratio: 0.78515625\n",
      "Epoch [1390], val_loss: 5598.7251\n",
      "gradient norm: 927.8672637939453, minimum ratio: 0.79296875\n",
      "Epoch [1391], val_loss: 5613.2964\n",
      "gradient norm: 929.5380630493164, minimum ratio: 0.79296875\n",
      "Epoch [1392], val_loss: 5627.8984\n",
      "gradient norm: 931.1741333007812, minimum ratio: 0.7734375\n",
      "Epoch [1393], val_loss: 5642.5254\n",
      "gradient norm: 932.8470993041992, minimum ratio: 0.7734375\n",
      "Epoch [1394], val_loss: 5657.1807\n",
      "gradient norm: 934.52392578125, minimum ratio: 0.81640625\n",
      "Epoch [1395], val_loss: 5671.8633\n",
      "gradient norm: 936.1987457275391, minimum ratio: 0.7890625\n",
      "Epoch [1396], val_loss: 5686.5747\n",
      "gradient norm: 937.8625717163086, minimum ratio: 0.81640625\n",
      "Epoch [1397], val_loss: 5701.3110\n",
      "gradient norm: 939.5415115356445, minimum ratio: 0.81640625\n",
      "Epoch [1398], val_loss: 5716.0771\n",
      "gradient norm: 941.2128601074219, minimum ratio: 0.78125\n",
      "Epoch [1399], val_loss: 5730.8711\n",
      "gradient norm: 942.8985900878906, minimum ratio: 0.7890625\n",
      "Epoch [1400], val_loss: 5745.6919\n",
      "gradient norm: 944.5963668823242, minimum ratio: 0.7890625\n",
      "Epoch [1401], val_loss: 5760.5405\n",
      "gradient norm: 946.2739105224609, minimum ratio: 0.8046875\n",
      "Epoch [1402], val_loss: 5775.4165\n",
      "gradient norm: 947.9394607543945, minimum ratio: 0.7734375\n",
      "Epoch [1403], val_loss: 5790.3193\n",
      "gradient norm: 949.6211853027344, minimum ratio: 0.77734375\n",
      "Epoch [1404], val_loss: 5805.2490\n",
      "gradient norm: 951.3261260986328, minimum ratio: 0.76953125\n",
      "Epoch [1405], val_loss: 5820.2070\n",
      "gradient norm: 953.0328216552734, minimum ratio: 0.78125\n",
      "Epoch [1406], val_loss: 5835.1924\n",
      "gradient norm: 954.7188415527344, minimum ratio: 0.78125\n",
      "Epoch [1407], val_loss: 5850.2056\n",
      "gradient norm: 956.4161529541016, minimum ratio: 0.8046875\n",
      "Epoch [1408], val_loss: 5865.2490\n",
      "gradient norm: 958.0997543334961, minimum ratio: 0.79296875\n",
      "Epoch [1409], val_loss: 5880.3188\n",
      "gradient norm: 959.7456512451172, minimum ratio: 0.796875\n",
      "Epoch [1410], val_loss: 5895.4175\n",
      "gradient norm: 961.4559707641602, minimum ratio: 0.78515625\n",
      "Epoch [1411], val_loss: 5910.5391\n",
      "gradient norm: 963.1597747802734, minimum ratio: 0.78515625\n",
      "Epoch [1412], val_loss: 5925.6914\n",
      "gradient norm: 964.8621673583984, minimum ratio: 0.79296875\n",
      "Epoch [1413], val_loss: 5940.8745\n",
      "gradient norm: 966.5844421386719, minimum ratio: 0.78125\n",
      "Epoch [1414], val_loss: 5956.0840\n",
      "gradient norm: 968.2871170043945, minimum ratio: 0.77734375\n",
      "Epoch [1415], val_loss: 5971.3193\n",
      "gradient norm: 969.9663467407227, minimum ratio: 0.796875\n",
      "Epoch [1416], val_loss: 5986.5850\n",
      "gradient norm: 971.6884613037109, minimum ratio: 0.78515625\n",
      "Epoch [1417], val_loss: 6001.8823\n",
      "gradient norm: 973.3925933837891, minimum ratio: 0.796875\n",
      "Epoch [1418], val_loss: 6017.2041\n",
      "gradient norm: 975.1095352172852, minimum ratio: 0.79296875\n",
      "Epoch [1419], val_loss: 6032.5562\n",
      "gradient norm: 976.8289947509766, minimum ratio: 0.78125\n",
      "Epoch [1420], val_loss: 6047.9346\n",
      "gradient norm: 978.5524139404297, minimum ratio: 0.79296875\n",
      "Epoch [1421], val_loss: 6063.3394\n",
      "gradient norm: 980.247673034668, minimum ratio: 0.76953125\n",
      "Epoch [1422], val_loss: 6078.7773\n",
      "gradient norm: 981.9727783203125, minimum ratio: 0.78125\n",
      "Epoch [1423], val_loss: 6094.2402\n",
      "gradient norm: 983.6970901489258, minimum ratio: 0.76953125\n",
      "Epoch [1424], val_loss: 6109.7334\n",
      "gradient norm: 985.3987503051758, minimum ratio: 0.7890625\n",
      "Epoch [1425], val_loss: 6125.2510\n",
      "gradient norm: 987.1427841186523, minimum ratio: 0.78515625\n",
      "Epoch [1426], val_loss: 6140.7969\n",
      "gradient norm: 988.8766479492188, minimum ratio: 0.76953125\n",
      "Epoch [1427], val_loss: 6156.3701\n",
      "gradient norm: 990.6240692138672, minimum ratio: 0.80859375\n",
      "Epoch [1428], val_loss: 6171.9697\n",
      "gradient norm: 992.3470077514648, minimum ratio: 0.8046875\n",
      "Epoch [1429], val_loss: 6187.5967\n",
      "gradient norm: 994.0859832763672, minimum ratio: 0.7890625\n",
      "Epoch [1430], val_loss: 6203.2505\n",
      "gradient norm: 995.827392578125, minimum ratio: 0.79296875\n",
      "Epoch [1431], val_loss: 6218.9341\n",
      "gradient norm: 997.5662841796875, minimum ratio: 0.79296875\n",
      "Epoch [1432], val_loss: 6234.6450\n",
      "gradient norm: 999.309928894043, minimum ratio: 0.80078125\n",
      "Epoch [1433], val_loss: 6250.3853\n",
      "gradient norm: 1001.0426406860352, minimum ratio: 0.78125\n",
      "Epoch [1434], val_loss: 6266.1562\n",
      "gradient norm: 1002.7871551513672, minimum ratio: 0.78125\n",
      "Epoch [1435], val_loss: 6281.9536\n",
      "gradient norm: 1004.5382843017578, minimum ratio: 0.79296875\n",
      "Epoch [1436], val_loss: 6297.7788\n",
      "gradient norm: 1006.2887878417969, minimum ratio: 0.81640625\n",
      "Epoch [1437], val_loss: 6313.6353\n",
      "gradient norm: 1008.0050277709961, minimum ratio: 0.796875\n",
      "Epoch [1438], val_loss: 6329.5176\n",
      "gradient norm: 1009.7446670532227, minimum ratio: 0.79296875\n",
      "Epoch [1439], val_loss: 6345.4321\n",
      "gradient norm: 1011.5020751953125, minimum ratio: 0.7890625\n",
      "Epoch [1440], val_loss: 6361.3745\n",
      "gradient norm: 1013.2591247558594, minimum ratio: 0.76953125\n",
      "Epoch [1441], val_loss: 6377.3501\n",
      "gradient norm: 1014.9721221923828, minimum ratio: 0.7734375\n",
      "Epoch [1442], val_loss: 6393.3501\n",
      "gradient norm: 1016.734375, minimum ratio: 0.78515625\n",
      "Epoch [1443], val_loss: 6409.3804\n",
      "gradient norm: 1018.5118026733398, minimum ratio: 0.80078125\n",
      "Epoch [1444], val_loss: 6425.4370\n",
      "gradient norm: 1020.2691650390625, minimum ratio: 0.78515625\n",
      "Epoch [1445], val_loss: 6441.5244\n",
      "gradient norm: 1022.0276641845703, minimum ratio: 0.76953125\n",
      "Epoch [1446], val_loss: 6457.6362\n",
      "gradient norm: 1023.7812423706055, minimum ratio: 0.7890625\n",
      "Epoch [1447], val_loss: 6473.7769\n",
      "gradient norm: 1025.5651397705078, minimum ratio: 0.79296875\n",
      "Epoch [1448], val_loss: 6489.9443\n",
      "gradient norm: 1027.3356704711914, minimum ratio: 0.765625\n",
      "Epoch [1449], val_loss: 6506.1377\n",
      "gradient norm: 1029.1091918945312, minimum ratio: 0.80859375\n",
      "Epoch [1450], val_loss: 6522.3594\n",
      "gradient norm: 1030.903907775879, minimum ratio: 0.77734375\n",
      "Epoch [1451], val_loss: 6538.6089\n",
      "gradient norm: 1032.6429901123047, minimum ratio: 0.7890625\n",
      "Epoch [1452], val_loss: 6554.8892\n",
      "gradient norm: 1034.4042663574219, minimum ratio: 0.80078125\n",
      "Epoch [1453], val_loss: 6571.1997\n",
      "gradient norm: 1036.1985626220703, minimum ratio: 0.80859375\n",
      "Epoch [1454], val_loss: 6587.5400\n",
      "gradient norm: 1037.9481201171875, minimum ratio: 0.77734375\n",
      "Epoch [1455], val_loss: 6603.9116\n",
      "gradient norm: 1039.7343521118164, minimum ratio: 0.7890625\n",
      "Epoch [1456], val_loss: 6620.3096\n",
      "gradient norm: 1041.5080642700195, minimum ratio: 0.78515625\n",
      "Epoch [1457], val_loss: 6636.7383\n",
      "gradient norm: 1043.283103942871, minimum ratio: 0.78125\n",
      "Epoch [1458], val_loss: 6653.1978\n",
      "gradient norm: 1045.0760040283203, minimum ratio: 0.78125\n",
      "Epoch [1459], val_loss: 6669.6919\n",
      "gradient norm: 1046.8674240112305, minimum ratio: 0.79296875\n",
      "Epoch [1460], val_loss: 6686.2158\n",
      "gradient norm: 1048.6227340698242, minimum ratio: 0.7890625\n",
      "Epoch [1461], val_loss: 6702.7705\n",
      "gradient norm: 1050.4204177856445, minimum ratio: 0.8125\n",
      "Epoch [1462], val_loss: 6719.3545\n",
      "gradient norm: 1052.2231826782227, minimum ratio: 0.78515625\n",
      "Epoch [1463], val_loss: 6735.9663\n",
      "gradient norm: 1054.0383071899414, minimum ratio: 0.7734375\n",
      "Epoch [1464], val_loss: 6752.6084\n",
      "gradient norm: 1055.8445053100586, minimum ratio: 0.7734375\n",
      "Epoch [1465], val_loss: 6769.2783\n",
      "gradient norm: 1057.6370162963867, minimum ratio: 0.796875\n",
      "Epoch [1466], val_loss: 6785.9780\n",
      "gradient norm: 1059.4273834228516, minimum ratio: 0.78125\n",
      "Epoch [1467], val_loss: 6802.7026\n",
      "gradient norm: 1061.240821838379, minimum ratio: 0.76953125\n",
      "Epoch [1468], val_loss: 6819.4590\n",
      "gradient norm: 1063.0273742675781, minimum ratio: 0.796875\n",
      "Epoch [1469], val_loss: 6836.2456\n",
      "gradient norm: 1064.841667175293, minimum ratio: 0.8125\n",
      "Epoch [1470], val_loss: 6853.0610\n",
      "gradient norm: 1066.637451171875, minimum ratio: 0.7734375\n",
      "Epoch [1471], val_loss: 6869.9077\n",
      "gradient norm: 1068.4673080444336, minimum ratio: 0.80859375\n",
      "Epoch [1472], val_loss: 6886.7803\n",
      "gradient norm: 1070.2847442626953, minimum ratio: 0.78125\n",
      "Epoch [1473], val_loss: 6903.6865\n",
      "gradient norm: 1072.105339050293, minimum ratio: 0.7734375\n",
      "Epoch [1474], val_loss: 6920.6240\n",
      "gradient norm: 1073.9418640136719, minimum ratio: 0.7890625\n",
      "Epoch [1475], val_loss: 6937.5928\n",
      "gradient norm: 1075.725845336914, minimum ratio: 0.76171875\n",
      "Epoch [1476], val_loss: 6954.5864\n",
      "gradient norm: 1077.5651092529297, minimum ratio: 0.78515625\n",
      "Epoch [1477], val_loss: 6971.6099\n",
      "gradient norm: 1079.3703918457031, minimum ratio: 0.7890625\n",
      "Epoch [1478], val_loss: 6988.6675\n",
      "gradient norm: 1081.2146911621094, minimum ratio: 0.796875\n",
      "Epoch [1479], val_loss: 7005.7510\n",
      "gradient norm: 1083.0443267822266, minimum ratio: 0.79296875\n",
      "Epoch [1480], val_loss: 7022.8667\n",
      "gradient norm: 1084.870849609375, minimum ratio: 0.78125\n",
      "Epoch [1481], val_loss: 7040.0083\n",
      "gradient norm: 1086.6915969848633, minimum ratio: 0.78515625\n",
      "Epoch [1482], val_loss: 7057.1826\n",
      "gradient norm: 1088.4976501464844, minimum ratio: 0.78125\n",
      "Epoch [1483], val_loss: 7074.3906\n",
      "gradient norm: 1090.3118438720703, minimum ratio: 0.78125\n",
      "Epoch [1484], val_loss: 7091.6201\n",
      "gradient norm: 1092.1655654907227, minimum ratio: 0.78515625\n",
      "Epoch [1485], val_loss: 7108.8794\n",
      "gradient norm: 1094.0212783813477, minimum ratio: 0.7890625\n",
      "Epoch [1486], val_loss: 7126.1665\n",
      "gradient norm: 1095.8479385375977, minimum ratio: 0.80859375\n",
      "Epoch [1487], val_loss: 7143.4902\n",
      "gradient norm: 1097.682601928711, minimum ratio: 0.79296875\n",
      "Epoch [1488], val_loss: 7160.8418\n",
      "gradient norm: 1099.516502380371, minimum ratio: 0.80078125\n",
      "Epoch [1489], val_loss: 7178.2241\n",
      "gradient norm: 1101.3630447387695, minimum ratio: 0.78515625\n",
      "Epoch [1490], val_loss: 7195.6382\n",
      "gradient norm: 1103.1880187988281, minimum ratio: 0.7734375\n",
      "Epoch [1491], val_loss: 7213.0825\n",
      "gradient norm: 1105.0295867919922, minimum ratio: 0.7890625\n",
      "Epoch [1492], val_loss: 7230.5576\n",
      "gradient norm: 1106.8862609863281, minimum ratio: 0.796875\n",
      "Epoch [1493], val_loss: 7248.0601\n",
      "gradient norm: 1108.7254943847656, minimum ratio: 0.78125\n",
      "Epoch [1494], val_loss: 7265.5952\n",
      "gradient norm: 1110.6090927124023, minimum ratio: 0.80078125\n",
      "Epoch [1495], val_loss: 7283.1602\n",
      "gradient norm: 1112.452896118164, minimum ratio: 0.78515625\n",
      "Epoch [1496], val_loss: 7300.7510\n",
      "gradient norm: 1114.2966384887695, minimum ratio: 0.77734375\n",
      "Epoch [1497], val_loss: 7318.3765\n",
      "gradient norm: 1116.1637344360352, minimum ratio: 0.7890625\n",
      "Epoch [1498], val_loss: 7336.0352\n",
      "gradient norm: 1118.009162902832, minimum ratio: 0.79296875\n",
      "Epoch [1499], val_loss: 7353.7231\n",
      "gradient norm: 1119.892677307129, minimum ratio: 0.76953125\n",
      "Epoch [1500], val_loss: 7371.4448\n",
      "gradient norm: 1121.7615814208984, minimum ratio: 0.78515625\n",
      "Epoch [1501], val_loss: 7389.1982\n",
      "gradient norm: 1123.6310195922852, minimum ratio: 0.79296875\n",
      "Epoch [1502], val_loss: 7406.9810\n",
      "gradient norm: 1125.5209121704102, minimum ratio: 0.78125\n",
      "Epoch [1503], val_loss: 7424.7939\n",
      "gradient norm: 1127.3769073486328, minimum ratio: 0.79296875\n",
      "Epoch [1504], val_loss: 7442.6367\n",
      "gradient norm: 1129.2224349975586, minimum ratio: 0.79296875\n",
      "Epoch [1505], val_loss: 7460.5083\n",
      "gradient norm: 1131.1171493530273, minimum ratio: 0.78515625\n",
      "Epoch [1506], val_loss: 7478.4126\n",
      "gradient norm: 1133.0144119262695, minimum ratio: 0.7890625\n",
      "Epoch [1507], val_loss: 7496.3467\n",
      "gradient norm: 1134.8516540527344, minimum ratio: 0.796875\n",
      "Epoch [1508], val_loss: 7514.3140\n",
      "gradient norm: 1136.723533630371, minimum ratio: 0.765625\n",
      "Epoch [1509], val_loss: 7532.3066\n",
      "gradient norm: 1138.5912399291992, minimum ratio: 0.78125\n",
      "Epoch [1510], val_loss: 7550.3311\n",
      "gradient norm: 1140.4651794433594, minimum ratio: 0.76171875\n",
      "Epoch [1511], val_loss: 7568.3882\n",
      "gradient norm: 1142.371810913086, minimum ratio: 0.765625\n",
      "Epoch [1512], val_loss: 7586.4707\n",
      "gradient norm: 1144.2792434692383, minimum ratio: 0.80859375\n",
      "Epoch [1513], val_loss: 7604.5918\n",
      "gradient norm: 1146.1423797607422, minimum ratio: 0.76953125\n",
      "Epoch [1514], val_loss: 7622.7432\n",
      "gradient norm: 1148.0246047973633, minimum ratio: 0.7890625\n",
      "Epoch [1515], val_loss: 7640.9248\n",
      "gradient norm: 1149.926170349121, minimum ratio: 0.78125\n",
      "Epoch [1516], val_loss: 7659.1377\n",
      "gradient norm: 1151.842658996582, minimum ratio: 0.78515625\n",
      "Epoch [1517], val_loss: 7677.3799\n",
      "gradient norm: 1153.758804321289, minimum ratio: 0.78125\n",
      "Epoch [1518], val_loss: 7695.6509\n",
      "gradient norm: 1155.6502685546875, minimum ratio: 0.8046875\n",
      "Epoch [1519], val_loss: 7713.9517\n",
      "gradient norm: 1157.5712814331055, minimum ratio: 0.7890625\n",
      "Epoch [1520], val_loss: 7732.2832\n",
      "gradient norm: 1159.4782104492188, minimum ratio: 0.78515625\n",
      "Epoch [1521], val_loss: 7750.6426\n",
      "gradient norm: 1161.414695739746, minimum ratio: 0.78515625\n",
      "Epoch [1522], val_loss: 7769.0366\n",
      "gradient norm: 1163.2913970947266, minimum ratio: 0.796875\n",
      "Epoch [1523], val_loss: 7787.4600\n",
      "gradient norm: 1165.188133239746, minimum ratio: 0.78515625\n",
      "Epoch [1524], val_loss: 7805.9165\n",
      "gradient norm: 1167.1014556884766, minimum ratio: 0.796875\n",
      "Epoch [1525], val_loss: 7824.4043\n",
      "gradient norm: 1168.995475769043, minimum ratio: 0.77734375\n",
      "Epoch [1526], val_loss: 7842.9268\n",
      "gradient norm: 1170.9108123779297, minimum ratio: 0.7890625\n",
      "Epoch [1527], val_loss: 7861.4824\n",
      "gradient norm: 1172.8340911865234, minimum ratio: 0.76953125\n",
      "Epoch [1528], val_loss: 7880.0635\n",
      "gradient norm: 1174.772445678711, minimum ratio: 0.78125\n",
      "Epoch [1529], val_loss: 7898.6816\n",
      "gradient norm: 1176.6122741699219, minimum ratio: 0.78125\n",
      "Epoch [1530], val_loss: 7917.3311\n",
      "gradient norm: 1178.5371856689453, minimum ratio: 0.78515625\n",
      "Epoch [1531], val_loss: 7936.0107\n",
      "gradient norm: 1180.4497451782227, minimum ratio: 0.77734375\n",
      "Epoch [1532], val_loss: 7954.7168\n",
      "gradient norm: 1182.3367614746094, minimum ratio: 0.7890625\n",
      "Epoch [1533], val_loss: 7973.4565\n",
      "gradient norm: 1184.2674942016602, minimum ratio: 0.78515625\n",
      "Epoch [1534], val_loss: 7992.2285\n",
      "gradient norm: 1186.2169876098633, minimum ratio: 0.78125\n",
      "Epoch [1535], val_loss: 8011.0352\n",
      "gradient norm: 1188.1489715576172, minimum ratio: 0.7890625\n",
      "Epoch [1536], val_loss: 8029.8765\n",
      "gradient norm: 1190.0854949951172, minimum ratio: 0.77734375\n",
      "Epoch [1537], val_loss: 8048.7476\n",
      "gradient norm: 1192.0121612548828, minimum ratio: 0.76953125\n",
      "Epoch [1538], val_loss: 8067.6543\n",
      "gradient norm: 1193.9379196166992, minimum ratio: 0.78515625\n",
      "Epoch [1539], val_loss: 8086.5884\n",
      "gradient norm: 1195.8647537231445, minimum ratio: 0.7734375\n",
      "Epoch [1540], val_loss: 8105.5557\n",
      "gradient norm: 1197.827507019043, minimum ratio: 0.76953125\n",
      "Epoch [1541], val_loss: 8124.5581\n",
      "gradient norm: 1199.7719039916992, minimum ratio: 0.77734375\n",
      "Epoch [1542], val_loss: 8143.5908\n",
      "gradient norm: 1201.7062301635742, minimum ratio: 0.78515625\n",
      "Epoch [1543], val_loss: 8162.6548\n",
      "gradient norm: 1203.6749954223633, minimum ratio: 0.78515625\n",
      "Epoch [1544], val_loss: 8181.7524\n",
      "gradient norm: 1205.630615234375, minimum ratio: 0.765625\n",
      "Epoch [1545], val_loss: 8200.8838\n",
      "gradient norm: 1207.5560150146484, minimum ratio: 0.78125\n",
      "Epoch [1546], val_loss: 8220.0459\n",
      "gradient norm: 1209.512939453125, minimum ratio: 0.77734375\n",
      "Epoch [1547], val_loss: 8239.2344\n",
      "gradient norm: 1211.4689178466797, minimum ratio: 0.76171875\n",
      "Epoch [1548], val_loss: 8258.4531\n",
      "gradient norm: 1213.4458770751953, minimum ratio: 0.79296875\n",
      "Epoch [1549], val_loss: 8277.7012\n",
      "gradient norm: 1215.3892135620117, minimum ratio: 0.77734375\n",
      "Epoch [1550], val_loss: 8296.9805\n",
      "gradient norm: 1217.3694305419922, minimum ratio: 0.79296875\n",
      "Epoch [1551], val_loss: 8316.2891\n",
      "gradient norm: 1219.3150177001953, minimum ratio: 0.78125\n",
      "Epoch [1552], val_loss: 8335.6338\n",
      "gradient norm: 1221.2852020263672, minimum ratio: 0.75390625\n",
      "Epoch [1553], val_loss: 8355.0068\n",
      "gradient norm: 1223.2640533447266, minimum ratio: 0.78125\n",
      "Epoch [1554], val_loss: 8374.4150\n",
      "gradient norm: 1225.2522430419922, minimum ratio: 0.77734375\n",
      "Epoch [1555], val_loss: 8393.8604\n",
      "gradient norm: 1227.2038497924805, minimum ratio: 0.80078125\n",
      "Epoch [1556], val_loss: 8413.3369\n",
      "gradient norm: 1229.1574401855469, minimum ratio: 0.79296875\n",
      "Epoch [1557], val_loss: 8432.8379\n",
      "gradient norm: 1231.1319198608398, minimum ratio: 0.78515625\n",
      "Epoch [1558], val_loss: 8452.3750\n",
      "gradient norm: 1233.0982055664062, minimum ratio: 0.765625\n",
      "Epoch [1559], val_loss: 8471.9443\n",
      "gradient norm: 1235.0962142944336, minimum ratio: 0.7890625\n",
      "Epoch [1560], val_loss: 8491.5391\n",
      "gradient norm: 1237.0292892456055, minimum ratio: 0.77734375\n",
      "Epoch [1561], val_loss: 8511.1631\n",
      "gradient norm: 1238.9996871948242, minimum ratio: 0.79296875\n",
      "Epoch [1562], val_loss: 8530.8271\n",
      "gradient norm: 1240.9831848144531, minimum ratio: 0.78125\n",
      "Epoch [1563], val_loss: 8550.5186\n",
      "gradient norm: 1242.9738540649414, minimum ratio: 0.76953125\n",
      "Epoch [1564], val_loss: 8570.2451\n",
      "gradient norm: 1244.9595565795898, minimum ratio: 0.7734375\n",
      "Epoch [1565], val_loss: 8590.0068\n",
      "gradient norm: 1246.968849182129, minimum ratio: 0.77734375\n",
      "Epoch [1566], val_loss: 8609.8018\n",
      "gradient norm: 1248.9385299682617, minimum ratio: 0.7890625\n",
      "Epoch [1567], val_loss: 8629.6250\n",
      "gradient norm: 1250.9125213623047, minimum ratio: 0.77734375\n",
      "Epoch [1568], val_loss: 8649.4775\n",
      "gradient norm: 1252.9263000488281, minimum ratio: 0.8046875\n",
      "Epoch [1569], val_loss: 8669.3691\n",
      "gradient norm: 1254.8825225830078, minimum ratio: 0.77734375\n",
      "Epoch [1570], val_loss: 8689.2930\n",
      "gradient norm: 1256.8690185546875, minimum ratio: 0.765625\n",
      "Epoch [1571], val_loss: 8709.2500\n",
      "gradient norm: 1258.8730773925781, minimum ratio: 0.77734375\n",
      "Epoch [1572], val_loss: 8729.2373\n",
      "gradient norm: 1260.8795700073242, minimum ratio: 0.78125\n",
      "Epoch [1573], val_loss: 8749.2598\n",
      "gradient norm: 1262.8900604248047, minimum ratio: 0.78515625\n",
      "Epoch [1574], val_loss: 8769.3135\n",
      "gradient norm: 1264.8995742797852, minimum ratio: 0.78125\n",
      "Epoch [1575], val_loss: 8789.4033\n",
      "gradient norm: 1266.8829193115234, minimum ratio: 0.78125\n",
      "Epoch [1576], val_loss: 8809.5283\n",
      "gradient norm: 1268.8753204345703, minimum ratio: 0.78515625\n",
      "Epoch [1577], val_loss: 8829.6816\n",
      "gradient norm: 1270.845588684082, minimum ratio: 0.7890625\n",
      "Epoch [1578], val_loss: 8849.8721\n",
      "gradient norm: 1272.8806762695312, minimum ratio: 0.78515625\n",
      "Epoch [1579], val_loss: 8870.0908\n",
      "gradient norm: 1274.9168701171875, minimum ratio: 0.79296875\n",
      "Epoch [1580], val_loss: 8890.3447\n",
      "gradient norm: 1276.9170684814453, minimum ratio: 0.76171875\n",
      "Epoch [1581], val_loss: 8910.6309\n",
      "gradient norm: 1278.8714447021484, minimum ratio: 0.7734375\n",
      "Epoch [1582], val_loss: 8930.9463\n",
      "gradient norm: 1280.8913650512695, minimum ratio: 0.78515625\n",
      "Epoch [1583], val_loss: 8951.2998\n",
      "gradient norm: 1282.9043655395508, minimum ratio: 0.79296875\n",
      "Epoch [1584], val_loss: 8971.6865\n",
      "gradient norm: 1284.951042175293, minimum ratio: 0.7890625\n",
      "Epoch [1585], val_loss: 8992.1104\n",
      "gradient norm: 1287.0002670288086, minimum ratio: 0.78125\n",
      "Epoch [1586], val_loss: 9012.5596\n",
      "gradient norm: 1288.9947662353516, minimum ratio: 0.78515625\n",
      "Epoch [1587], val_loss: 9033.0449\n",
      "gradient norm: 1291.0469207763672, minimum ratio: 0.78515625\n",
      "Epoch [1588], val_loss: 9053.5615\n",
      "gradient norm: 1293.1008682250977, minimum ratio: 0.796875\n",
      "Epoch [1589], val_loss: 9074.1133\n",
      "gradient norm: 1295.1571960449219, minimum ratio: 0.78515625\n",
      "Epoch [1590], val_loss: 9094.7012\n",
      "gradient norm: 1297.1942138671875, minimum ratio: 0.80859375\n",
      "Epoch [1591], val_loss: 9115.3203\n",
      "gradient norm: 1299.219108581543, minimum ratio: 0.7890625\n",
      "Epoch [1592], val_loss: 9135.9717\n",
      "gradient norm: 1301.2404861450195, minimum ratio: 0.765625\n",
      "Epoch [1593], val_loss: 9156.6562\n",
      "gradient norm: 1303.304428100586, minimum ratio: 0.77734375\n",
      "Epoch [1594], val_loss: 9177.3770\n",
      "gradient norm: 1305.3219299316406, minimum ratio: 0.78125\n",
      "Epoch [1595], val_loss: 9198.1318\n",
      "gradient norm: 1307.390022277832, minimum ratio: 0.79296875\n",
      "Epoch [1596], val_loss: 9218.9170\n",
      "gradient norm: 1309.381492614746, minimum ratio: 0.7734375\n",
      "Epoch [1597], val_loss: 9239.7354\n",
      "gradient norm: 1311.4532012939453, minimum ratio: 0.78515625\n",
      "Epoch [1598], val_loss: 9260.5820\n",
      "gradient norm: 1313.4468536376953, minimum ratio: 0.79296875\n",
      "Epoch [1599], val_loss: 9281.4629\n",
      "gradient norm: 1315.5213241577148, minimum ratio: 0.7890625\n",
      "Epoch [1600], val_loss: 9302.3750\n",
      "gradient norm: 1317.5423278808594, minimum ratio: 0.7890625\n",
      "Epoch [1601], val_loss: 9323.3271\n",
      "gradient norm: 1319.6214065551758, minimum ratio: 0.80078125\n",
      "Epoch [1602], val_loss: 9344.3135\n",
      "gradient norm: 1321.7032775878906, minimum ratio: 0.77734375\n",
      "Epoch [1603], val_loss: 9365.3350\n",
      "gradient norm: 1323.7389450073242, minimum ratio: 0.78515625\n",
      "Epoch [1604], val_loss: 9386.3916\n",
      "gradient norm: 1325.8247375488281, minimum ratio: 0.78125\n",
      "Epoch [1605], val_loss: 9407.4805\n",
      "gradient norm: 1327.889877319336, minimum ratio: 0.796875\n",
      "Epoch [1606], val_loss: 9428.6084\n",
      "gradient norm: 1329.960563659668, minimum ratio: 0.7890625\n",
      "Epoch [1607], val_loss: 9449.7695\n",
      "gradient norm: 1332.033302307129, minimum ratio: 0.78125\n",
      "Epoch [1608], val_loss: 9470.9658\n",
      "gradient norm: 1334.1283340454102, minimum ratio: 0.796875\n",
      "Epoch [1609], val_loss: 9492.1953\n",
      "gradient norm: 1336.2253341674805, minimum ratio: 0.77734375\n",
      "Epoch [1610], val_loss: 9513.4609\n",
      "gradient norm: 1338.275894165039, minimum ratio: 0.7890625\n",
      "Epoch [1611], val_loss: 9534.7559\n",
      "gradient norm: 1340.3227233886719, minimum ratio: 0.76953125\n",
      "Epoch [1612], val_loss: 9556.0879\n",
      "gradient norm: 1342.3938903808594, minimum ratio: 0.78125\n",
      "Epoch [1613], val_loss: 9577.4502\n",
      "gradient norm: 1344.498046875, minimum ratio: 0.80078125\n",
      "Epoch [1614], val_loss: 9598.8457\n",
      "gradient norm: 1346.5639572143555, minimum ratio: 0.77734375\n",
      "Epoch [1615], val_loss: 9620.2764\n",
      "gradient norm: 1348.6504211425781, minimum ratio: 0.8046875\n",
      "Epoch [1616], val_loss: 9641.7412\n",
      "gradient norm: 1350.7606887817383, minimum ratio: 0.76953125\n",
      "Epoch [1617], val_loss: 9663.2422\n",
      "gradient norm: 1352.8076171875, minimum ratio: 0.7890625\n",
      "Epoch [1618], val_loss: 9684.7734\n",
      "gradient norm: 1354.8729934692383, minimum ratio: 0.7734375\n",
      "Epoch [1619], val_loss: 9706.3398\n",
      "gradient norm: 1356.9710998535156, minimum ratio: 0.7890625\n",
      "Epoch [1620], val_loss: 9727.9414\n",
      "gradient norm: 1359.0552520751953, minimum ratio: 0.77734375\n",
      "Epoch [1621], val_loss: 9749.5771\n",
      "gradient norm: 1361.1748504638672, minimum ratio: 0.765625\n",
      "Epoch [1622], val_loss: 9771.2490\n",
      "gradient norm: 1363.2974548339844, minimum ratio: 0.7734375\n",
      "Epoch [1623], val_loss: 9792.9512\n",
      "gradient norm: 1365.4003524780273, minimum ratio: 0.7890625\n",
      "Epoch [1624], val_loss: 9814.6885\n",
      "gradient norm: 1367.5261764526367, minimum ratio: 0.765625\n",
      "Epoch [1625], val_loss: 9836.4600\n",
      "gradient norm: 1369.630516052246, minimum ratio: 0.765625\n",
      "Epoch [1626], val_loss: 9858.2676\n",
      "gradient norm: 1371.7008514404297, minimum ratio: 0.7734375\n",
      "Epoch [1627], val_loss: 9880.1064\n",
      "gradient norm: 1373.7243881225586, minimum ratio: 0.76953125\n",
      "Epoch [1628], val_loss: 9901.9766\n",
      "gradient norm: 1375.8334655761719, minimum ratio: 0.76171875\n",
      "Epoch [1629], val_loss: 9923.8828\n",
      "gradient norm: 1377.9234619140625, minimum ratio: 0.75390625\n",
      "Epoch [1630], val_loss: 9945.8252\n",
      "gradient norm: 1380.0607604980469, minimum ratio: 0.76953125\n",
      "Epoch [1631], val_loss: 9967.8057\n",
      "gradient norm: 1382.1527481079102, minimum ratio: 0.76953125\n",
      "Epoch [1632], val_loss: 9989.8223\n",
      "gradient norm: 1384.250717163086, minimum ratio: 0.8046875\n",
      "Epoch [1633], val_loss: 10011.8770\n",
      "gradient norm: 1386.3728866577148, minimum ratio: 0.765625\n",
      "Epoch [1634], val_loss: 10033.9648\n",
      "gradient norm: 1388.4733657836914, minimum ratio: 0.78515625\n",
      "Epoch [1635], val_loss: 10056.0889\n",
      "gradient norm: 1390.6218643188477, minimum ratio: 0.765625\n",
      "Epoch [1636], val_loss: 10078.2510\n",
      "gradient norm: 1392.7588424682617, minimum ratio: 0.79296875\n",
      "Epoch [1637], val_loss: 10100.4473\n",
      "gradient norm: 1394.8601455688477, minimum ratio: 0.76171875\n",
      "Epoch [1638], val_loss: 10122.6787\n",
      "gradient norm: 1397.0153350830078, minimum ratio: 0.76953125\n",
      "Epoch [1639], val_loss: 10144.9404\n",
      "gradient norm: 1399.1489868164062, minimum ratio: 0.78125\n",
      "Epoch [1640], val_loss: 10167.2305\n",
      "gradient norm: 1401.2629928588867, minimum ratio: 0.78515625\n",
      "Epoch [1641], val_loss: 10189.5566\n",
      "gradient norm: 1403.4215774536133, minimum ratio: 0.7734375\n",
      "Epoch [1642], val_loss: 10211.9209\n",
      "gradient norm: 1405.5834197998047, minimum ratio: 0.76953125\n",
      "Epoch [1643], val_loss: 10234.3213\n",
      "gradient norm: 1407.7258071899414, minimum ratio: 0.78125\n",
      "Epoch [1644], val_loss: 10256.7539\n",
      "gradient norm: 1409.8750762939453, minimum ratio: 0.7890625\n",
      "Epoch [1645], val_loss: 10279.2227\n",
      "gradient norm: 1412.0183639526367, minimum ratio: 0.79296875\n",
      "Epoch [1646], val_loss: 10301.7246\n",
      "gradient norm: 1414.1879043579102, minimum ratio: 0.7734375\n",
      "Epoch [1647], val_loss: 10324.2646\n",
      "gradient norm: 1416.3175201416016, minimum ratio: 0.7890625\n",
      "Epoch [1648], val_loss: 10346.8418\n",
      "gradient norm: 1418.4923706054688, minimum ratio: 0.796875\n",
      "Epoch [1649], val_loss: 10369.4473\n",
      "gradient norm: 1420.6289749145508, minimum ratio: 0.79296875\n",
      "Epoch [1650], val_loss: 10392.0820\n",
      "gradient norm: 1422.7826766967773, minimum ratio: 0.765625\n",
      "Epoch [1651], val_loss: 10414.7529\n",
      "gradient norm: 1424.916648864746, minimum ratio: 0.8125\n",
      "Epoch [1652], val_loss: 10437.4639\n",
      "gradient norm: 1427.0780029296875, minimum ratio: 0.7890625\n",
      "Epoch [1653], val_loss: 10460.2119\n",
      "gradient norm: 1429.241470336914, minimum ratio: 0.77734375\n",
      "Epoch [1654], val_loss: 10482.9912\n",
      "gradient norm: 1431.4264907836914, minimum ratio: 0.77734375\n",
      "Epoch [1655], val_loss: 10505.8057\n",
      "gradient norm: 1433.5928039550781, minimum ratio: 0.7890625\n",
      "Epoch [1656], val_loss: 10528.6562\n",
      "gradient norm: 1435.733169555664, minimum ratio: 0.76953125\n",
      "Epoch [1657], val_loss: 10551.5430\n",
      "gradient norm: 1437.8598022460938, minimum ratio: 0.7734375\n",
      "Epoch [1658], val_loss: 10574.4668\n",
      "gradient norm: 1440.0337677001953, minimum ratio: 0.7734375\n",
      "Epoch [1659], val_loss: 10597.4219\n",
      "gradient norm: 1442.157241821289, minimum ratio: 0.78125\n",
      "Epoch [1660], val_loss: 10620.4150\n",
      "gradient norm: 1444.3295516967773, minimum ratio: 0.80078125\n",
      "Epoch [1661], val_loss: 10643.4434\n",
      "gradient norm: 1446.492202758789, minimum ratio: 0.78125\n",
      "Epoch [1662], val_loss: 10666.5146\n",
      "gradient norm: 1448.6554107666016, minimum ratio: 0.78125\n",
      "Epoch [1663], val_loss: 10689.6143\n",
      "gradient norm: 1450.8176574707031, minimum ratio: 0.78125\n",
      "Epoch [1664], val_loss: 10712.7529\n",
      "gradient norm: 1452.9430084228516, minimum ratio: 0.7734375\n",
      "Epoch [1665], val_loss: 10735.9307\n",
      "gradient norm: 1455.0785827636719, minimum ratio: 0.77734375\n",
      "Epoch [1666], val_loss: 10759.1396\n",
      "gradient norm: 1457.2875366210938, minimum ratio: 0.765625\n",
      "Epoch [1667], val_loss: 10782.3838\n",
      "gradient norm: 1459.4984893798828, minimum ratio: 0.78515625\n",
      "Epoch [1668], val_loss: 10805.6611\n",
      "gradient norm: 1461.710678100586, minimum ratio: 0.78125\n",
      "Epoch [1669], val_loss: 10828.9775\n",
      "gradient norm: 1463.9019775390625, minimum ratio: 0.80078125\n",
      "Epoch [1670], val_loss: 10852.3320\n",
      "gradient norm: 1466.1194305419922, minimum ratio: 0.76953125\n",
      "Epoch [1671], val_loss: 10875.7227\n",
      "gradient norm: 1468.2973022460938, minimum ratio: 0.77734375\n",
      "Epoch [1672], val_loss: 10899.1484\n",
      "gradient norm: 1470.5191192626953, minimum ratio: 0.78515625\n",
      "Epoch [1673], val_loss: 10922.6104\n",
      "gradient norm: 1472.716552734375, minimum ratio: 0.7734375\n",
      "Epoch [1674], val_loss: 10946.1104\n",
      "gradient norm: 1474.920166015625, minimum ratio: 0.7734375\n",
      "Epoch [1675], val_loss: 10969.6436\n",
      "gradient norm: 1477.1224670410156, minimum ratio: 0.77734375\n",
      "Epoch [1676], val_loss: 10993.2178\n",
      "gradient norm: 1479.3169708251953, minimum ratio: 0.78125\n",
      "Epoch [1677], val_loss: 11016.8203\n",
      "gradient norm: 1481.4967651367188, minimum ratio: 0.78125\n",
      "Epoch [1678], val_loss: 11040.4580\n",
      "gradient norm: 1483.6899719238281, minimum ratio: 0.796875\n",
      "Epoch [1679], val_loss: 11064.1338\n",
      "gradient norm: 1485.925033569336, minimum ratio: 0.8125\n",
      "Epoch [1680], val_loss: 11087.8438\n",
      "gradient norm: 1488.1609191894531, minimum ratio: 0.7734375\n",
      "Epoch [1681], val_loss: 11111.5859\n",
      "gradient norm: 1490.3990478515625, minimum ratio: 0.78515625\n",
      "Epoch [1682], val_loss: 11135.3643\n",
      "gradient norm: 1492.6184997558594, minimum ratio: 0.76171875\n",
      "Epoch [1683], val_loss: 11159.1729\n",
      "gradient norm: 1494.8592376708984, minimum ratio: 0.77734375\n",
      "Epoch [1684], val_loss: 11183.0215\n",
      "gradient norm: 1497.020004272461, minimum ratio: 0.78125\n",
      "Epoch [1685], val_loss: 11206.9082\n",
      "gradient norm: 1499.2393341064453, minimum ratio: 0.76953125\n",
      "Epoch [1686], val_loss: 11230.8320\n",
      "gradient norm: 1501.4626159667969, minimum ratio: 0.76953125\n",
      "Epoch [1687], val_loss: 11254.7910\n",
      "gradient norm: 1503.712646484375, minimum ratio: 0.7734375\n",
      "Epoch [1688], val_loss: 11278.7832\n",
      "gradient norm: 1505.8460540771484, minimum ratio: 0.78125\n",
      "Epoch [1689], val_loss: 11302.8086\n",
      "gradient norm: 1508.0814819335938, minimum ratio: 0.765625\n",
      "Epoch [1690], val_loss: 11326.8740\n",
      "gradient norm: 1510.311538696289, minimum ratio: 0.76953125\n",
      "Epoch [1691], val_loss: 11350.9756\n",
      "gradient norm: 1512.5229034423828, minimum ratio: 0.8046875\n",
      "Epoch [1692], val_loss: 11375.1084\n",
      "gradient norm: 1514.7421417236328, minimum ratio: 0.7734375\n",
      "Epoch [1693], val_loss: 11399.2764\n",
      "gradient norm: 1516.9797668457031, minimum ratio: 0.78125\n",
      "Epoch [1694], val_loss: 11423.4863\n",
      "gradient norm: 1519.1894836425781, minimum ratio: 0.78125\n",
      "Epoch [1695], val_loss: 11447.7236\n",
      "gradient norm: 1521.4257354736328, minimum ratio: 0.77734375\n",
      "Epoch [1696], val_loss: 11472.0000\n",
      "gradient norm: 1523.6693267822266, minimum ratio: 0.77734375\n",
      "Epoch [1697], val_loss: 11496.3164\n",
      "gradient norm: 1525.91748046875, minimum ratio: 0.7578125\n",
      "Epoch [1698], val_loss: 11520.6680\n",
      "gradient norm: 1528.1602172851562, minimum ratio: 0.7734375\n",
      "Epoch [1699], val_loss: 11545.0635\n",
      "gradient norm: 1530.3791046142578, minimum ratio: 0.77734375\n",
      "Epoch [1700], val_loss: 11569.4951\n",
      "gradient norm: 1532.654281616211, minimum ratio: 0.77734375\n",
      "Epoch [1701], val_loss: 11593.9619\n",
      "gradient norm: 1534.9314575195312, minimum ratio: 0.76953125\n",
      "Epoch [1702], val_loss: 11618.4678\n",
      "gradient norm: 1537.168472290039, minimum ratio: 0.8125\n",
      "Epoch [1703], val_loss: 11643.0059\n",
      "gradient norm: 1539.4047546386719, minimum ratio: 0.78125\n",
      "Epoch [1704], val_loss: 11667.5801\n",
      "gradient norm: 1541.6874084472656, minimum ratio: 0.7734375\n",
      "Epoch [1705], val_loss: 11692.1846\n",
      "gradient norm: 1543.9227600097656, minimum ratio: 0.7890625\n",
      "Epoch [1706], val_loss: 11716.8291\n",
      "gradient norm: 1546.1966400146484, minimum ratio: 0.7578125\n",
      "Epoch [1707], val_loss: 11741.5088\n",
      "gradient norm: 1548.4842834472656, minimum ratio: 0.765625\n",
      "Epoch [1708], val_loss: 11766.2256\n",
      "gradient norm: 1550.7495727539062, minimum ratio: 0.76953125\n",
      "Epoch [1709], val_loss: 11790.9775\n",
      "gradient norm: 1553.041030883789, minimum ratio: 0.78125\n",
      "Epoch [1710], val_loss: 11815.7754\n",
      "gradient norm: 1555.2696533203125, minimum ratio: 0.76953125\n",
      "Epoch [1711], val_loss: 11840.6113\n",
      "gradient norm: 1557.5676879882812, minimum ratio: 0.7734375\n",
      "Epoch [1712], val_loss: 11865.4854\n",
      "gradient norm: 1559.8361511230469, minimum ratio: 0.78515625\n",
      "Epoch [1713], val_loss: 11890.3916\n",
      "gradient norm: 1562.1371307373047, minimum ratio: 0.75390625\n",
      "Epoch [1714], val_loss: 11915.3350\n",
      "gradient norm: 1564.3863220214844, minimum ratio: 0.796875\n",
      "Epoch [1715], val_loss: 11940.3174\n",
      "gradient norm: 1566.6911010742188, minimum ratio: 0.79296875\n",
      "Epoch [1716], val_loss: 11965.3281\n",
      "gradient norm: 1568.911361694336, minimum ratio: 0.7734375\n",
      "Epoch [1717], val_loss: 11990.3789\n",
      "gradient norm: 1571.1969909667969, minimum ratio: 0.7890625\n",
      "Epoch [1718], val_loss: 12015.4697\n",
      "gradient norm: 1573.4550018310547, minimum ratio: 0.76171875\n",
      "Epoch [1719], val_loss: 12040.5947\n",
      "gradient norm: 1575.6943969726562, minimum ratio: 0.7734375\n",
      "Epoch [1720], val_loss: 12065.7617\n",
      "gradient norm: 1577.9572296142578, minimum ratio: 0.78125\n",
      "Epoch [1721], val_loss: 12090.9600\n",
      "gradient norm: 1580.2037506103516, minimum ratio: 0.78515625\n",
      "Epoch [1722], val_loss: 12116.1973\n",
      "gradient norm: 1582.492919921875, minimum ratio: 0.78125\n",
      "Epoch [1723], val_loss: 12141.4707\n",
      "gradient norm: 1584.7845916748047, minimum ratio: 0.76953125\n",
      "Epoch [1724], val_loss: 12166.7822\n",
      "gradient norm: 1587.0777435302734, minimum ratio: 0.77734375\n",
      "Epoch [1725], val_loss: 12192.1348\n",
      "gradient norm: 1589.371810913086, minimum ratio: 0.7734375\n",
      "Epoch [1726], val_loss: 12217.5215\n",
      "gradient norm: 1591.6588745117188, minimum ratio: 0.75390625\n",
      "Epoch [1727], val_loss: 12242.9482\n",
      "gradient norm: 1593.9864349365234, minimum ratio: 0.79296875\n",
      "Epoch [1728], val_loss: 12268.4150\n",
      "gradient norm: 1596.3173828125, minimum ratio: 0.78515625\n",
      "Epoch [1729], val_loss: 12293.9229\n",
      "gradient norm: 1598.6506042480469, minimum ratio: 0.76953125\n",
      "Epoch [1730], val_loss: 12319.4629\n",
      "gradient norm: 1600.9304962158203, minimum ratio: 0.76953125\n",
      "Epoch [1731], val_loss: 12345.0391\n",
      "gradient norm: 1603.2665252685547, minimum ratio: 0.765625\n",
      "Epoch [1732], val_loss: 12370.6621\n",
      "gradient norm: 1605.5308990478516, minimum ratio: 0.77734375\n",
      "Epoch [1733], val_loss: 12396.3271\n",
      "gradient norm: 1607.7985076904297, minimum ratio: 0.79296875\n",
      "Epoch [1734], val_loss: 12422.0234\n",
      "gradient norm: 1610.1160278320312, minimum ratio: 0.796875\n",
      "Epoch [1735], val_loss: 12447.7666\n",
      "gradient norm: 1612.3813934326172, minimum ratio: 0.78515625\n",
      "Epoch [1736], val_loss: 12473.5518\n",
      "gradient norm: 1614.731918334961, minimum ratio: 0.7890625\n",
      "Epoch [1737], val_loss: 12499.3779\n",
      "gradient norm: 1617.0848083496094, minimum ratio: 0.76953125\n",
      "Epoch [1738], val_loss: 12525.2354\n",
      "gradient norm: 1619.4173736572266, minimum ratio: 0.77734375\n",
      "Epoch [1739], val_loss: 12551.1221\n",
      "gradient norm: 1621.7714080810547, minimum ratio: 0.78515625\n",
      "Epoch [1740], val_loss: 12577.0459\n",
      "gradient norm: 1624.0743103027344, minimum ratio: 0.77734375\n",
      "Epoch [1741], val_loss: 12603.0088\n",
      "gradient norm: 1626.3992309570312, minimum ratio: 0.80078125\n",
      "Epoch [1742], val_loss: 12629.0127\n",
      "gradient norm: 1628.759994506836, minimum ratio: 0.77734375\n",
      "Epoch [1743], val_loss: 12655.0527\n",
      "gradient norm: 1631.0971069335938, minimum ratio: 0.80859375\n",
      "Epoch [1744], val_loss: 12681.1279\n",
      "gradient norm: 1633.4420776367188, minimum ratio: 0.78125\n",
      "Epoch [1745], val_loss: 12707.2412\n",
      "gradient norm: 1635.8076782226562, minimum ratio: 0.78125\n",
      "Epoch [1746], val_loss: 12733.3975\n",
      "gradient norm: 1638.1766510009766, minimum ratio: 0.7890625\n",
      "Epoch [1747], val_loss: 12759.5908\n",
      "gradient norm: 1640.5478515625, minimum ratio: 0.76953125\n",
      "Epoch [1748], val_loss: 12785.8262\n",
      "gradient norm: 1642.8926849365234, minimum ratio: 0.80078125\n",
      "Epoch [1749], val_loss: 12812.0986\n",
      "gradient norm: 1645.199935913086, minimum ratio: 0.77734375\n",
      "Epoch [1750], val_loss: 12838.4150\n",
      "gradient norm: 1647.5350799560547, minimum ratio: 0.79296875\n",
      "Epoch [1751], val_loss: 12864.7646\n",
      "gradient norm: 1649.8636779785156, minimum ratio: 0.7890625\n",
      "Epoch [1752], val_loss: 12891.1553\n",
      "gradient norm: 1652.1507110595703, minimum ratio: 0.78125\n",
      "Epoch [1753], val_loss: 12917.5820\n",
      "gradient norm: 1654.5027770996094, minimum ratio: 0.77734375\n",
      "Epoch [1754], val_loss: 12944.0479\n",
      "gradient norm: 1656.8883972167969, minimum ratio: 0.7734375\n",
      "Epoch [1755], val_loss: 12970.5537\n",
      "gradient norm: 1659.2518920898438, minimum ratio: 0.78515625\n",
      "Epoch [1756], val_loss: 12997.1045\n",
      "gradient norm: 1661.5790252685547, minimum ratio: 0.796875\n",
      "Epoch [1757], val_loss: 13023.6973\n",
      "gradient norm: 1663.9429626464844, minimum ratio: 0.7734375\n",
      "Epoch [1758], val_loss: 13050.3281\n",
      "gradient norm: 1666.3202667236328, minimum ratio: 0.765625\n",
      "Epoch [1759], val_loss: 13076.9971\n",
      "gradient norm: 1668.7183685302734, minimum ratio: 0.7890625\n",
      "Epoch [1760], val_loss: 13103.7070\n",
      "gradient norm: 1671.0588684082031, minimum ratio: 0.7734375\n",
      "Epoch [1761], val_loss: 13130.4580\n",
      "gradient norm: 1673.4366149902344, minimum ratio: 0.78125\n",
      "Epoch [1762], val_loss: 13157.2471\n",
      "gradient norm: 1675.8103485107422, minimum ratio: 0.77734375\n",
      "Epoch [1763], val_loss: 13184.0840\n",
      "gradient norm: 1678.1907348632812, minimum ratio: 0.78125\n",
      "Epoch [1764], val_loss: 13210.9639\n",
      "gradient norm: 1680.5535583496094, minimum ratio: 0.78515625\n",
      "Epoch [1765], val_loss: 13237.8750\n",
      "gradient norm: 1682.9659576416016, minimum ratio: 0.7578125\n",
      "Epoch [1766], val_loss: 13264.8271\n",
      "gradient norm: 1685.3217468261719, minimum ratio: 0.765625\n",
      "Epoch [1767], val_loss: 13291.8125\n",
      "gradient norm: 1687.707046508789, minimum ratio: 0.7890625\n",
      "Epoch [1768], val_loss: 13318.8301\n",
      "gradient norm: 1690.1001434326172, minimum ratio: 0.7734375\n",
      "Epoch [1769], val_loss: 13345.8896\n",
      "gradient norm: 1692.4898834228516, minimum ratio: 0.78125\n",
      "Epoch [1770], val_loss: 13372.9844\n",
      "gradient norm: 1694.9102172851562, minimum ratio: 0.78515625\n",
      "Epoch [1771], val_loss: 13400.1250\n",
      "gradient norm: 1697.2296295166016, minimum ratio: 0.7890625\n",
      "Epoch [1772], val_loss: 13427.3086\n",
      "gradient norm: 1699.6055450439453, minimum ratio: 0.796875\n",
      "Epoch [1773], val_loss: 13454.5273\n",
      "gradient norm: 1702.0033111572266, minimum ratio: 0.76171875\n",
      "Epoch [1774], val_loss: 13481.7871\n",
      "gradient norm: 1704.353012084961, minimum ratio: 0.76953125\n",
      "Epoch [1775], val_loss: 13509.0771\n",
      "gradient norm: 1706.7588958740234, minimum ratio: 0.78125\n",
      "Epoch [1776], val_loss: 13536.4102\n",
      "gradient norm: 1709.1920013427734, minimum ratio: 0.78515625\n",
      "Epoch [1777], val_loss: 13563.7783\n",
      "gradient norm: 1711.6309661865234, minimum ratio: 0.78515625\n",
      "Epoch [1778], val_loss: 13591.1846\n",
      "gradient norm: 1714.0370178222656, minimum ratio: 0.78125\n",
      "Epoch [1779], val_loss: 13618.6338\n",
      "gradient norm: 1716.4761962890625, minimum ratio: 0.77734375\n",
      "Epoch [1780], val_loss: 13646.1172\n",
      "gradient norm: 1718.8218841552734, minimum ratio: 0.76171875\n",
      "Epoch [1781], val_loss: 13673.6279\n",
      "gradient norm: 1721.2614288330078, minimum ratio: 0.79296875\n",
      "Epoch [1782], val_loss: 13701.1865\n",
      "gradient norm: 1723.7052612304688, minimum ratio: 0.78515625\n",
      "Epoch [1783], val_loss: 13728.7939\n",
      "gradient norm: 1726.1526336669922, minimum ratio: 0.77734375\n",
      "Epoch [1784], val_loss: 13756.4316\n",
      "gradient norm: 1728.5762634277344, minimum ratio: 0.76953125\n",
      "Epoch [1785], val_loss: 13784.1162\n",
      "gradient norm: 1730.9640655517578, minimum ratio: 0.76171875\n",
      "Epoch [1786], val_loss: 13811.8291\n",
      "gradient norm: 1733.389175415039, minimum ratio: 0.79296875\n",
      "Epoch [1787], val_loss: 13839.5762\n",
      "gradient norm: 1735.8412170410156, minimum ratio: 0.7890625\n",
      "Epoch [1788], val_loss: 13867.3584\n",
      "gradient norm: 1738.2610321044922, minimum ratio: 0.77734375\n",
      "Epoch [1789], val_loss: 13895.1689\n",
      "gradient norm: 1740.6328125, minimum ratio: 0.7578125\n",
      "Epoch [1790], val_loss: 13923.0195\n",
      "gradient norm: 1743.0295715332031, minimum ratio: 0.7734375\n",
      "Epoch [1791], val_loss: 13950.9180\n",
      "gradient norm: 1745.4284973144531, minimum ratio: 0.75390625\n",
      "Epoch [1792], val_loss: 13978.8613\n",
      "gradient norm: 1747.8438262939453, minimum ratio: 0.78125\n",
      "Epoch [1793], val_loss: 14006.8467\n",
      "gradient norm: 1750.2446899414062, minimum ratio: 0.765625\n",
      "Epoch [1794], val_loss: 14034.8779\n",
      "gradient norm: 1752.6790924072266, minimum ratio: 0.7734375\n",
      "Epoch [1795], val_loss: 14062.9434\n",
      "gradient norm: 1755.1483459472656, minimum ratio: 0.76171875\n",
      "Epoch [1796], val_loss: 14091.0566\n",
      "gradient norm: 1757.5858306884766, minimum ratio: 0.765625\n",
      "Epoch [1797], val_loss: 14119.2080\n",
      "gradient norm: 1760.0602264404297, minimum ratio: 0.77734375\n",
      "Epoch [1798], val_loss: 14147.3965\n",
      "gradient norm: 1762.5352172851562, minimum ratio: 0.7890625\n",
      "Epoch [1799], val_loss: 14175.6279\n",
      "gradient norm: 1764.9684143066406, minimum ratio: 0.7890625\n",
      "Epoch [1800], val_loss: 14203.9033\n",
      "gradient norm: 1767.4176788330078, minimum ratio: 0.78125\n",
      "Epoch [1801], val_loss: 14232.2246\n",
      "gradient norm: 1769.7589263916016, minimum ratio: 0.77734375\n",
      "Epoch [1802], val_loss: 14260.5869\n",
      "gradient norm: 1772.2127380371094, minimum ratio: 0.796875\n",
      "Epoch [1803], val_loss: 14288.9883\n",
      "gradient norm: 1774.6712036132812, minimum ratio: 0.7734375\n",
      "Epoch [1804], val_loss: 14317.4297\n",
      "gradient norm: 1777.1615905761719, minimum ratio: 0.80078125\n",
      "Epoch [1805], val_loss: 14345.9033\n",
      "gradient norm: 1779.6206817626953, minimum ratio: 0.78125\n",
      "Epoch [1806], val_loss: 14374.4297\n",
      "gradient norm: 1782.1165466308594, minimum ratio: 0.7890625\n",
      "Epoch [1807], val_loss: 14402.9951\n",
      "gradient norm: 1784.613540649414, minimum ratio: 0.76953125\n",
      "Epoch [1808], val_loss: 14431.5986\n",
      "gradient norm: 1787.0355529785156, minimum ratio: 0.77734375\n",
      "Epoch [1809], val_loss: 14460.2412\n",
      "gradient norm: 1789.5011138916016, minimum ratio: 0.77734375\n",
      "Epoch [1810], val_loss: 14488.9268\n",
      "gradient norm: 1792.0110321044922, minimum ratio: 0.765625\n",
      "Epoch [1811], val_loss: 14517.6602\n",
      "gradient norm: 1794.4693298339844, minimum ratio: 0.77734375\n",
      "Epoch [1812], val_loss: 14546.4199\n",
      "gradient norm: 1796.9320068359375, minimum ratio: 0.7421875\n",
      "Epoch [1813], val_loss: 14575.2217\n",
      "gradient norm: 1799.4405822753906, minimum ratio: 0.7890625\n",
      "Epoch [1814], val_loss: 14604.0615\n",
      "gradient norm: 1801.9168853759766, minimum ratio: 0.77734375\n",
      "Epoch [1815], val_loss: 14632.9502\n",
      "gradient norm: 1804.4031372070312, minimum ratio: 0.78125\n",
      "Epoch [1816], val_loss: 14661.8799\n",
      "gradient norm: 1806.919418334961, minimum ratio: 0.78515625\n",
      "Epoch [1817], val_loss: 14690.8486\n",
      "gradient norm: 1809.4125213623047, minimum ratio: 0.7890625\n",
      "Epoch [1818], val_loss: 14719.8652\n",
      "gradient norm: 1811.907470703125, minimum ratio: 0.7734375\n",
      "Epoch [1819], val_loss: 14748.9199\n",
      "gradient norm: 1814.3631439208984, minimum ratio: 0.765625\n",
      "Epoch [1820], val_loss: 14778.0020\n",
      "gradient norm: 1816.8564758300781, minimum ratio: 0.76171875\n",
      "Epoch [1821], val_loss: 14807.1299\n",
      "gradient norm: 1819.3821105957031, minimum ratio: 0.77734375\n",
      "Epoch [1822], val_loss: 14836.2930\n",
      "gradient norm: 1821.8381805419922, minimum ratio: 0.75390625\n",
      "Epoch [1823], val_loss: 14865.4912\n",
      "gradient norm: 1824.3334655761719, minimum ratio: 0.765625\n",
      "Epoch [1824], val_loss: 14894.7236\n",
      "gradient norm: 1826.7688903808594, minimum ratio: 0.78125\n",
      "Epoch [1825], val_loss: 14923.9902\n",
      "gradient norm: 1829.2367706298828, minimum ratio: 0.7578125\n",
      "Epoch [1826], val_loss: 14953.3018\n",
      "gradient norm: 1831.6759338378906, minimum ratio: 0.77734375\n",
      "Epoch [1827], val_loss: 14982.6533\n",
      "gradient norm: 1834.186996459961, minimum ratio: 0.76171875\n",
      "Epoch [1828], val_loss: 15012.0518\n",
      "gradient norm: 1836.7234802246094, minimum ratio: 0.7890625\n",
      "Epoch [1829], val_loss: 15041.4912\n",
      "gradient norm: 1839.262680053711, minimum ratio: 0.77734375\n",
      "Epoch [1830], val_loss: 15070.9697\n",
      "gradient norm: 1841.7642211914062, minimum ratio: 0.75390625\n",
      "Epoch [1831], val_loss: 15100.4971\n",
      "gradient norm: 1844.2747802734375, minimum ratio: 0.7734375\n",
      "Epoch [1832], val_loss: 15130.0596\n",
      "gradient norm: 1846.780044555664, minimum ratio: 0.77734375\n",
      "Epoch [1833], val_loss: 15159.6650\n",
      "gradient norm: 1849.2664184570312, minimum ratio: 0.77734375\n",
      "Epoch [1834], val_loss: 15189.3164\n",
      "gradient norm: 1851.7843170166016, minimum ratio: 0.78515625\n",
      "Epoch [1835], val_loss: 15219.0068\n",
      "gradient norm: 1854.3093566894531, minimum ratio: 0.7734375\n",
      "Epoch [1836], val_loss: 15248.7402\n",
      "gradient norm: 1856.8643188476562, minimum ratio: 0.77734375\n",
      "Epoch [1837], val_loss: 15278.5137\n",
      "gradient norm: 1859.4209594726562, minimum ratio: 0.765625\n",
      "Epoch [1838], val_loss: 15308.3281\n",
      "gradient norm: 1861.9431610107422, minimum ratio: 0.7734375\n",
      "Epoch [1839], val_loss: 15338.1934\n",
      "gradient norm: 1864.4711303710938, minimum ratio: 0.7734375\n",
      "Epoch [1840], val_loss: 15368.0996\n",
      "gradient norm: 1866.9663391113281, minimum ratio: 0.796875\n",
      "Epoch [1841], val_loss: 15398.0518\n",
      "gradient norm: 1869.4751434326172, minimum ratio: 0.78515625\n",
      "Epoch [1842], val_loss: 15428.0352\n",
      "gradient norm: 1872.0430908203125, minimum ratio: 0.76953125\n",
      "Epoch [1843], val_loss: 15458.0635\n",
      "gradient norm: 1874.5290222167969, minimum ratio: 0.79296875\n",
      "Epoch [1844], val_loss: 15488.1279\n",
      "gradient norm: 1877.100845336914, minimum ratio: 0.7734375\n",
      "Epoch [1845], val_loss: 15518.2334\n",
      "gradient norm: 1879.6446838378906, minimum ratio: 0.77734375\n",
      "Epoch [1846], val_loss: 15548.3770\n",
      "gradient norm: 1882.2194213867188, minimum ratio: 0.765625\n",
      "Epoch [1847], val_loss: 15578.5537\n",
      "gradient norm: 1884.7012786865234, minimum ratio: 0.765625\n",
      "Epoch [1848], val_loss: 15608.7695\n",
      "gradient norm: 1887.2785034179688, minimum ratio: 0.77734375\n",
      "Epoch [1849], val_loss: 15639.0303\n",
      "gradient norm: 1889.8580474853516, minimum ratio: 0.77734375\n",
      "Epoch [1850], val_loss: 15669.3281\n",
      "gradient norm: 1892.3717346191406, minimum ratio: 0.796875\n",
      "Epoch [1851], val_loss: 15699.6729\n",
      "gradient norm: 1894.92626953125, minimum ratio: 0.76953125\n",
      "Epoch [1852], val_loss: 15730.0479\n",
      "gradient norm: 1897.510238647461, minimum ratio: 0.78515625\n",
      "Epoch [1853], val_loss: 15760.4688\n",
      "gradient norm: 1900.035873413086, minimum ratio: 0.765625\n",
      "Epoch [1854], val_loss: 15790.9268\n",
      "gradient norm: 1902.4866638183594, minimum ratio: 0.7890625\n",
      "Epoch [1855], val_loss: 15821.4336\n",
      "gradient norm: 1905.0383758544922, minimum ratio: 0.77734375\n",
      "Epoch [1856], val_loss: 15851.9746\n",
      "gradient norm: 1907.630599975586, minimum ratio: 0.76953125\n",
      "Epoch [1857], val_loss: 15882.5654\n",
      "gradient norm: 1910.2256927490234, minimum ratio: 0.77734375\n",
      "Epoch [1858], val_loss: 15913.1953\n",
      "gradient norm: 1912.7567443847656, minimum ratio: 0.76953125\n",
      "Epoch [1859], val_loss: 15943.8633\n",
      "gradient norm: 1915.3331298828125, minimum ratio: 0.80078125\n",
      "Epoch [1860], val_loss: 15974.5771\n",
      "gradient norm: 1917.9087829589844, minimum ratio: 0.7890625\n",
      "Epoch [1861], val_loss: 16005.3330\n",
      "gradient norm: 1920.4752197265625, minimum ratio: 0.78515625\n",
      "Epoch [1862], val_loss: 16036.1387\n",
      "gradient norm: 1922.9810028076172, minimum ratio: 0.78125\n",
      "Epoch [1863], val_loss: 16066.9863\n",
      "gradient norm: 1925.5903930664062, minimum ratio: 0.79296875\n",
      "Epoch [1864], val_loss: 16097.8779\n",
      "gradient norm: 1928.135513305664, minimum ratio: 0.7890625\n",
      "Epoch [1865], val_loss: 16128.8164\n",
      "gradient norm: 1930.7197723388672, minimum ratio: 0.76953125\n",
      "Epoch [1866], val_loss: 16159.7930\n",
      "gradient norm: 1933.3363342285156, minimum ratio: 0.8125\n",
      "Epoch [1867], val_loss: 16190.8164\n",
      "gradient norm: 1935.9555969238281, minimum ratio: 0.78125\n",
      "Epoch [1868], val_loss: 16221.8779\n",
      "gradient norm: 1938.5501251220703, minimum ratio: 0.7734375\n",
      "Epoch [1869], val_loss: 16252.9746\n",
      "gradient norm: 1941.1717681884766, minimum ratio: 0.79296875\n",
      "Epoch [1870], val_loss: 16284.1152\n",
      "gradient norm: 1943.7660064697266, minimum ratio: 0.78125\n",
      "Epoch [1871], val_loss: 16315.2969\n",
      "gradient norm: 1946.4009094238281, minimum ratio: 0.796875\n",
      "Epoch [1872], val_loss: 16346.5264\n",
      "gradient norm: 1948.9925689697266, minimum ratio: 0.7734375\n",
      "Epoch [1873], val_loss: 16377.7930\n",
      "gradient norm: 1951.622543334961, minimum ratio: 0.76171875\n",
      "Epoch [1874], val_loss: 16409.1094\n",
      "gradient norm: 1954.227798461914, minimum ratio: 0.78515625\n",
      "Epoch [1875], val_loss: 16440.4707\n",
      "gradient norm: 1956.8101654052734, minimum ratio: 0.7890625\n",
      "Epoch [1876], val_loss: 16471.8887\n",
      "gradient norm: 1959.4510345458984, minimum ratio: 0.7734375\n",
      "Epoch [1877], val_loss: 16503.3438\n",
      "gradient norm: 1961.986587524414, minimum ratio: 0.76171875\n",
      "Epoch [1878], val_loss: 16534.8301\n",
      "gradient norm: 1964.5907440185547, minimum ratio: 0.76953125\n",
      "Epoch [1879], val_loss: 16566.3613\n",
      "gradient norm: 1967.1647338867188, minimum ratio: 0.76953125\n",
      "Epoch [1880], val_loss: 16597.9336\n",
      "gradient norm: 1969.8113098144531, minimum ratio: 0.7734375\n",
      "Epoch [1881], val_loss: 16629.5527\n",
      "gradient norm: 1972.2854919433594, minimum ratio: 0.76953125\n",
      "Epoch [1882], val_loss: 16661.2207\n",
      "gradient norm: 1974.9017333984375, minimum ratio: 0.77734375\n",
      "Epoch [1883], val_loss: 16692.9375\n",
      "gradient norm: 1977.520004272461, minimum ratio: 0.76953125\n",
      "Epoch [1884], val_loss: 16724.6875\n",
      "gradient norm: 1980.1761627197266, minimum ratio: 0.78125\n",
      "Epoch [1885], val_loss: 16756.4707\n",
      "gradient norm: 1982.8074645996094, minimum ratio: 0.76953125\n",
      "Epoch [1886], val_loss: 16788.3008\n",
      "gradient norm: 1985.429672241211, minimum ratio: 0.76953125\n",
      "Epoch [1887], val_loss: 16820.1719\n",
      "gradient norm: 1988.0498809814453, minimum ratio: 0.77734375\n",
      "Epoch [1888], val_loss: 16852.0820\n",
      "gradient norm: 1990.6810760498047, minimum ratio: 0.7734375\n",
      "Epoch [1889], val_loss: 16884.0352\n",
      "gradient norm: 1993.2678680419922, minimum ratio: 0.79296875\n",
      "Epoch [1890], val_loss: 16916.0410\n",
      "gradient norm: 1995.833984375, minimum ratio: 0.74609375\n",
      "Epoch [1891], val_loss: 16948.0898\n",
      "gradient norm: 1998.465576171875, minimum ratio: 0.76171875\n",
      "Epoch [1892], val_loss: 16980.1855\n",
      "gradient norm: 2001.138442993164, minimum ratio: 0.77734375\n",
      "Epoch [1893], val_loss: 17012.3301\n",
      "gradient norm: 2003.785415649414, minimum ratio: 0.765625\n",
      "Epoch [1894], val_loss: 17044.5117\n",
      "gradient norm: 2006.4244079589844, minimum ratio: 0.79296875\n",
      "Epoch [1895], val_loss: 17076.7402\n",
      "gradient norm: 2009.0916137695312, minimum ratio: 0.78125\n",
      "Epoch [1896], val_loss: 17109.0059\n",
      "gradient norm: 2011.6886749267578, minimum ratio: 0.765625\n",
      "Epoch [1897], val_loss: 17141.3223\n",
      "gradient norm: 2014.3733673095703, minimum ratio: 0.78125\n",
      "Epoch [1898], val_loss: 17173.6855\n",
      "gradient norm: 2017.0216369628906, minimum ratio: 0.796875\n",
      "Epoch [1899], val_loss: 17206.0898\n",
      "gradient norm: 2019.6828918457031, minimum ratio: 0.78125\n",
      "Epoch [1900], val_loss: 17238.5391\n",
      "gradient norm: 2022.375015258789, minimum ratio: 0.7578125\n",
      "Epoch [1901], val_loss: 17271.0391\n",
      "gradient norm: 2025.0709381103516, minimum ratio: 0.765625\n",
      "Epoch [1902], val_loss: 17303.5879\n",
      "gradient norm: 2027.7086791992188, minimum ratio: 0.77734375\n",
      "Epoch [1903], val_loss: 17336.1758\n",
      "gradient norm: 2030.4093017578125, minimum ratio: 0.79296875\n",
      "Epoch [1904], val_loss: 17368.8008\n",
      "gradient norm: 2033.03857421875, minimum ratio: 0.77734375\n",
      "Epoch [1905], val_loss: 17401.4746\n",
      "gradient norm: 2035.7432708740234, minimum ratio: 0.7734375\n",
      "Epoch [1906], val_loss: 17434.1934\n",
      "gradient norm: 2038.4185943603516, minimum ratio: 0.7890625\n",
      "Epoch [1907], val_loss: 17466.9531\n",
      "gradient norm: 2041.127197265625, minimum ratio: 0.77734375\n",
      "Epoch [1908], val_loss: 17499.7559\n",
      "gradient norm: 2043.750244140625, minimum ratio: 0.7734375\n",
      "Epoch [1909], val_loss: 17532.5957\n",
      "gradient norm: 2046.4214324951172, minimum ratio: 0.765625\n",
      "Epoch [1910], val_loss: 17565.4922\n",
      "gradient norm: 2049.137252807617, minimum ratio: 0.78125\n",
      "Epoch [1911], val_loss: 17598.4258\n",
      "gradient norm: 2051.8316345214844, minimum ratio: 0.78125\n",
      "Epoch [1912], val_loss: 17631.4062\n",
      "gradient norm: 2054.518829345703, minimum ratio: 0.7578125\n",
      "Epoch [1913], val_loss: 17664.4219\n",
      "gradient norm: 2057.168411254883, minimum ratio: 0.76953125\n",
      "Epoch [1914], val_loss: 17697.4727\n",
      "gradient norm: 2059.8617095947266, minimum ratio: 0.76953125\n",
      "Epoch [1915], val_loss: 17730.5625\n",
      "gradient norm: 2062.583511352539, minimum ratio: 0.77734375\n",
      "Epoch [1916], val_loss: 17763.6992\n",
      "gradient norm: 2065.3076171875, minimum ratio: 0.7578125\n",
      "Epoch [1917], val_loss: 17796.8770\n",
      "gradient norm: 2067.99267578125, minimum ratio: 0.75390625\n",
      "Epoch [1918], val_loss: 17830.0957\n",
      "gradient norm: 2070.7206420898438, minimum ratio: 0.76171875\n",
      "Epoch [1919], val_loss: 17863.3652\n",
      "gradient norm: 2073.4124450683594, minimum ratio: 0.77734375\n",
      "Epoch [1920], val_loss: 17896.6836\n",
      "gradient norm: 2076.146957397461, minimum ratio: 0.796875\n",
      "Epoch [1921], val_loss: 17930.0449\n",
      "gradient norm: 2078.8080444335938, minimum ratio: 0.78125\n",
      "Epoch [1922], val_loss: 17963.4512\n",
      "gradient norm: 2081.439987182617, minimum ratio: 0.76171875\n",
      "Epoch [1923], val_loss: 17996.8965\n",
      "gradient norm: 2084.109161376953, minimum ratio: 0.78515625\n",
      "Epoch [1924], val_loss: 18030.3867\n",
      "gradient norm: 2086.851333618164, minimum ratio: 0.79296875\n",
      "Epoch [1925], val_loss: 18063.9238\n",
      "gradient norm: 2089.3921813964844, minimum ratio: 0.76953125\n",
      "Epoch [1926], val_loss: 18097.5078\n",
      "gradient norm: 2092.0311889648438, minimum ratio: 0.78515625\n",
      "Epoch [1927], val_loss: 18131.1406\n",
      "gradient norm: 2094.737289428711, minimum ratio: 0.7890625\n",
      "Epoch [1928], val_loss: 18164.8223\n",
      "gradient norm: 2097.4904174804688, minimum ratio: 0.80078125\n",
      "Epoch [1929], val_loss: 18198.5430\n",
      "gradient norm: 2100.196548461914, minimum ratio: 0.76953125\n",
      "Epoch [1930], val_loss: 18232.3145\n",
      "gradient norm: 2102.954376220703, minimum ratio: 0.76953125\n",
      "Epoch [1931], val_loss: 18266.1348\n",
      "gradient norm: 2105.642852783203, minimum ratio: 0.78515625\n",
      "Epoch [1932], val_loss: 18299.9902\n",
      "gradient norm: 2108.4033966064453, minimum ratio: 0.7734375\n",
      "Epoch [1933], val_loss: 18333.8887\n",
      "gradient norm: 2111.16650390625, minimum ratio: 0.76953125\n",
      "Epoch [1934], val_loss: 18367.8281\n",
      "gradient norm: 2113.931121826172, minimum ratio: 0.765625\n",
      "Epoch [1935], val_loss: 18401.8125\n",
      "gradient norm: 2116.697784423828, minimum ratio: 0.77734375\n",
      "Epoch [1936], val_loss: 18435.8359\n",
      "gradient norm: 2119.3817138671875, minimum ratio: 0.7734375\n",
      "Epoch [1937], val_loss: 18469.9102\n",
      "gradient norm: 2122.110382080078, minimum ratio: 0.77734375\n",
      "Epoch [1938], val_loss: 18504.0293\n",
      "gradient norm: 2124.8831481933594, minimum ratio: 0.7734375\n",
      "Epoch [1939], val_loss: 18538.1934\n",
      "gradient norm: 2127.5851287841797, minimum ratio: 0.78125\n",
      "Epoch [1940], val_loss: 18572.3965\n",
      "gradient norm: 2130.291030883789, minimum ratio: 0.765625\n",
      "Epoch [1941], val_loss: 18606.6387\n",
      "gradient norm: 2133.0076904296875, minimum ratio: 0.7890625\n",
      "Epoch [1942], val_loss: 18640.9258\n",
      "gradient norm: 2135.787368774414, minimum ratio: 0.78125\n",
      "Epoch [1943], val_loss: 18675.2461\n",
      "gradient norm: 2138.533676147461, minimum ratio: 0.7890625\n",
      "Epoch [1944], val_loss: 18709.6172\n",
      "gradient norm: 2141.254196166992, minimum ratio: 0.7734375\n",
      "Epoch [1945], val_loss: 18744.0410\n",
      "gradient norm: 2143.9896697998047, minimum ratio: 0.77734375\n",
      "Epoch [1946], val_loss: 18778.5059\n",
      "gradient norm: 2146.7779235839844, minimum ratio: 0.79296875\n",
      "Epoch [1947], val_loss: 18813.0176\n",
      "gradient norm: 2149.5332641601562, minimum ratio: 0.796875\n",
      "Epoch [1948], val_loss: 18847.5703\n",
      "gradient norm: 2152.2877655029297, minimum ratio: 0.79296875\n",
      "Epoch [1949], val_loss: 18882.1582\n",
      "gradient norm: 2155.0799255371094, minimum ratio: 0.78125\n",
      "Epoch [1950], val_loss: 18916.7891\n",
      "gradient norm: 2157.8736114501953, minimum ratio: 0.78125\n",
      "Epoch [1951], val_loss: 18951.4668\n",
      "gradient norm: 2160.6541748046875, minimum ratio: 0.7734375\n",
      "Epoch [1952], val_loss: 18986.1875\n",
      "gradient norm: 2163.4111938476562, minimum ratio: 0.7890625\n",
      "Epoch [1953], val_loss: 19020.9512\n",
      "gradient norm: 2166.179916381836, minimum ratio: 0.78515625\n",
      "Epoch [1954], val_loss: 19055.7754\n",
      "gradient norm: 2168.985137939453, minimum ratio: 0.7890625\n",
      "Epoch [1955], val_loss: 19090.6406\n",
      "gradient norm: 2171.791458129883, minimum ratio: 0.77734375\n",
      "Epoch [1956], val_loss: 19125.5449\n",
      "gradient norm: 2174.529769897461, minimum ratio: 0.77734375\n",
      "Epoch [1957], val_loss: 19160.4961\n",
      "gradient norm: 2177.340301513672, minimum ratio: 0.79296875\n",
      "Epoch [1958], val_loss: 19195.4902\n",
      "gradient norm: 2180.107421875, minimum ratio: 0.78515625\n",
      "Epoch [1959], val_loss: 19230.5176\n",
      "gradient norm: 2182.9197692871094, minimum ratio: 0.796875\n",
      "Epoch [1960], val_loss: 19265.6035\n",
      "gradient norm: 2185.6637115478516, minimum ratio: 0.78125\n",
      "Epoch [1961], val_loss: 19300.7305\n",
      "gradient norm: 2188.3905334472656, minimum ratio: 0.7734375\n",
      "Epoch [1962], val_loss: 19335.9062\n",
      "gradient norm: 2190.9541778564453, minimum ratio: 0.77734375\n",
      "Epoch [1963], val_loss: 19371.1348\n",
      "gradient norm: 2193.777603149414, minimum ratio: 0.80078125\n",
      "Epoch [1964], val_loss: 19406.4082\n",
      "gradient norm: 2196.6035766601562, minimum ratio: 0.7734375\n",
      "Epoch [1965], val_loss: 19441.7168\n",
      "gradient norm: 2199.4052124023438, minimum ratio: 0.77734375\n",
      "Epoch [1966], val_loss: 19477.0645\n",
      "gradient norm: 2202.1886444091797, minimum ratio: 0.78515625\n",
      "Epoch [1967], val_loss: 19512.4570\n",
      "gradient norm: 2205.0171661376953, minimum ratio: 0.77734375\n",
      "Epoch [1968], val_loss: 19547.8926\n",
      "gradient norm: 2207.7635040283203, minimum ratio: 0.76953125\n",
      "Epoch [1969], val_loss: 19583.3770\n",
      "gradient norm: 2210.5506896972656, minimum ratio: 0.77734375\n",
      "Epoch [1970], val_loss: 19618.9043\n",
      "gradient norm: 2213.3395385742188, minimum ratio: 0.78125\n",
      "Epoch [1971], val_loss: 19654.4785\n",
      "gradient norm: 2216.0101165771484, minimum ratio: 0.78515625\n",
      "Epoch [1972], val_loss: 19690.1035\n",
      "gradient norm: 2218.849624633789, minimum ratio: 0.7734375\n",
      "Epoch [1973], val_loss: 19725.7793\n",
      "gradient norm: 2221.6929779052734, minimum ratio: 0.7890625\n",
      "Epoch [1974], val_loss: 19761.4941\n",
      "gradient norm: 2224.494110107422, minimum ratio: 0.7890625\n",
      "Epoch [1975], val_loss: 19797.2598\n",
      "gradient norm: 2227.278030395508, minimum ratio: 0.77734375\n",
      "Epoch [1976], val_loss: 19833.0703\n",
      "gradient norm: 2230.0635375976562, minimum ratio: 0.7734375\n",
      "Epoch [1977], val_loss: 19868.9238\n",
      "gradient norm: 2232.913787841797, minimum ratio: 0.76953125\n",
      "Epoch [1978], val_loss: 19904.8105\n",
      "gradient norm: 2235.724105834961, minimum ratio: 0.8046875\n",
      "Epoch [1979], val_loss: 19940.7461\n",
      "gradient norm: 2238.5769958496094, minimum ratio: 0.796875\n",
      "Epoch [1980], val_loss: 19976.7227\n",
      "gradient norm: 2241.3954467773438, minimum ratio: 0.80078125\n",
      "Epoch [1981], val_loss: 20012.7422\n",
      "gradient norm: 2244.251724243164, minimum ratio: 0.79296875\n",
      "Epoch [1982], val_loss: 20048.8203\n",
      "gradient norm: 2247.1117401123047, minimum ratio: 0.7734375\n",
      "Epoch [1983], val_loss: 20084.9473\n",
      "gradient norm: 2249.8931732177734, minimum ratio: 0.77734375\n",
      "Epoch [1984], val_loss: 20121.1211\n",
      "gradient norm: 2252.6869354248047, minimum ratio: 0.78515625\n",
      "Epoch [1985], val_loss: 20157.3457\n",
      "gradient norm: 2255.518310546875, minimum ratio: 0.7734375\n",
      "Epoch [1986], val_loss: 20193.6328\n",
      "gradient norm: 2258.3141021728516, minimum ratio: 0.78515625\n",
      "Epoch [1987], val_loss: 20229.9492\n",
      "gradient norm: 2261.1448822021484, minimum ratio: 0.78515625\n",
      "Epoch [1988], val_loss: 20266.3164\n",
      "gradient norm: 2264.0209045410156, minimum ratio: 0.78125\n",
      "Epoch [1989], val_loss: 20302.7266\n",
      "gradient norm: 2266.898880004883, minimum ratio: 0.76953125\n",
      "Epoch [1990], val_loss: 20339.1875\n",
      "gradient norm: 2269.780075073242, minimum ratio: 0.78125\n",
      "Epoch [1991], val_loss: 20375.7031\n",
      "gradient norm: 2272.664337158203, minimum ratio: 0.77734375\n",
      "Epoch [1992], val_loss: 20412.2559\n",
      "gradient norm: 2275.550048828125, minimum ratio: 0.76953125\n",
      "Epoch [1993], val_loss: 20448.8574\n",
      "gradient norm: 2278.3544158935547, minimum ratio: 0.765625\n",
      "Epoch [1994], val_loss: 20485.5059\n",
      "gradient norm: 2281.245315551758, minimum ratio: 0.78125\n",
      "Epoch [1995], val_loss: 20522.1973\n",
      "gradient norm: 2284.1231536865234, minimum ratio: 0.7890625\n",
      "Epoch [1996], val_loss: 20558.9355\n",
      "gradient norm: 2287.0171661376953, minimum ratio: 0.79296875\n",
      "Epoch [1997], val_loss: 20595.7148\n",
      "gradient norm: 2289.8326721191406, minimum ratio: 0.78515625\n",
      "Epoch [1998], val_loss: 20632.5469\n",
      "gradient norm: 2292.680191040039, minimum ratio: 0.76953125\n",
      "Epoch [1999], val_loss: 20669.4297\n"
     ]
    }
   ],
   "source": [
    "history_1,grad_norm_1,model  = fit(num_epochs, lr, model, data_loader, criterion,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9ba1f99",
    "outputId": "82ad84f9-815c-49bd-d573-c3844be7ee27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'grad_norm': 0.1595542598515749, 'ratio': 0.6953125},\n",
       " 1: {'grad_norm': 0.2094506649300456, 'ratio': 0.72265625},\n",
       " 2: {'grad_norm': 0.26820208225399256, 'ratio': 0.71875},\n",
       " 3: {'grad_norm': 0.33107542991638184, 'ratio': 0.7109375},\n",
       " 4: {'grad_norm': 0.3960586152970791, 'ratio': 0.72265625},\n",
       " 5: {'grad_norm': 0.4622521437704563, 'ratio': 0.7265625},\n",
       " 6: {'grad_norm': 0.5292240530252457, 'ratio': 0.71875},\n",
       " 7: {'grad_norm': 0.5967067517340183, 'ratio': 0.734375},\n",
       " 8: {'grad_norm': 0.6645790655165911, 'ratio': 0.73828125},\n",
       " 9: {'grad_norm': 0.7327027712017298, 'ratio': 0.7265625},\n",
       " 10: {'grad_norm': 0.8010115306824446, 'ratio': 0.74609375},\n",
       " 11: {'grad_norm': 0.8694694936275482, 'ratio': 0.75390625},\n",
       " 12: {'grad_norm': 0.9380294010043144, 'ratio': 0.7421875},\n",
       " 13: {'grad_norm': 1.0066642668098211, 'ratio': 0.72265625},\n",
       " 14: {'grad_norm': 1.0753631498664618, 'ratio': 0.734375},\n",
       " 15: {'grad_norm': 1.1441313102841377, 'ratio': 0.74609375},\n",
       " 16: {'grad_norm': 1.2129315882921219, 'ratio': 0.74609375},\n",
       " 17: {'grad_norm': 1.2817705646157265, 'ratio': 0.72265625},\n",
       " 18: {'grad_norm': 1.3506463151425123, 'ratio': 0.7265625},\n",
       " 19: {'grad_norm': 1.419563189148903, 'ratio': 0.7421875},\n",
       " 20: {'grad_norm': 1.4885116927325726, 'ratio': 0.75390625},\n",
       " 21: {'grad_norm': 1.5574947148561478, 'ratio': 0.73046875},\n",
       " 22: {'grad_norm': 1.6264960542321205, 'ratio': 0.7578125},\n",
       " 23: {'grad_norm': 1.6955148689448833, 'ratio': 0.73828125},\n",
       " 24: {'grad_norm': 1.7645537219941616, 'ratio': 0.734375},\n",
       " 25: {'grad_norm': 1.8336182422935963, 'ratio': 0.7421875},\n",
       " 26: {'grad_norm': 1.902698252350092, 'ratio': 0.74609375},\n",
       " 27: {'grad_norm': 1.9718145467340946, 'ratio': 0.734375},\n",
       " 28: {'grad_norm': 2.040947999805212, 'ratio': 0.765625},\n",
       " 29: {'grad_norm': 2.110126093029976, 'ratio': 0.75390625},\n",
       " 30: {'grad_norm': 2.1793279498815536, 'ratio': 0.76171875},\n",
       " 31: {'grad_norm': 2.2485699616372585, 'ratio': 0.7578125},\n",
       " 32: {'grad_norm': 2.317864004522562, 'ratio': 0.7421875},\n",
       " 33: {'grad_norm': 2.3871997334063053, 'ratio': 0.74609375},\n",
       " 34: {'grad_norm': 2.4565730802714825, 'ratio': 0.75390625},\n",
       " 35: {'grad_norm': 2.5260083004832268, 'ratio': 0.76953125},\n",
       " 36: {'grad_norm': 2.5954735167324543, 'ratio': 0.75},\n",
       " 37: {'grad_norm': 2.6649860218167305, 'ratio': 0.75},\n",
       " 38: {'grad_norm': 2.7345538921654224, 'ratio': 0.75390625},\n",
       " 39: {'grad_norm': 2.8041690476238728, 'ratio': 0.73828125},\n",
       " 40: {'grad_norm': 2.8738328889012337, 'ratio': 0.7421875},\n",
       " 41: {'grad_norm': 2.943562127649784, 'ratio': 0.78125},\n",
       " 42: {'grad_norm': 3.013341136276722, 'ratio': 0.74609375},\n",
       " 43: {'grad_norm': 3.083170846104622, 'ratio': 0.7421875},\n",
       " 44: {'grad_norm': 3.153075151145458, 'ratio': 0.73046875},\n",
       " 45: {'grad_norm': 3.2230469062924385, 'ratio': 0.7578125},\n",
       " 46: {'grad_norm': 3.2931126430630684, 'ratio': 0.7421875},\n",
       " 47: {'grad_norm': 3.3632274121046066, 'ratio': 0.75390625},\n",
       " 48: {'grad_norm': 3.433441236615181, 'ratio': 0.74609375},\n",
       " 49: {'grad_norm': 3.5037543326616287, 'ratio': 0.75},\n",
       " 50: {'grad_norm': 3.5741244927048683, 'ratio': 0.76953125},\n",
       " 51: {'grad_norm': 3.6446010023355484, 'ratio': 0.75390625},\n",
       " 52: {'grad_norm': 3.715147949755192, 'ratio': 0.76171875},\n",
       " 53: {'grad_norm': 3.7857967615127563, 'ratio': 0.77734375},\n",
       " 54: {'grad_norm': 3.8565235286951065, 'ratio': 0.74609375},\n",
       " 55: {'grad_norm': 3.927388660609722, 'ratio': 0.73828125},\n",
       " 56: {'grad_norm': 3.9983451887965202, 'ratio': 0.7578125},\n",
       " 57: {'grad_norm': 4.069390468299389, 'ratio': 0.75},\n",
       " 58: {'grad_norm': 4.140541560947895, 'ratio': 0.7578125},\n",
       " 59: {'grad_norm': 4.211826652288437, 'ratio': 0.75390625},\n",
       " 60: {'grad_norm': 4.283198304474354, 'ratio': 0.7578125},\n",
       " 61: {'grad_norm': 4.354689441621304, 'ratio': 0.734375},\n",
       " 62: {'grad_norm': 4.4263667315244675, 'ratio': 0.765625},\n",
       " 63: {'grad_norm': 4.498207218945026, 'ratio': 0.7578125},\n",
       " 64: {'grad_norm': 4.570139117538929, 'ratio': 0.7578125},\n",
       " 65: {'grad_norm': 4.642175495624542, 'ratio': 0.76171875},\n",
       " 66: {'grad_norm': 4.714385360479355, 'ratio': 0.76171875},\n",
       " 67: {'grad_norm': 4.786804534494877, 'ratio': 0.76171875},\n",
       " 68: {'grad_norm': 4.859292410314083, 'ratio': 0.7578125},\n",
       " 69: {'grad_norm': 4.931917458772659, 'ratio': 0.7578125},\n",
       " 70: {'grad_norm': 5.004721410572529, 'ratio': 0.7578125},\n",
       " 71: {'grad_norm': 5.077643357217312, 'ratio': 0.7578125},\n",
       " 72: {'grad_norm': 5.150816932320595, 'ratio': 0.76953125},\n",
       " 73: {'grad_norm': 5.224122770130634, 'ratio': 0.74609375},\n",
       " 74: {'grad_norm': 5.297606125473976, 'ratio': 0.7421875},\n",
       " 75: {'grad_norm': 5.37124228477478, 'ratio': 0.7578125},\n",
       " 76: {'grad_norm': 5.445077687501907, 'ratio': 0.76953125},\n",
       " 77: {'grad_norm': 5.519049882888794, 'ratio': 0.765625},\n",
       " 78: {'grad_norm': 5.593173161149025, 'ratio': 0.78125},\n",
       " 79: {'grad_norm': 5.667496636509895, 'ratio': 0.7734375},\n",
       " 80: {'grad_norm': 5.742038577795029, 'ratio': 0.76171875},\n",
       " 81: {'grad_norm': 5.816716060042381, 'ratio': 0.7734375},\n",
       " 82: {'grad_norm': 5.891599953174591, 'ratio': 0.7578125},\n",
       " 83: {'grad_norm': 5.966644749045372, 'ratio': 0.76953125},\n",
       " 84: {'grad_norm': 6.0419289618730545, 'ratio': 0.76171875},\n",
       " 85: {'grad_norm': 6.11739182472229, 'ratio': 0.7578125},\n",
       " 86: {'grad_norm': 6.193044945597649, 'ratio': 0.78125},\n",
       " 87: {'grad_norm': 6.268871992826462, 'ratio': 0.76953125},\n",
       " 88: {'grad_norm': 6.345002084970474, 'ratio': 0.76171875},\n",
       " 89: {'grad_norm': 6.421279281377792, 'ratio': 0.765625},\n",
       " 90: {'grad_norm': 6.497784644365311, 'ratio': 0.75390625},\n",
       " 91: {'grad_norm': 6.574507340788841, 'ratio': 0.76171875},\n",
       " 92: {'grad_norm': 6.6515453308820724, 'ratio': 0.7734375},\n",
       " 93: {'grad_norm': 6.728732347488403, 'ratio': 0.7890625},\n",
       " 94: {'grad_norm': 6.806164249777794, 'ratio': 0.7578125},\n",
       " 95: {'grad_norm': 6.883824184536934, 'ratio': 0.7578125},\n",
       " 96: {'grad_norm': 6.961805611848831, 'ratio': 0.75390625},\n",
       " 97: {'grad_norm': 7.039968430995941, 'ratio': 0.76953125},\n",
       " 98: {'grad_norm': 7.118344575166702, 'ratio': 0.76953125},\n",
       " 99: {'grad_norm': 7.1968682408332825, 'ratio': 0.7578125},\n",
       " 100: {'grad_norm': 7.275738641619682, 'ratio': 0.7578125},\n",
       " 101: {'grad_norm': 7.354835346341133, 'ratio': 0.765625},\n",
       " 102: {'grad_norm': 7.4341786950826645, 'ratio': 0.77734375},\n",
       " 103: {'grad_norm': 7.513653665781021, 'ratio': 0.7734375},\n",
       " 104: {'grad_norm': 7.593543991446495, 'ratio': 0.76171875},\n",
       " 105: {'grad_norm': 7.673623815178871, 'ratio': 0.75390625},\n",
       " 106: {'grad_norm': 7.754004195332527, 'ratio': 0.77734375},\n",
       " 107: {'grad_norm': 7.8346433490514755, 'ratio': 0.77734375},\n",
       " 108: {'grad_norm': 7.915520042181015, 'ratio': 0.75},\n",
       " 109: {'grad_norm': 7.996672257781029, 'ratio': 0.78125},\n",
       " 110: {'grad_norm': 8.078104808926582, 'ratio': 0.78125},\n",
       " 111: {'grad_norm': 8.159812152385712, 'ratio': 0.76953125},\n",
       " 112: {'grad_norm': 8.241850912570953, 'ratio': 0.77734375},\n",
       " 113: {'grad_norm': 8.324113488197327, 'ratio': 0.7890625},\n",
       " 114: {'grad_norm': 8.40665477514267, 'ratio': 0.76171875},\n",
       " 115: {'grad_norm': 8.48942843079567, 'ratio': 0.7421875},\n",
       " 116: {'grad_norm': 8.57265517115593, 'ratio': 0.77734375},\n",
       " 117: {'grad_norm': 8.656071946024895, 'ratio': 0.76171875},\n",
       " 118: {'grad_norm': 8.739738211035728, 'ratio': 0.7578125},\n",
       " 119: {'grad_norm': 8.823742970824242, 'ratio': 0.75},\n",
       " 120: {'grad_norm': 8.908215299248695, 'ratio': 0.7890625},\n",
       " 121: {'grad_norm': 8.99287286400795, 'ratio': 0.77734375},\n",
       " 122: {'grad_norm': 9.077823847532272, 'ratio': 0.76953125},\n",
       " 123: {'grad_norm': 9.163044422864914, 'ratio': 0.76171875},\n",
       " 124: {'grad_norm': 9.248678624629974, 'ratio': 0.7578125},\n",
       " 125: {'grad_norm': 9.334556698799133, 'ratio': 0.76953125},\n",
       " 126: {'grad_norm': 9.420774042606354, 'ratio': 0.7578125},\n",
       " 127: {'grad_norm': 9.507086127996445, 'ratio': 0.76953125},\n",
       " 128: {'grad_norm': 9.59396880865097, 'ratio': 0.77734375},\n",
       " 129: {'grad_norm': 9.681065499782562, 'ratio': 0.796875},\n",
       " 130: {'grad_norm': 9.768432199954987, 'ratio': 0.75390625},\n",
       " 131: {'grad_norm': 9.856180846691132, 'ratio': 0.77734375},\n",
       " 132: {'grad_norm': 9.944322139024734, 'ratio': 0.77734375},\n",
       " 133: {'grad_norm': 10.032695472240448, 'ratio': 0.78515625},\n",
       " 134: {'grad_norm': 10.121411710977554, 'ratio': 0.76953125},\n",
       " 135: {'grad_norm': 10.210413455963135, 'ratio': 0.78125},\n",
       " 136: {'grad_norm': 10.299745827913284, 'ratio': 0.76171875},\n",
       " 137: {'grad_norm': 10.389426171779633, 'ratio': 0.7578125},\n",
       " 138: {'grad_norm': 10.479498714208603, 'ratio': 0.76171875},\n",
       " 139: {'grad_norm': 10.569867074489594, 'ratio': 0.7734375},\n",
       " 140: {'grad_norm': 10.660697907209396, 'ratio': 0.7734375},\n",
       " 141: {'grad_norm': 10.751773566007614, 'ratio': 0.765625},\n",
       " 142: {'grad_norm': 10.84323900938034, 'ratio': 0.77734375},\n",
       " 143: {'grad_norm': 10.934924334287643, 'ratio': 0.796875},\n",
       " 144: {'grad_norm': 11.027116864919662, 'ratio': 0.76953125},\n",
       " 145: {'grad_norm': 11.11952629685402, 'ratio': 0.7578125},\n",
       " 146: {'grad_norm': 11.212225914001465, 'ratio': 0.76171875},\n",
       " 147: {'grad_norm': 11.305315881967545, 'ratio': 0.765625},\n",
       " 148: {'grad_norm': 11.398839294910431, 'ratio': 0.76171875},\n",
       " 149: {'grad_norm': 11.492722243070602, 'ratio': 0.77734375},\n",
       " 150: {'grad_norm': 11.58699581027031, 'ratio': 0.76953125},\n",
       " 151: {'grad_norm': 11.681494921445847, 'ratio': 0.77734375},\n",
       " 152: {'grad_norm': 11.776516914367676, 'ratio': 0.765625},\n",
       " 153: {'grad_norm': 11.872004240751266, 'ratio': 0.77734375},\n",
       " 154: {'grad_norm': 11.96747624874115, 'ratio': 0.78515625},\n",
       " 155: {'grad_norm': 12.063512951135635, 'ratio': 0.79296875},\n",
       " 156: {'grad_norm': 12.160088211297989, 'ratio': 0.7734375},\n",
       " 157: {'grad_norm': 12.256972879171371, 'ratio': 0.78515625},\n",
       " 158: {'grad_norm': 12.354355543851852, 'ratio': 0.74609375},\n",
       " 159: {'grad_norm': 12.451938480138779, 'ratio': 0.78515625},\n",
       " 160: {'grad_norm': 12.550048440694809, 'ratio': 0.76953125},\n",
       " 161: {'grad_norm': 12.648294001817703, 'ratio': 0.7734375},\n",
       " 162: {'grad_norm': 12.747056871652603, 'ratio': 0.75390625},\n",
       " 163: {'grad_norm': 12.846123069524765, 'ratio': 0.78125},\n",
       " 164: {'grad_norm': 12.94593670964241, 'ratio': 0.77734375},\n",
       " 165: {'grad_norm': 13.04587498307228, 'ratio': 0.765625},\n",
       " 166: {'grad_norm': 13.14609882235527, 'ratio': 0.79296875},\n",
       " 167: {'grad_norm': 13.24698743224144, 'ratio': 0.7734375},\n",
       " 168: {'grad_norm': 13.348338305950165, 'ratio': 0.79296875},\n",
       " 169: {'grad_norm': 13.449901819229126, 'ratio': 0.78125},\n",
       " 170: {'grad_norm': 13.552005767822266, 'ratio': 0.75390625},\n",
       " 171: {'grad_norm': 13.65447935461998, 'ratio': 0.765625},\n",
       " 172: {'grad_norm': 13.757465094327927, 'ratio': 0.765625},\n",
       " 173: {'grad_norm': 13.86077681183815, 'ratio': 0.76953125},\n",
       " 174: {'grad_norm': 13.96444508433342, 'ratio': 0.78515625},\n",
       " 175: {'grad_norm': 14.068779796361923, 'ratio': 0.78515625},\n",
       " 176: {'grad_norm': 14.173608273267746, 'ratio': 0.77734375},\n",
       " 177: {'grad_norm': 14.27859702706337, 'ratio': 0.78125},\n",
       " 178: {'grad_norm': 14.384081810712814, 'ratio': 0.80078125},\n",
       " 179: {'grad_norm': 14.489714711904526, 'ratio': 0.80078125},\n",
       " 180: {'grad_norm': 14.596275985240936, 'ratio': 0.77734375},\n",
       " 181: {'grad_norm': 14.702959775924683, 'ratio': 0.7734375},\n",
       " 182: {'grad_norm': 14.809945344924927, 'ratio': 0.76953125},\n",
       " 183: {'grad_norm': 14.917517006397247, 'ratio': 0.77734375},\n",
       " 184: {'grad_norm': 15.02567046880722, 'ratio': 0.78515625},\n",
       " 185: {'grad_norm': 15.134279906749725, 'ratio': 0.76953125},\n",
       " 186: {'grad_norm': 15.243325412273407, 'ratio': 0.7734375},\n",
       " 187: {'grad_norm': 15.352776765823364, 'ratio': 0.7890625},\n",
       " 188: {'grad_norm': 15.4631068110466, 'ratio': 0.76953125},\n",
       " 189: {'grad_norm': 15.573565125465393, 'ratio': 0.77734375},\n",
       " 190: {'grad_norm': 15.68432754278183, 'ratio': 0.80859375},\n",
       " 191: {'grad_norm': 15.795458197593689, 'ratio': 0.77734375},\n",
       " 192: {'grad_norm': 15.907231450080872, 'ratio': 0.77734375},\n",
       " 193: {'grad_norm': 16.019344449043274, 'ratio': 0.7890625},\n",
       " 194: {'grad_norm': 16.132106363773346, 'ratio': 0.78515625},\n",
       " 195: {'grad_norm': 16.245247185230255, 'ratio': 0.7890625},\n",
       " 196: {'grad_norm': 16.359030842781067, 'ratio': 0.765625},\n",
       " 197: {'grad_norm': 16.47320795059204, 'ratio': 0.765625},\n",
       " 198: {'grad_norm': 16.587705850601196, 'ratio': 0.7890625},\n",
       " 199: {'grad_norm': 16.702760457992554, 'ratio': 0.7734375},\n",
       " 200: {'grad_norm': 16.81817764043808, 'ratio': 0.7890625},\n",
       " 201: {'grad_norm': 16.9343404173851, 'ratio': 0.7890625},\n",
       " 202: {'grad_norm': 17.050927758216858, 'ratio': 0.7890625},\n",
       " 203: {'grad_norm': 17.167871117591858, 'ratio': 0.78515625},\n",
       " 204: {'grad_norm': 17.2851642370224, 'ratio': 0.80859375},\n",
       " 205: {'grad_norm': 17.40317314863205, 'ratio': 0.78515625},\n",
       " 206: {'grad_norm': 17.521783649921417, 'ratio': 0.7890625},\n",
       " 207: {'grad_norm': 17.640821397304535, 'ratio': 0.78125},\n",
       " 208: {'grad_norm': 17.76016789674759, 'ratio': 0.765625},\n",
       " 209: {'grad_norm': 17.88019359111786, 'ratio': 0.76953125},\n",
       " 210: {'grad_norm': 18.00052285194397, 'ratio': 0.78515625},\n",
       " 211: {'grad_norm': 18.121722280979156, 'ratio': 0.7890625},\n",
       " 212: {'grad_norm': 18.243246495723724, 'ratio': 0.78515625},\n",
       " 213: {'grad_norm': 18.365497052669525, 'ratio': 0.80078125},\n",
       " 214: {'grad_norm': 18.488178849220276, 'ratio': 0.78515625},\n",
       " 215: {'grad_norm': 18.61121916770935, 'ratio': 0.7734375},\n",
       " 216: {'grad_norm': 18.73475354909897, 'ratio': 0.7890625},\n",
       " 217: {'grad_norm': 18.859077990055084, 'ratio': 0.78125},\n",
       " 218: {'grad_norm': 18.98362970352173, 'ratio': 0.80078125},\n",
       " 219: {'grad_norm': 19.108748614788055, 'ratio': 0.78125},\n",
       " 220: {'grad_norm': 19.23454123735428, 'ratio': 0.7734375},\n",
       " 221: {'grad_norm': 19.360678136348724, 'ratio': 0.80078125},\n",
       " 222: {'grad_norm': 19.487628281116486, 'ratio': 0.77734375},\n",
       " 223: {'grad_norm': 19.61468207836151, 'ratio': 0.78125},\n",
       " 224: {'grad_norm': 19.742334067821503, 'ratio': 0.8046875},\n",
       " 225: {'grad_norm': 19.87104856967926, 'ratio': 0.7890625},\n",
       " 226: {'grad_norm': 20.000033259391785, 'ratio': 0.75390625},\n",
       " 227: {'grad_norm': 20.12926185131073, 'ratio': 0.76953125},\n",
       " 228: {'grad_norm': 20.258980572223663, 'ratio': 0.78125},\n",
       " 229: {'grad_norm': 20.389650344848633, 'ratio': 0.765625},\n",
       " 230: {'grad_norm': 20.520667254924774, 'ratio': 0.76953125},\n",
       " 231: {'grad_norm': 20.652153968811035, 'ratio': 0.7890625},\n",
       " 232: {'grad_norm': 20.784332871437073, 'ratio': 0.7890625},\n",
       " 233: {'grad_norm': 20.916797041893005, 'ratio': 0.796875},\n",
       " 234: {'grad_norm': 21.05013871192932, 'ratio': 0.7734375},\n",
       " 235: {'grad_norm': 21.184303641319275, 'ratio': 0.80078125},\n",
       " 236: {'grad_norm': 21.318386018276215, 'ratio': 0.80859375},\n",
       " 237: {'grad_norm': 21.453251838684082, 'ratio': 0.78515625},\n",
       " 238: {'grad_norm': 21.588607728481293, 'ratio': 0.80078125},\n",
       " 239: {'grad_norm': 21.72487962245941, 'ratio': 0.77734375},\n",
       " 240: {'grad_norm': 21.861485838890076, 'ratio': 0.796875},\n",
       " 241: {'grad_norm': 21.99835878610611, 'ratio': 0.78515625},\n",
       " 242: {'grad_norm': 22.1361044049263, 'ratio': 0.78515625},\n",
       " 243: {'grad_norm': 22.274724781513214, 'ratio': 0.79296875},\n",
       " 244: {'grad_norm': 22.413540482521057, 'ratio': 0.78125},\n",
       " 245: {'grad_norm': 22.55281662940979, 'ratio': 0.78515625},\n",
       " 246: {'grad_norm': 22.69246244430542, 'ratio': 0.7890625},\n",
       " 247: {'grad_norm': 22.833127737045288, 'ratio': 0.7734375},\n",
       " 248: {'grad_norm': 22.974510729312897, 'ratio': 0.77734375},\n",
       " 249: {'grad_norm': 23.116175651550293, 'ratio': 0.76953125},\n",
       " 250: {'grad_norm': 23.258371829986572, 'ratio': 0.78515625},\n",
       " 251: {'grad_norm': 23.40132212638855, 'ratio': 0.76953125},\n",
       " 252: {'grad_norm': 23.544822692871094, 'ratio': 0.78125},\n",
       " 253: {'grad_norm': 23.689165830612183, 'ratio': 0.7890625},\n",
       " 254: {'grad_norm': 23.833796858787537, 'ratio': 0.76171875},\n",
       " 255: {'grad_norm': 23.978774666786194, 'ratio': 0.765625},\n",
       " 256: {'grad_norm': 24.124648928642273, 'ratio': 0.7734375},\n",
       " 257: {'grad_norm': 24.271281242370605, 'ratio': 0.8046875},\n",
       " 258: {'grad_norm': 24.4185152053833, 'ratio': 0.78515625},\n",
       " 259: {'grad_norm': 24.566241145133972, 'ratio': 0.78125},\n",
       " 260: {'grad_norm': 24.7144877910614, 'ratio': 0.78515625},\n",
       " 261: {'grad_norm': 24.863710284233093, 'ratio': 0.79296875},\n",
       " 262: {'grad_norm': 25.013393998146057, 'ratio': 0.80078125},\n",
       " 263: {'grad_norm': 25.163777351379395, 'ratio': 0.78515625},\n",
       " 264: {'grad_norm': 25.314029693603516, 'ratio': 0.78125},\n",
       " 265: {'grad_norm': 25.465564131736755, 'ratio': 0.78125},\n",
       " 266: {'grad_norm': 25.617995500564575, 'ratio': 0.7890625},\n",
       " 267: {'grad_norm': 25.770911812782288, 'ratio': 0.77734375},\n",
       " 268: {'grad_norm': 25.924214959144592, 'ratio': 0.80078125},\n",
       " 269: {'grad_norm': 26.077919840812683, 'ratio': 0.8046875},\n",
       " 270: {'grad_norm': 26.232766151428223, 'ratio': 0.8046875},\n",
       " 271: {'grad_norm': 26.387635231018066, 'ratio': 0.80078125},\n",
       " 272: {'grad_norm': 26.543240547180176, 'ratio': 0.79296875},\n",
       " 273: {'grad_norm': 26.699783086776733, 'ratio': 0.78125},\n",
       " 274: {'grad_norm': 26.856603026390076, 'ratio': 0.78125},\n",
       " 275: {'grad_norm': 27.014711499214172, 'ratio': 0.8125},\n",
       " 276: {'grad_norm': 27.173380732536316, 'ratio': 0.796875},\n",
       " 277: {'grad_norm': 27.332038402557373, 'ratio': 0.7890625},\n",
       " 278: {'grad_norm': 27.491957306861877, 'ratio': 0.796875},\n",
       " 279: {'grad_norm': 27.65227496623993, 'ratio': 0.8125},\n",
       " 280: {'grad_norm': 27.813512206077576, 'ratio': 0.8046875},\n",
       " 281: {'grad_norm': 27.974791049957275, 'ratio': 0.78125},\n",
       " 282: {'grad_norm': 28.13680398464203, 'ratio': 0.80078125},\n",
       " 283: {'grad_norm': 28.299840331077576, 'ratio': 0.796875},\n",
       " 284: {'grad_norm': 28.464003920555115, 'ratio': 0.765625},\n",
       " 285: {'grad_norm': 28.62835204601288, 'ratio': 0.78515625},\n",
       " 286: {'grad_norm': 28.793337106704712, 'ratio': 0.78515625},\n",
       " 287: {'grad_norm': 28.958956718444824, 'ratio': 0.79296875},\n",
       " 288: {'grad_norm': 29.125217080116272, 'ratio': 0.80078125},\n",
       " 289: {'grad_norm': 29.292521595954895, 'ratio': 0.76953125},\n",
       " 290: {'grad_norm': 29.45994210243225, 'ratio': 0.77734375},\n",
       " 291: {'grad_norm': 29.62786030769348, 'ratio': 0.78125},\n",
       " 292: {'grad_norm': 29.796311736106873, 'ratio': 0.78125},\n",
       " 293: {'grad_norm': 29.965842962265015, 'ratio': 0.76953125},\n",
       " 294: {'grad_norm': 30.13609552383423, 'ratio': 0.77734375},\n",
       " 295: {'grad_norm': 30.306761384010315, 'ratio': 0.79296875},\n",
       " 296: {'grad_norm': 30.47876274585724, 'ratio': 0.76953125},\n",
       " 297: {'grad_norm': 30.65106976032257, 'ratio': 0.796875},\n",
       " 298: {'grad_norm': 30.82368803024292, 'ratio': 0.79296875},\n",
       " 299: {'grad_norm': 30.997902750968933, 'ratio': 0.796875},\n",
       " 300: {'grad_norm': 31.17157483100891, 'ratio': 0.80078125},\n",
       " 301: {'grad_norm': 31.346673011779785, 'ratio': 0.7734375},\n",
       " 302: {'grad_norm': 31.522297739982605, 'ratio': 0.77734375},\n",
       " 303: {'grad_norm': 31.69866693019867, 'ratio': 0.78125},\n",
       " 304: {'grad_norm': 31.875772356987, 'ratio': 0.79296875},\n",
       " 305: {'grad_norm': 32.05377781391144, 'ratio': 0.765625},\n",
       " 306: {'grad_norm': 32.232298493385315, 'ratio': 0.80078125},\n",
       " 307: {'grad_norm': 32.41074204444885, 'ratio': 0.76953125},\n",
       " 308: {'grad_norm': 32.59096705913544, 'ratio': 0.80078125},\n",
       " 309: {'grad_norm': 32.771209478378296, 'ratio': 0.78515625},\n",
       " 310: {'grad_norm': 32.95252573490143, 'ratio': 0.796875},\n",
       " 311: {'grad_norm': 33.1345249414444, 'ratio': 0.796875},\n",
       " 312: {'grad_norm': 33.316978335380554, 'ratio': 0.796875},\n",
       " 313: {'grad_norm': 33.50044822692871, 'ratio': 0.81640625},\n",
       " 314: {'grad_norm': 33.68448293209076, 'ratio': 0.7734375},\n",
       " 315: {'grad_norm': 33.869344830513, 'ratio': 0.78515625},\n",
       " 316: {'grad_norm': 34.055440187454224, 'ratio': 0.79296875},\n",
       " 317: {'grad_norm': 34.24182331562042, 'ratio': 0.796875},\n",
       " 318: {'grad_norm': 34.429022908210754, 'ratio': 0.77734375},\n",
       " 319: {'grad_norm': 34.61562430858612, 'ratio': 0.7890625},\n",
       " 320: {'grad_norm': 34.80438756942749, 'ratio': 0.77734375},\n",
       " 321: {'grad_norm': 34.99372434616089, 'ratio': 0.80078125},\n",
       " 322: {'grad_norm': 35.183679819107056, 'ratio': 0.8046875},\n",
       " 323: {'grad_norm': 35.37485122680664, 'ratio': 0.78125},\n",
       " 324: {'grad_norm': 35.5660719871521, 'ratio': 0.80078125},\n",
       " 325: {'grad_norm': 35.758116722106934, 'ratio': 0.796875},\n",
       " 326: {'grad_norm': 35.951019406318665, 'ratio': 0.8125},\n",
       " 327: {'grad_norm': 36.14460361003876, 'ratio': 0.7890625},\n",
       " 328: {'grad_norm': 36.339051604270935, 'ratio': 0.8125},\n",
       " 329: {'grad_norm': 36.53449988365173, 'ratio': 0.79296875},\n",
       " 330: {'grad_norm': 36.729586243629456, 'ratio': 0.80078125},\n",
       " 331: {'grad_norm': 36.92630231380463, 'ratio': 0.7890625},\n",
       " 332: {'grad_norm': 37.123679876327515, 'ratio': 0.81640625},\n",
       " 333: {'grad_norm': 37.32147812843323, 'ratio': 0.7734375},\n",
       " 334: {'grad_norm': 37.519999623298645, 'ratio': 0.8046875},\n",
       " 335: {'grad_norm': 37.719677805900574, 'ratio': 0.79296875},\n",
       " 336: {'grad_norm': 37.919978857040405, 'ratio': 0.80078125},\n",
       " 337: {'grad_norm': 38.12117350101471, 'ratio': 0.765625},\n",
       " 338: {'grad_norm': 38.323237657547, 'ratio': 0.796875},\n",
       " 339: {'grad_norm': 38.525880336761475, 'ratio': 0.78125},\n",
       " 340: {'grad_norm': 38.72966480255127, 'ratio': 0.78515625},\n",
       " 341: {'grad_norm': 38.93336272239685, 'ratio': 0.7890625},\n",
       " 342: {'grad_norm': 39.13803219795227, 'ratio': 0.78515625},\n",
       " 343: {'grad_norm': 39.34333920478821, 'ratio': 0.7890625},\n",
       " 344: {'grad_norm': 39.55007290840149, 'ratio': 0.7890625},\n",
       " 345: {'grad_norm': 39.75719213485718, 'ratio': 0.78125},\n",
       " 346: {'grad_norm': 39.96527314186096, 'ratio': 0.79296875},\n",
       " 347: {'grad_norm': 40.17442035675049, 'ratio': 0.79296875},\n",
       " 348: {'grad_norm': 40.38296461105347, 'ratio': 0.76953125},\n",
       " 349: {'grad_norm': 40.59327149391174, 'ratio': 0.7734375},\n",
       " 350: {'grad_norm': 40.80462908744812, 'ratio': 0.78515625},\n",
       " 351: {'grad_norm': 41.0167236328125, 'ratio': 0.765625},\n",
       " 352: {'grad_norm': 41.22838830947876, 'ratio': 0.79296875},\n",
       " 353: {'grad_norm': 41.4419584274292, 'ratio': 0.78515625},\n",
       " 354: {'grad_norm': 41.6563880443573, 'ratio': 0.78125},\n",
       " 355: {'grad_norm': 41.87197828292847, 'ratio': 0.76953125},\n",
       " 356: {'grad_norm': 42.08744716644287, 'ratio': 0.78515625},\n",
       " 357: {'grad_norm': 42.304038524627686, 'ratio': 0.78515625},\n",
       " 358: {'grad_norm': 42.5214467048645, 'ratio': 0.78515625},\n",
       " 359: {'grad_norm': 42.739027976989746, 'ratio': 0.8203125},\n",
       " 360: {'grad_norm': 42.958298683166504, 'ratio': 0.765625},\n",
       " 361: {'grad_norm': 43.178309202194214, 'ratio': 0.80078125},\n",
       " 362: {'grad_norm': 43.399495124816895, 'ratio': 0.7890625},\n",
       " 363: {'grad_norm': 43.620479345321655, 'ratio': 0.7890625},\n",
       " 364: {'grad_norm': 43.84312129020691, 'ratio': 0.78515625},\n",
       " 365: {'grad_norm': 44.06620478630066, 'ratio': 0.78515625},\n",
       " 366: {'grad_norm': 44.290284872055054, 'ratio': 0.8125},\n",
       " 367: {'grad_norm': 44.514989376068115, 'ratio': 0.7890625},\n",
       " 368: {'grad_norm': 44.741307973861694, 'ratio': 0.7890625},\n",
       " 369: {'grad_norm': 44.96802639961243, 'ratio': 0.796875},\n",
       " 370: {'grad_norm': 45.195592164993286, 'ratio': 0.8046875},\n",
       " 371: {'grad_norm': 45.42417120933533, 'ratio': 0.78515625},\n",
       " 372: {'grad_norm': 45.65339541435242, 'ratio': 0.80859375},\n",
       " 373: {'grad_norm': 45.88276028633118, 'ratio': 0.78515625},\n",
       " 374: {'grad_norm': 46.11378359794617, 'ratio': 0.80859375},\n",
       " 375: {'grad_norm': 46.344905614852905, 'ratio': 0.7890625},\n",
       " 376: {'grad_norm': 46.57682228088379, 'ratio': 0.7890625},\n",
       " 377: {'grad_norm': 46.80987524986267, 'ratio': 0.81640625},\n",
       " 378: {'grad_norm': 47.04327082633972, 'ratio': 0.796875},\n",
       " 379: {'grad_norm': 47.27831029891968, 'ratio': 0.7890625},\n",
       " 380: {'grad_norm': 47.514487981796265, 'ratio': 0.8046875},\n",
       " 381: {'grad_norm': 47.7497239112854, 'ratio': 0.81640625},\n",
       " 382: {'grad_norm': 47.98708915710449, 'ratio': 0.81640625},\n",
       " 383: {'grad_norm': 48.2252140045166, 'ratio': 0.80078125},\n",
       " 384: {'grad_norm': 48.4641695022583, 'ratio': 0.796875},\n",
       " 385: {'grad_norm': 48.70342803001404, 'ratio': 0.79296875},\n",
       " 386: {'grad_norm': 48.94404053688049, 'ratio': 0.796875},\n",
       " 387: {'grad_norm': 49.18424868583679, 'ratio': 0.79296875},\n",
       " 388: {'grad_norm': 49.426485776901245, 'ratio': 0.796875},\n",
       " 389: {'grad_norm': 49.669496297836304, 'ratio': 0.80078125},\n",
       " 390: {'grad_norm': 49.9136905670166, 'ratio': 0.78125},\n",
       " 391: {'grad_norm': 50.159008264541626, 'ratio': 0.80078125},\n",
       " 392: {'grad_norm': 50.404698848724365, 'ratio': 0.79296875},\n",
       " 393: {'grad_norm': 50.6513888835907, 'ratio': 0.796875},\n",
       " 394: {'grad_norm': 50.89918351173401, 'ratio': 0.7890625},\n",
       " 395: {'grad_norm': 51.146549701690674, 'ratio': 0.8046875},\n",
       " 396: {'grad_norm': 51.39583373069763, 'ratio': 0.78125},\n",
       " 397: {'grad_norm': 51.645753622055054, 'ratio': 0.796875},\n",
       " 398: {'grad_norm': 51.897642374038696, 'ratio': 0.7734375},\n",
       " 399: {'grad_norm': 52.150049686431885, 'ratio': 0.78125},\n",
       " 400: {'grad_norm': 52.402657985687256, 'ratio': 0.796875},\n",
       " 401: {'grad_norm': 52.65685558319092, 'ratio': 0.78515625},\n",
       " 402: {'grad_norm': 52.911887407302856, 'ratio': 0.79296875},\n",
       " 403: {'grad_norm': 53.1671142578125, 'ratio': 0.8046875},\n",
       " 404: {'grad_norm': 53.42296624183655, 'ratio': 0.796875},\n",
       " 405: {'grad_norm': 53.68019104003906, 'ratio': 0.77734375},\n",
       " 406: {'grad_norm': 53.93682146072388, 'ratio': 0.8046875},\n",
       " 407: {'grad_norm': 54.19573211669922, 'ratio': 0.796875},\n",
       " 408: {'grad_norm': 54.4550666809082, 'ratio': 0.7890625},\n",
       " 409: {'grad_norm': 54.71548271179199, 'ratio': 0.80078125},\n",
       " 410: {'grad_norm': 54.977296113967896, 'ratio': 0.796875},\n",
       " 411: {'grad_norm': 55.23910331726074, 'ratio': 0.78515625},\n",
       " 412: {'grad_norm': 55.50237512588501, 'ratio': 0.78515625},\n",
       " 413: {'grad_norm': 55.7657425403595, 'ratio': 0.796875},\n",
       " 414: {'grad_norm': 56.03025770187378, 'ratio': 0.796875},\n",
       " 415: {'grad_norm': 56.29656744003296, 'ratio': 0.78125},\n",
       " 416: {'grad_norm': 56.562758445739746, 'ratio': 0.80859375},\n",
       " 417: {'grad_norm': 56.83084034919739, 'ratio': 0.8046875},\n",
       " 418: {'grad_norm': 57.09891629219055, 'ratio': 0.79296875},\n",
       " 419: {'grad_norm': 57.36914944648743, 'ratio': 0.80859375},\n",
       " 420: {'grad_norm': 57.63912749290466, 'ratio': 0.78125},\n",
       " 421: {'grad_norm': 57.90994906425476, 'ratio': 0.78125},\n",
       " 422: {'grad_norm': 58.18233871459961, 'ratio': 0.8046875},\n",
       " 423: {'grad_norm': 58.45580458641052, 'ratio': 0.78515625},\n",
       " 424: {'grad_norm': 58.72980546951294, 'ratio': 0.78515625},\n",
       " 425: {'grad_norm': 59.00456738471985, 'ratio': 0.7890625},\n",
       " 426: {'grad_norm': 59.28049564361572, 'ratio': 0.7890625},\n",
       " 427: {'grad_norm': 59.55701780319214, 'ratio': 0.8125},\n",
       " 428: {'grad_norm': 59.83396124839783, 'ratio': 0.78515625},\n",
       " 429: {'grad_norm': 60.11321949958801, 'ratio': 0.79296875},\n",
       " 430: {'grad_norm': 60.392802238464355, 'ratio': 0.78125},\n",
       " 431: {'grad_norm': 60.67274284362793, 'ratio': 0.8046875},\n",
       " 432: {'grad_norm': 60.954845666885376, 'ratio': 0.80078125},\n",
       " 433: {'grad_norm': 61.23627328872681, 'ratio': 0.78515625},\n",
       " 434: {'grad_norm': 61.519469022750854, 'ratio': 0.7734375},\n",
       " 435: {'grad_norm': 61.80342102050781, 'ratio': 0.79296875},\n",
       " 436: {'grad_norm': 62.08969759941101, 'ratio': 0.77734375},\n",
       " 437: {'grad_norm': 62.376338720321655, 'ratio': 0.76171875},\n",
       " 438: {'grad_norm': 62.66303253173828, 'ratio': 0.76953125},\n",
       " 439: {'grad_norm': 62.95138430595398, 'ratio': 0.78515625},\n",
       " 440: {'grad_norm': 63.24058771133423, 'ratio': 0.796875},\n",
       " 441: {'grad_norm': 63.53021478652954, 'ratio': 0.76953125},\n",
       " 442: {'grad_norm': 63.82154989242554, 'ratio': 0.78515625},\n",
       " 443: {'grad_norm': 64.11332869529724, 'ratio': 0.7890625},\n",
       " 444: {'grad_norm': 64.40676140785217, 'ratio': 0.78515625},\n",
       " 445: {'grad_norm': 64.70092535018921, 'ratio': 0.765625},\n",
       " 446: {'grad_norm': 64.9945821762085, 'ratio': 0.8046875},\n",
       " 447: {'grad_norm': 65.29071474075317, 'ratio': 0.8046875},\n",
       " 448: {'grad_norm': 65.58737516403198, 'ratio': 0.78515625},\n",
       " 449: {'grad_norm': 65.88523435592651, 'ratio': 0.7734375},\n",
       " 450: {'grad_norm': 66.18385553359985, 'ratio': 0.79296875},\n",
       " 451: {'grad_norm': 66.48347568511963, 'ratio': 0.78515625},\n",
       " 452: {'grad_norm': 66.78359508514404, 'ratio': 0.8203125},\n",
       " 453: {'grad_norm': 67.08518028259277, 'ratio': 0.8046875},\n",
       " 454: {'grad_norm': 67.38820171356201, 'ratio': 0.78515625},\n",
       " 455: {'grad_norm': 67.6920337677002, 'ratio': 0.796875},\n",
       " 456: {'grad_norm': 67.9970293045044, 'ratio': 0.79296875},\n",
       " 457: {'grad_norm': 68.30296421051025, 'ratio': 0.796875},\n",
       " 458: {'grad_norm': 68.60947370529175, 'ratio': 0.78125},\n",
       " 459: {'grad_norm': 68.91696739196777, 'ratio': 0.8046875},\n",
       " 460: {'grad_norm': 69.22545957565308, 'ratio': 0.80078125},\n",
       " 461: {'grad_norm': 69.53527498245239, 'ratio': 0.796875},\n",
       " 462: {'grad_norm': 69.84615564346313, 'ratio': 0.8125},\n",
       " 463: {'grad_norm': 70.15693473815918, 'ratio': 0.80078125},\n",
       " 464: {'grad_norm': 70.46906423568726, 'ratio': 0.8125},\n",
       " 465: {'grad_norm': 70.78300619125366, 'ratio': 0.796875},\n",
       " 466: {'grad_norm': 71.09758472442627, 'ratio': 0.796875},\n",
       " 467: {'grad_norm': 71.4134292602539, 'ratio': 0.7890625},\n",
       " 468: {'grad_norm': 71.7300124168396, 'ratio': 0.8046875},\n",
       " 469: {'grad_norm': 72.04635906219482, 'ratio': 0.80078125},\n",
       " 470: {'grad_norm': 72.36480569839478, 'ratio': 0.796875},\n",
       " 471: {'grad_norm': 72.68477249145508, 'ratio': 0.7890625},\n",
       " 472: {'grad_norm': 73.00512886047363, 'ratio': 0.7890625},\n",
       " 473: {'grad_norm': 73.32743120193481, 'ratio': 0.79296875},\n",
       " 474: {'grad_norm': 73.65093994140625, 'ratio': 0.80078125},\n",
       " 475: {'grad_norm': 73.97405529022217, 'ratio': 0.796875},\n",
       " 476: {'grad_norm': 74.29684734344482, 'ratio': 0.78515625},\n",
       " 477: {'grad_norm': 74.62197971343994, 'ratio': 0.7890625},\n",
       " 478: {'grad_norm': 74.94723129272461, 'ratio': 0.796875},\n",
       " 479: {'grad_norm': 75.27503728866577, 'ratio': 0.78515625},\n",
       " 480: {'grad_norm': 75.60373830795288, 'ratio': 0.7890625},\n",
       " 481: {'grad_norm': 75.9330883026123, 'ratio': 0.8046875},\n",
       " 482: {'grad_norm': 76.26353883743286, 'ratio': 0.80078125},\n",
       " 483: {'grad_norm': 76.59563779830933, 'ratio': 0.8203125},\n",
       " 484: {'grad_norm': 76.92787837982178, 'ratio': 0.7734375},\n",
       " 485: {'grad_norm': 77.26139974594116, 'ratio': 0.8046875},\n",
       " 486: {'grad_norm': 77.59587383270264, 'ratio': 0.796875},\n",
       " 487: {'grad_norm': 77.9293704032898, 'ratio': 0.796875},\n",
       " 488: {'grad_norm': 78.266197681427, 'ratio': 0.80078125},\n",
       " 489: {'grad_norm': 78.60364770889282, 'ratio': 0.78125},\n",
       " 490: {'grad_norm': 78.94192409515381, 'ratio': 0.7890625},\n",
       " 491: {'grad_norm': 79.28240585327148, 'ratio': 0.8125},\n",
       " 492: {'grad_norm': 79.62365388870239, 'ratio': 0.81640625},\n",
       " 493: {'grad_norm': 79.96476984024048, 'ratio': 0.8046875},\n",
       " 494: {'grad_norm': 80.30628776550293, 'ratio': 0.7890625},\n",
       " 495: {'grad_norm': 80.64985513687134, 'ratio': 0.80859375},\n",
       " 496: {'grad_norm': 80.99414682388306, 'ratio': 0.796875},\n",
       " 497: {'grad_norm': 81.33891296386719, 'ratio': 0.8125},\n",
       " 498: {'grad_norm': 81.68627786636353, 'ratio': 0.78515625},\n",
       " 499: {'grad_norm': 82.0340256690979, 'ratio': 0.78515625},\n",
       " 500: {'grad_norm': 82.38363313674927, 'ratio': 0.8046875},\n",
       " 501: {'grad_norm': 82.73314762115479, 'ratio': 0.8046875},\n",
       " 502: {'grad_norm': 83.08523082733154, 'ratio': 0.80859375},\n",
       " 503: {'grad_norm': 83.43777132034302, 'ratio': 0.79296875},\n",
       " 504: {'grad_norm': 83.79150295257568, 'ratio': 0.80859375},\n",
       " 505: {'grad_norm': 84.14541864395142, 'ratio': 0.81640625},\n",
       " 506: {'grad_norm': 84.4995379447937, 'ratio': 0.765625},\n",
       " 507: {'grad_norm': 84.85531425476074, 'ratio': 0.796875},\n",
       " 508: {'grad_norm': 85.21168899536133, 'ratio': 0.8046875},\n",
       " 509: {'grad_norm': 85.5702953338623, 'ratio': 0.77734375},\n",
       " 510: {'grad_norm': 85.93018436431885, 'ratio': 0.796875},\n",
       " 511: {'grad_norm': 86.29103899002075, 'ratio': 0.76953125},\n",
       " 512: {'grad_norm': 86.65367221832275, 'ratio': 0.7890625},\n",
       " 513: {'grad_norm': 87.01628160476685, 'ratio': 0.7890625},\n",
       " 514: {'grad_norm': 87.37852382659912, 'ratio': 0.78125},\n",
       " 515: {'grad_norm': 87.74422550201416, 'ratio': 0.77734375},\n",
       " 516: {'grad_norm': 88.11065149307251, 'ratio': 0.796875},\n",
       " 517: {'grad_norm': 88.47853565216064, 'ratio': 0.78515625},\n",
       " 518: {'grad_norm': 88.84627676010132, 'ratio': 0.7890625},\n",
       " 519: {'grad_norm': 89.21548748016357, 'ratio': 0.796875},\n",
       " 520: {'grad_norm': 89.58581113815308, 'ratio': 0.7890625},\n",
       " 521: {'grad_norm': 89.95655012130737, 'ratio': 0.7734375},\n",
       " 522: {'grad_norm': 90.32955694198608, 'ratio': 0.7890625},\n",
       " 523: {'grad_norm': 90.70185613632202, 'ratio': 0.78125},\n",
       " 524: {'grad_norm': 91.07527542114258, 'ratio': 0.796875},\n",
       " 525: {'grad_norm': 91.4507646560669, 'ratio': 0.80859375},\n",
       " 526: {'grad_norm': 91.82654666900635, 'ratio': 0.78125},\n",
       " 527: {'grad_norm': 92.20462846755981, 'ratio': 0.80859375},\n",
       " 528: {'grad_norm': 92.58323431015015, 'ratio': 0.77734375},\n",
       " 529: {'grad_norm': 92.96259307861328, 'ratio': 0.76953125},\n",
       " 530: {'grad_norm': 93.34493780136108, 'ratio': 0.78515625},\n",
       " 531: {'grad_norm': 93.72719669342041, 'ratio': 0.7890625},\n",
       " 532: {'grad_norm': 94.1113715171814, 'ratio': 0.8046875},\n",
       " 533: {'grad_norm': 94.4954743385315, 'ratio': 0.76171875},\n",
       " 534: {'grad_norm': 94.87906265258789, 'ratio': 0.7890625},\n",
       " 535: {'grad_norm': 95.26595258712769, 'ratio': 0.78515625},\n",
       " 536: {'grad_norm': 95.65416860580444, 'ratio': 0.8125},\n",
       " 537: {'grad_norm': 96.04165697097778, 'ratio': 0.8046875},\n",
       " 538: {'grad_norm': 96.4327392578125, 'ratio': 0.77734375},\n",
       " 539: {'grad_norm': 96.82310724258423, 'ratio': 0.79296875},\n",
       " 540: {'grad_norm': 97.21459579467773, 'ratio': 0.7890625},\n",
       " 541: {'grad_norm': 97.60831880569458, 'ratio': 0.7890625},\n",
       " 542: {'grad_norm': 98.00282907485962, 'ratio': 0.8046875},\n",
       " 543: {'grad_norm': 98.39654541015625, 'ratio': 0.79296875},\n",
       " 544: {'grad_norm': 98.79363489151001, 'ratio': 0.77734375},\n",
       " 545: {'grad_norm': 99.19160175323486, 'ratio': 0.77734375},\n",
       " 546: {'grad_norm': 99.59184980392456, 'ratio': 0.79296875},\n",
       " 547: {'grad_norm': 99.99293088912964, 'ratio': 0.765625},\n",
       " 548: {'grad_norm': 100.39248180389404, 'ratio': 0.78125},\n",
       " 549: {'grad_norm': 100.79537868499756, 'ratio': 0.78125},\n",
       " 550: {'grad_norm': 101.19682836532593, 'ratio': 0.765625},\n",
       " 551: {'grad_norm': 101.60061693191528, 'ratio': 0.80078125},\n",
       " 552: {'grad_norm': 102.00423192977905, 'ratio': 0.7890625},\n",
       " 553: {'grad_norm': 102.41238355636597, 'ratio': 0.79296875},\n",
       " 554: {'grad_norm': 102.82027673721313, 'ratio': 0.80078125},\n",
       " 555: {'grad_norm': 103.22910022735596, 'ratio': 0.7890625},\n",
       " 556: {'grad_norm': 103.63550186157227, 'ratio': 0.7734375},\n",
       " 557: {'grad_norm': 104.04722785949707, 'ratio': 0.79296875},\n",
       " 558: {'grad_norm': 104.45963907241821, 'ratio': 0.78125},\n",
       " 559: {'grad_norm': 104.87307643890381, 'ratio': 0.7890625},\n",
       " 560: {'grad_norm': 105.28829860687256, 'ratio': 0.7890625},\n",
       " 561: {'grad_norm': 105.70346355438232, 'ratio': 0.7734375},\n",
       " 562: {'grad_norm': 106.11804533004761, 'ratio': 0.77734375},\n",
       " 563: {'grad_norm': 106.53458499908447, 'ratio': 0.78125},\n",
       " 564: {'grad_norm': 106.95378303527832, 'ratio': 0.7890625},\n",
       " 565: {'grad_norm': 107.37422704696655, 'ratio': 0.77734375},\n",
       " 566: {'grad_norm': 107.79560947418213, 'ratio': 0.796875},\n",
       " 567: {'grad_norm': 108.21783256530762, 'ratio': 0.78515625},\n",
       " 568: {'grad_norm': 108.6421971321106, 'ratio': 0.79296875},\n",
       " 569: {'grad_norm': 109.06486797332764, 'ratio': 0.8046875},\n",
       " 570: {'grad_norm': 109.49130153656006, 'ratio': 0.7890625},\n",
       " 571: {'grad_norm': 109.91688299179077, 'ratio': 0.7890625},\n",
       " 572: {'grad_norm': 110.34334802627563, 'ratio': 0.7890625},\n",
       " 573: {'grad_norm': 110.76966381072998, 'ratio': 0.76953125},\n",
       " 574: {'grad_norm': 111.20028400421143, 'ratio': 0.78125},\n",
       " 575: {'grad_norm': 111.63281154632568, 'ratio': 0.78125},\n",
       " 576: {'grad_norm': 112.06524085998535, 'ratio': 0.76953125},\n",
       " 577: {'grad_norm': 112.49803733825684, 'ratio': 0.79296875},\n",
       " 578: {'grad_norm': 112.93128395080566, 'ratio': 0.78125},\n",
       " 579: {'grad_norm': 113.36825370788574, 'ratio': 0.76953125},\n",
       " 580: {'grad_norm': 113.80703449249268, 'ratio': 0.77734375},\n",
       " 581: {'grad_norm': 114.24319458007812, 'ratio': 0.7890625},\n",
       " 582: {'grad_norm': 114.68292808532715, 'ratio': 0.79296875},\n",
       " 583: {'grad_norm': 115.12251853942871, 'ratio': 0.79296875},\n",
       " 584: {'grad_norm': 115.56279468536377, 'ratio': 0.796875},\n",
       " 585: {'grad_norm': 116.0052375793457, 'ratio': 0.78125},\n",
       " 586: {'grad_norm': 116.44813060760498, 'ratio': 0.7890625},\n",
       " 587: {'grad_norm': 116.89457511901855, 'ratio': 0.78515625},\n",
       " 588: {'grad_norm': 117.34040355682373, 'ratio': 0.796875},\n",
       " 589: {'grad_norm': 117.78848171234131, 'ratio': 0.7734375},\n",
       " 590: {'grad_norm': 118.2357988357544, 'ratio': 0.77734375},\n",
       " 591: {'grad_norm': 118.68449687957764, 'ratio': 0.8046875},\n",
       " 592: {'grad_norm': 119.13445281982422, 'ratio': 0.80078125},\n",
       " 593: {'grad_norm': 119.58486557006836, 'ratio': 0.76953125},\n",
       " 594: {'grad_norm': 120.03816986083984, 'ratio': 0.78515625},\n",
       " 595: {'grad_norm': 120.49219989776611, 'ratio': 0.80078125},\n",
       " 596: {'grad_norm': 120.94523620605469, 'ratio': 0.78515625},\n",
       " 597: {'grad_norm': 121.40263175964355, 'ratio': 0.8125},\n",
       " 598: {'grad_norm': 121.85889339447021, 'ratio': 0.796875},\n",
       " 599: {'grad_norm': 122.31909465789795, 'ratio': 0.78125},\n",
       " 600: {'grad_norm': 122.7786455154419, 'ratio': 0.80078125},\n",
       " 601: {'grad_norm': 123.23907089233398, 'ratio': 0.79296875},\n",
       " 602: {'grad_norm': 123.69989967346191, 'ratio': 0.78515625},\n",
       " 603: {'grad_norm': 124.16410541534424, 'ratio': 0.78515625},\n",
       " 604: {'grad_norm': 124.62820243835449, 'ratio': 0.7890625},\n",
       " 605: {'grad_norm': 125.09370136260986, 'ratio': 0.8125},\n",
       " 606: {'grad_norm': 125.56095600128174, 'ratio': 0.79296875},\n",
       " 607: {'grad_norm': 126.02993106842041, 'ratio': 0.796875},\n",
       " 608: {'grad_norm': 126.4980297088623, 'ratio': 0.8046875},\n",
       " 609: {'grad_norm': 126.96859645843506, 'ratio': 0.77734375},\n",
       " 610: {'grad_norm': 127.43973445892334, 'ratio': 0.7890625},\n",
       " 611: {'grad_norm': 127.91395854949951, 'ratio': 0.7734375},\n",
       " 612: {'grad_norm': 128.38992977142334, 'ratio': 0.80859375},\n",
       " 613: {'grad_norm': 128.86613273620605, 'ratio': 0.7890625},\n",
       " 614: {'grad_norm': 129.3434066772461, 'ratio': 0.7890625},\n",
       " 615: {'grad_norm': 129.81986904144287, 'ratio': 0.79296875},\n",
       " 616: {'grad_norm': 130.2998275756836, 'ratio': 0.8046875},\n",
       " 617: {'grad_norm': 130.77801895141602, 'ratio': 0.78515625},\n",
       " 618: {'grad_norm': 131.2581386566162, 'ratio': 0.79296875},\n",
       " 619: {'grad_norm': 131.7411642074585, 'ratio': 0.80078125},\n",
       " 620: {'grad_norm': 132.2240514755249, 'ratio': 0.80078125},\n",
       " 621: {'grad_norm': 132.70625400543213, 'ratio': 0.7734375},\n",
       " 622: {'grad_norm': 133.19324684143066, 'ratio': 0.81640625},\n",
       " 623: {'grad_norm': 133.67858695983887, 'ratio': 0.7890625},\n",
       " 624: {'grad_norm': 134.16562461853027, 'ratio': 0.796875},\n",
       " 625: {'grad_norm': 134.65437984466553, 'ratio': 0.80078125},\n",
       " 626: {'grad_norm': 135.14440727233887, 'ratio': 0.80078125},\n",
       " 627: {'grad_norm': 135.6366138458252, 'ratio': 0.7890625},\n",
       " 628: {'grad_norm': 136.1300172805786, 'ratio': 0.80859375},\n",
       " 629: {'grad_norm': 136.6239528656006, 'ratio': 0.79296875},\n",
       " 630: {'grad_norm': 137.12032413482666, 'ratio': 0.77734375},\n",
       " 631: {'grad_norm': 137.61719036102295, 'ratio': 0.7890625},\n",
       " 632: {'grad_norm': 138.11321544647217, 'ratio': 0.80859375},\n",
       " 633: {'grad_norm': 138.61034965515137, 'ratio': 0.77734375},\n",
       " 634: {'grad_norm': 139.1096305847168, 'ratio': 0.8125},\n",
       " 635: {'grad_norm': 139.6123561859131, 'ratio': 0.78515625},\n",
       " 636: {'grad_norm': 140.1151294708252, 'ratio': 0.80859375},\n",
       " 637: {'grad_norm': 140.61916828155518, 'ratio': 0.7890625},\n",
       " 638: {'grad_norm': 141.12052822113037, 'ratio': 0.796875},\n",
       " 639: {'grad_norm': 141.62646293640137, 'ratio': 0.80078125},\n",
       " 640: {'grad_norm': 142.13383197784424, 'ratio': 0.765625},\n",
       " 641: {'grad_norm': 142.64362335205078, 'ratio': 0.7890625},\n",
       " 642: {'grad_norm': 143.1548204421997, 'ratio': 0.7890625},\n",
       " 643: {'grad_norm': 143.66447257995605, 'ratio': 0.796875},\n",
       " 644: {'grad_norm': 144.17649364471436, 'ratio': 0.7890625},\n",
       " 645: {'grad_norm': 144.68971920013428, 'ratio': 0.78515625},\n",
       " 646: {'grad_norm': 145.20233249664307, 'ratio': 0.8203125},\n",
       " 647: {'grad_norm': 145.71825885772705, 'ratio': 0.80078125},\n",
       " 648: {'grad_norm': 146.23435497283936, 'ratio': 0.78125},\n",
       " 649: {'grad_norm': 146.75327682495117, 'ratio': 0.76171875},\n",
       " 650: {'grad_norm': 147.2737216949463, 'ratio': 0.7890625},\n",
       " 651: {'grad_norm': 147.79595279693604, 'ratio': 0.79296875},\n",
       " 652: {'grad_norm': 148.31740283966064, 'ratio': 0.79296875},\n",
       " 653: {'grad_norm': 148.83904838562012, 'ratio': 0.8046875},\n",
       " 654: {'grad_norm': 149.36083221435547, 'ratio': 0.796875},\n",
       " 655: {'grad_norm': 149.88436698913574, 'ratio': 0.79296875},\n",
       " 656: {'grad_norm': 150.40906715393066, 'ratio': 0.796875},\n",
       " 657: {'grad_norm': 150.93733596801758, 'ratio': 0.78515625},\n",
       " 658: {'grad_norm': 151.46503162384033, 'ratio': 0.80078125},\n",
       " 659: {'grad_norm': 151.9952907562256, 'ratio': 0.796875},\n",
       " 660: {'grad_norm': 152.52776527404785, 'ratio': 0.78125},\n",
       " 661: {'grad_norm': 153.0592679977417, 'ratio': 0.8125},\n",
       " 662: {'grad_norm': 153.5905466079712, 'ratio': 0.80078125},\n",
       " 663: {'grad_norm': 154.12636280059814, 'ratio': 0.78125},\n",
       " 664: {'grad_norm': 154.6618423461914, 'ratio': 0.8046875},\n",
       " 665: {'grad_norm': 155.19808673858643, 'ratio': 0.78515625},\n",
       " 666: {'grad_norm': 155.738263130188, 'ratio': 0.78125},\n",
       " 667: {'grad_norm': 156.27626419067383, 'ratio': 0.7890625},\n",
       " 668: {'grad_norm': 156.81522369384766, 'ratio': 0.77734375},\n",
       " 669: {'grad_norm': 157.35802364349365, 'ratio': 0.796875},\n",
       " 670: {'grad_norm': 157.8990879058838, 'ratio': 0.76953125},\n",
       " 671: {'grad_norm': 158.44212341308594, 'ratio': 0.80078125},\n",
       " 672: {'grad_norm': 158.98743534088135, 'ratio': 0.7890625},\n",
       " 673: {'grad_norm': 159.53524494171143, 'ratio': 0.77734375},\n",
       " 674: {'grad_norm': 160.08265209197998, 'ratio': 0.7890625},\n",
       " 675: {'grad_norm': 160.6333465576172, 'ratio': 0.796875},\n",
       " 676: {'grad_norm': 161.18320751190186, 'ratio': 0.79296875},\n",
       " 677: {'grad_norm': 161.73729419708252, 'ratio': 0.78515625},\n",
       " 678: {'grad_norm': 162.29271602630615, 'ratio': 0.78125},\n",
       " 679: {'grad_norm': 162.84836769104004, 'ratio': 0.7890625},\n",
       " 680: {'grad_norm': 163.40308284759521, 'ratio': 0.7890625},\n",
       " 681: {'grad_norm': 163.95997619628906, 'ratio': 0.7578125},\n",
       " 682: {'grad_norm': 164.51569747924805, 'ratio': 0.78515625},\n",
       " 683: {'grad_norm': 165.07336044311523, 'ratio': 0.78125},\n",
       " 684: {'grad_norm': 165.63357734680176, 'ratio': 0.78515625},\n",
       " 685: {'grad_norm': 166.195294380188, 'ratio': 0.76953125},\n",
       " 686: {'grad_norm': 166.75782680511475, 'ratio': 0.79296875},\n",
       " 687: {'grad_norm': 167.32309246063232, 'ratio': 0.78515625},\n",
       " 688: {'grad_norm': 167.88693809509277, 'ratio': 0.78125},\n",
       " 689: {'grad_norm': 168.45370864868164, 'ratio': 0.78515625},\n",
       " 690: {'grad_norm': 169.02066135406494, 'ratio': 0.796875},\n",
       " 691: {'grad_norm': 169.58750438690186, 'ratio': 0.78125},\n",
       " 692: {'grad_norm': 170.15798473358154, 'ratio': 0.77734375},\n",
       " 693: {'grad_norm': 170.7302303314209, 'ratio': 0.77734375},\n",
       " 694: {'grad_norm': 171.30447483062744, 'ratio': 0.79296875},\n",
       " 695: {'grad_norm': 171.8761968612671, 'ratio': 0.7890625},\n",
       " 696: {'grad_norm': 172.4536418914795, 'ratio': 0.78125},\n",
       " 697: {'grad_norm': 173.0283088684082, 'ratio': 0.80078125},\n",
       " 698: {'grad_norm': 173.60735607147217, 'ratio': 0.7734375},\n",
       " 699: {'grad_norm': 174.18745136260986, 'ratio': 0.77734375},\n",
       " 700: {'grad_norm': 174.76840019226074, 'ratio': 0.78515625},\n",
       " 701: {'grad_norm': 175.35150146484375, 'ratio': 0.79296875},\n",
       " 702: {'grad_norm': 175.93572807312012, 'ratio': 0.78515625},\n",
       " 703: {'grad_norm': 176.51913833618164, 'ratio': 0.78515625},\n",
       " 704: {'grad_norm': 177.1008653640747, 'ratio': 0.79296875},\n",
       " 705: {'grad_norm': 177.6887788772583, 'ratio': 0.80078125},\n",
       " 706: {'grad_norm': 178.2776107788086, 'ratio': 0.79296875},\n",
       " 707: {'grad_norm': 178.86817073822021, 'ratio': 0.80078125},\n",
       " 708: {'grad_norm': 179.46063327789307, 'ratio': 0.79296875},\n",
       " 709: {'grad_norm': 180.0522928237915, 'ratio': 0.80078125},\n",
       " 710: {'grad_norm': 180.64337539672852, 'ratio': 0.78125},\n",
       " 711: {'grad_norm': 181.2358913421631, 'ratio': 0.76953125},\n",
       " 712: {'grad_norm': 181.83368587493896, 'ratio': 0.80078125},\n",
       " 713: {'grad_norm': 182.4270486831665, 'ratio': 0.78515625},\n",
       " 714: {'grad_norm': 183.02570629119873, 'ratio': 0.78125},\n",
       " 715: {'grad_norm': 183.6274175643921, 'ratio': 0.77734375},\n",
       " 716: {'grad_norm': 184.22868061065674, 'ratio': 0.77734375},\n",
       " 717: {'grad_norm': 184.83141994476318, 'ratio': 0.77734375},\n",
       " 718: {'grad_norm': 185.43634796142578, 'ratio': 0.796875},\n",
       " 719: {'grad_norm': 186.04096794128418, 'ratio': 0.7890625},\n",
       " 720: {'grad_norm': 186.64611530303955, 'ratio': 0.7734375},\n",
       " 721: {'grad_norm': 187.2531499862671, 'ratio': 0.79296875},\n",
       " 722: {'grad_norm': 187.86514568328857, 'ratio': 0.76953125},\n",
       " 723: {'grad_norm': 188.47056198120117, 'ratio': 0.79296875},\n",
       " 724: {'grad_norm': 189.08054637908936, 'ratio': 0.78125},\n",
       " 725: {'grad_norm': 189.69469165802002, 'ratio': 0.78125},\n",
       " 726: {'grad_norm': 190.31220626831055, 'ratio': 0.7734375},\n",
       " 727: {'grad_norm': 190.92687892913818, 'ratio': 0.79296875},\n",
       " 728: {'grad_norm': 191.5469627380371, 'ratio': 0.8125},\n",
       " 729: {'grad_norm': 192.16517734527588, 'ratio': 0.78515625},\n",
       " 730: {'grad_norm': 192.78556728363037, 'ratio': 0.7890625},\n",
       " 731: {'grad_norm': 193.40776348114014, 'ratio': 0.7734375},\n",
       " 732: {'grad_norm': 194.03141117095947, 'ratio': 0.7890625},\n",
       " 733: {'grad_norm': 194.65592670440674, 'ratio': 0.79296875},\n",
       " 734: {'grad_norm': 195.28311157226562, 'ratio': 0.7890625},\n",
       " 735: {'grad_norm': 195.90942573547363, 'ratio': 0.796875},\n",
       " 736: {'grad_norm': 196.53236770629883, 'ratio': 0.796875},\n",
       " 737: {'grad_norm': 197.16063404083252, 'ratio': 0.80078125},\n",
       " 738: {'grad_norm': 197.78659343719482, 'ratio': 0.78125},\n",
       " 739: {'grad_norm': 198.41778564453125, 'ratio': 0.7890625},\n",
       " 740: {'grad_norm': 199.0506591796875, 'ratio': 0.78515625},\n",
       " 741: {'grad_norm': 199.68473434448242, 'ratio': 0.78125},\n",
       " 742: {'grad_norm': 200.3186264038086, 'ratio': 0.796875},\n",
       " 743: {'grad_norm': 200.9572238922119, 'ratio': 0.79296875},\n",
       " 744: {'grad_norm': 201.59499549865723, 'ratio': 0.7890625},\n",
       " 745: {'grad_norm': 202.23428344726562, 'ratio': 0.78515625},\n",
       " 746: {'grad_norm': 202.8691120147705, 'ratio': 0.796875},\n",
       " 747: {'grad_norm': 203.51021766662598, 'ratio': 0.78515625},\n",
       " 748: {'grad_norm': 204.15159797668457, 'ratio': 0.8046875},\n",
       " 749: {'grad_norm': 204.79407691955566, 'ratio': 0.76953125},\n",
       " 750: {'grad_norm': 205.4380989074707, 'ratio': 0.8046875},\n",
       " 751: {'grad_norm': 206.08557319641113, 'ratio': 0.80078125},\n",
       " 752: {'grad_norm': 206.7285041809082, 'ratio': 0.8046875},\n",
       " 753: {'grad_norm': 207.37965965270996, 'ratio': 0.7890625},\n",
       " 754: {'grad_norm': 208.02808380126953, 'ratio': 0.78125},\n",
       " 755: {'grad_norm': 208.6777400970459, 'ratio': 0.80078125},\n",
       " 756: {'grad_norm': 209.33323287963867, 'ratio': 0.77734375},\n",
       " 757: {'grad_norm': 209.99016761779785, 'ratio': 0.796875},\n",
       " 758: {'grad_norm': 210.64282989501953, 'ratio': 0.7890625},\n",
       " 759: {'grad_norm': 211.30267715454102, 'ratio': 0.78515625},\n",
       " 760: {'grad_norm': 211.96023178100586, 'ratio': 0.80078125},\n",
       " 761: {'grad_norm': 212.61606407165527, 'ratio': 0.78515625},\n",
       " 762: {'grad_norm': 213.27881050109863, 'ratio': 0.77734375},\n",
       " 763: {'grad_norm': 213.94165992736816, 'ratio': 0.7734375},\n",
       " 764: {'grad_norm': 214.60516357421875, 'ratio': 0.78515625},\n",
       " 765: {'grad_norm': 215.27099800109863, 'ratio': 0.796875},\n",
       " 766: {'grad_norm': 215.9347438812256, 'ratio': 0.7890625},\n",
       " 767: {'grad_norm': 216.60284996032715, 'ratio': 0.7734375},\n",
       " 768: {'grad_norm': 217.2707691192627, 'ratio': 0.77734375},\n",
       " 769: {'grad_norm': 217.93989944458008, 'ratio': 0.78515625},\n",
       " 770: {'grad_norm': 218.6137466430664, 'ratio': 0.78125},\n",
       " 771: {'grad_norm': 219.28478813171387, 'ratio': 0.79296875},\n",
       " 772: {'grad_norm': 219.95139122009277, 'ratio': 0.78125},\n",
       " 773: {'grad_norm': 220.62628555297852, 'ratio': 0.77734375},\n",
       " 774: {'grad_norm': 221.30594444274902, 'ratio': 0.7734375},\n",
       " 775: {'grad_norm': 221.9868450164795, 'ratio': 0.78125},\n",
       " 776: {'grad_norm': 222.66898345947266, 'ratio': 0.796875},\n",
       " 777: {'grad_norm': 223.3513126373291, 'ratio': 0.796875},\n",
       " 778: {'grad_norm': 224.03538703918457, 'ratio': 0.78515625},\n",
       " 779: {'grad_norm': 224.7227382659912, 'ratio': 0.8203125},\n",
       " 780: {'grad_norm': 225.40608978271484, 'ratio': 0.796875},\n",
       " 781: {'grad_norm': 226.09263610839844, 'ratio': 0.79296875},\n",
       " 782: {'grad_norm': 226.78243827819824, 'ratio': 0.79296875},\n",
       " 783: {'grad_norm': 227.4708709716797, 'ratio': 0.79296875},\n",
       " 784: {'grad_norm': 228.1553497314453, 'ratio': 0.80859375},\n",
       " 785: {'grad_norm': 228.84864234924316, 'ratio': 0.796875},\n",
       " 786: {'grad_norm': 229.54251289367676, 'ratio': 0.78125},\n",
       " 787: {'grad_norm': 230.23706817626953, 'ratio': 0.78515625},\n",
       " 788: {'grad_norm': 230.9355354309082, 'ratio': 0.796875},\n",
       " 789: {'grad_norm': 231.6333465576172, 'ratio': 0.78125},\n",
       " 790: {'grad_norm': 232.33070945739746, 'ratio': 0.7890625},\n",
       " 791: {'grad_norm': 233.03229904174805, 'ratio': 0.78125},\n",
       " 792: {'grad_norm': 233.73604011535645, 'ratio': 0.77734375},\n",
       " 793: {'grad_norm': 234.43867111206055, 'ratio': 0.78125},\n",
       " 794: {'grad_norm': 235.13493156433105, 'ratio': 0.796875},\n",
       " 795: {'grad_norm': 235.84171676635742, 'ratio': 0.79296875},\n",
       " 796: {'grad_norm': 236.5489959716797, 'ratio': 0.7890625},\n",
       " 797: {'grad_norm': 237.2560329437256, 'ratio': 0.78125},\n",
       " 798: {'grad_norm': 237.96455192565918, 'ratio': 0.78125},\n",
       " 799: {'grad_norm': 238.67646026611328, 'ratio': 0.79296875},\n",
       " 800: {'grad_norm': 239.3919734954834, 'ratio': 0.79296875},\n",
       " 801: {'grad_norm': 240.10597038269043, 'ratio': 0.796875},\n",
       " 802: {'grad_norm': 240.8224639892578, 'ratio': 0.77734375},\n",
       " 803: {'grad_norm': 241.5370635986328, 'ratio': 0.77734375},\n",
       " 804: {'grad_norm': 242.25397491455078, 'ratio': 0.7890625},\n",
       " 805: {'grad_norm': 242.9749412536621, 'ratio': 0.78515625},\n",
       " 806: {'grad_norm': 243.6978244781494, 'ratio': 0.7890625},\n",
       " 807: {'grad_norm': 244.42389488220215, 'ratio': 0.76953125},\n",
       " 808: {'grad_norm': 245.14505577087402, 'ratio': 0.78515625},\n",
       " 809: {'grad_norm': 245.86775970458984, 'ratio': 0.828125},\n",
       " 810: {'grad_norm': 246.59641456604004, 'ratio': 0.8125},\n",
       " 811: {'grad_norm': 247.32160758972168, 'ratio': 0.79296875},\n",
       " 812: {'grad_norm': 248.04918479919434, 'ratio': 0.77734375},\n",
       " 813: {'grad_norm': 248.7812671661377, 'ratio': 0.78125},\n",
       " 814: {'grad_norm': 249.50482559204102, 'ratio': 0.78125},\n",
       " 815: {'grad_norm': 250.23649978637695, 'ratio': 0.76953125},\n",
       " 816: {'grad_norm': 250.96864700317383, 'ratio': 0.8046875},\n",
       " 817: {'grad_norm': 251.70865058898926, 'ratio': 0.79296875},\n",
       " 818: {'grad_norm': 252.44992446899414, 'ratio': 0.78515625},\n",
       " 819: {'grad_norm': 253.19183349609375, 'ratio': 0.77734375},\n",
       " 820: {'grad_norm': 253.9361515045166, 'ratio': 0.7890625},\n",
       " 821: {'grad_norm': 254.67974853515625, 'ratio': 0.7734375},\n",
       " 822: {'grad_norm': 255.41827201843262, 'ratio': 0.7890625},\n",
       " 823: {'grad_norm': 256.1608142852783, 'ratio': 0.76953125},\n",
       " 824: {'grad_norm': 256.9082508087158, 'ratio': 0.80078125},\n",
       " 825: {'grad_norm': 257.6588897705078, 'ratio': 0.78515625},\n",
       " 826: {'grad_norm': 258.4062252044678, 'ratio': 0.7734375},\n",
       " 827: {'grad_norm': 259.16026306152344, 'ratio': 0.80078125},\n",
       " 828: {'grad_norm': 259.9049987792969, 'ratio': 0.79296875},\n",
       " 829: {'grad_norm': 260.6600704193115, 'ratio': 0.8125},\n",
       " 830: {'grad_norm': 261.4143524169922, 'ratio': 0.79296875},\n",
       " 831: {'grad_norm': 262.1705741882324, 'ratio': 0.7890625},\n",
       " 832: {'grad_norm': 262.9303321838379, 'ratio': 0.796875},\n",
       " 833: {'grad_norm': 263.6922721862793, 'ratio': 0.78125},\n",
       " 834: {'grad_norm': 264.4471035003662, 'ratio': 0.8046875},\n",
       " 835: {'grad_norm': 265.2086715698242, 'ratio': 0.81640625},\n",
       " 836: {'grad_norm': 265.9681053161621, 'ratio': 0.76171875},\n",
       " 837: {'grad_norm': 266.7294273376465, 'ratio': 0.79296875},\n",
       " 838: {'grad_norm': 267.49798583984375, 'ratio': 0.77734375},\n",
       " 839: {'grad_norm': 268.267183303833, 'ratio': 0.7890625},\n",
       " 840: {'grad_norm': 269.0314254760742, 'ratio': 0.80859375},\n",
       " 841: {'grad_norm': 269.7978286743164, 'ratio': 0.78515625},\n",
       " 842: {'grad_norm': 270.57130432128906, 'ratio': 0.80078125},\n",
       " 843: {'grad_norm': 271.3451328277588, 'ratio': 0.8046875},\n",
       " 844: {'grad_norm': 272.11861991882324, 'ratio': 0.78125},\n",
       " 845: {'grad_norm': 272.89024353027344, 'ratio': 0.796875},\n",
       " 846: {'grad_norm': 273.67082595825195, 'ratio': 0.80859375},\n",
       " 847: {'grad_norm': 274.44739723205566, 'ratio': 0.8046875},\n",
       " 848: {'grad_norm': 275.22621154785156, 'ratio': 0.79296875},\n",
       " 849: {'grad_norm': 276.0056629180908, 'ratio': 0.78515625},\n",
       " 850: {'grad_norm': 276.7864685058594, 'ratio': 0.7890625},\n",
       " 851: {'grad_norm': 277.5695266723633, 'ratio': 0.7734375},\n",
       " 852: {'grad_norm': 278.3537292480469, 'ratio': 0.80078125},\n",
       " 853: {'grad_norm': 279.1410388946533, 'ratio': 0.78515625},\n",
       " 854: {'grad_norm': 279.9266662597656, 'ratio': 0.80078125},\n",
       " 855: {'grad_norm': 280.7204532623291, 'ratio': 0.796875},\n",
       " 856: {'grad_norm': 281.50682640075684, 'ratio': 0.78125},\n",
       " 857: {'grad_norm': 282.3039798736572, 'ratio': 0.78515625},\n",
       " 858: {'grad_norm': 283.0989761352539, 'ratio': 0.78515625},\n",
       " 859: {'grad_norm': 283.89227294921875, 'ratio': 0.8046875},\n",
       " 860: {'grad_norm': 284.6839351654053, 'ratio': 0.80078125},\n",
       " 861: {'grad_norm': 285.4823474884033, 'ratio': 0.77734375},\n",
       " 862: {'grad_norm': 286.28182792663574, 'ratio': 0.80859375},\n",
       " 863: {'grad_norm': 287.0804691314697, 'ratio': 0.77734375},\n",
       " 864: {'grad_norm': 287.8806896209717, 'ratio': 0.796875},\n",
       " 865: {'grad_norm': 288.6858940124512, 'ratio': 0.79296875},\n",
       " 866: {'grad_norm': 289.4889831542969, 'ratio': 0.796875},\n",
       " 867: {'grad_norm': 290.2963695526123, 'ratio': 0.8046875},\n",
       " 868: {'grad_norm': 291.1075248718262, 'ratio': 0.78125},\n",
       " 869: {'grad_norm': 291.9111957550049, 'ratio': 0.78515625},\n",
       " 870: {'grad_norm': 292.71890449523926, 'ratio': 0.80078125},\n",
       " 871: {'grad_norm': 293.52556800842285, 'ratio': 0.80078125},\n",
       " 872: {'grad_norm': 294.3419380187988, 'ratio': 0.8046875},\n",
       " 873: {'grad_norm': 295.1606330871582, 'ratio': 0.78515625},\n",
       " 874: {'grad_norm': 295.9789733886719, 'ratio': 0.8046875},\n",
       " 875: {'grad_norm': 296.79787254333496, 'ratio': 0.79296875},\n",
       " 876: {'grad_norm': 297.6129398345947, 'ratio': 0.7890625},\n",
       " 877: {'grad_norm': 298.43342781066895, 'ratio': 0.796875},\n",
       " 878: {'grad_norm': 299.25523567199707, 'ratio': 0.79296875},\n",
       " 879: {'grad_norm': 300.07525062561035, 'ratio': 0.78515625},\n",
       " 880: {'grad_norm': 300.8999710083008, 'ratio': 0.8046875},\n",
       " 881: {'grad_norm': 301.72943115234375, 'ratio': 0.7890625},\n",
       " 882: {'grad_norm': 302.5561103820801, 'ratio': 0.79296875},\n",
       " 883: {'grad_norm': 303.3826541900635, 'ratio': 0.77734375},\n",
       " 884: {'grad_norm': 304.2118320465088, 'ratio': 0.796875},\n",
       " 885: {'grad_norm': 305.04931259155273, 'ratio': 0.7890625},\n",
       " 886: {'grad_norm': 305.8838920593262, 'ratio': 0.79296875},\n",
       " 887: {'grad_norm': 306.7079734802246, 'ratio': 0.7734375},\n",
       " 888: {'grad_norm': 307.5449962615967, 'ratio': 0.79296875},\n",
       " 889: {'grad_norm': 308.3841609954834, 'ratio': 0.76953125},\n",
       " 890: {'grad_norm': 309.2183647155762, 'ratio': 0.80078125},\n",
       " 891: {'grad_norm': 310.06170654296875, 'ratio': 0.78515625},\n",
       " 892: {'grad_norm': 310.9072742462158, 'ratio': 0.8046875},\n",
       " 893: {'grad_norm': 311.7539920806885, 'ratio': 0.796875},\n",
       " 894: {'grad_norm': 312.6004219055176, 'ratio': 0.796875},\n",
       " 895: {'grad_norm': 313.44470977783203, 'ratio': 0.80078125},\n",
       " 896: {'grad_norm': 314.2982006072998, 'ratio': 0.8125},\n",
       " 897: {'grad_norm': 315.14678382873535, 'ratio': 0.796875},\n",
       " 898: {'grad_norm': 315.99964332580566, 'ratio': 0.78125},\n",
       " 899: {'grad_norm': 316.84066581726074, 'ratio': 0.79296875},\n",
       " 900: {'grad_norm': 317.69474601745605, 'ratio': 0.8046875},\n",
       " 901: {'grad_norm': 318.55389404296875, 'ratio': 0.80078125},\n",
       " 902: {'grad_norm': 319.40885162353516, 'ratio': 0.78515625},\n",
       " 903: {'grad_norm': 320.2637424468994, 'ratio': 0.78125},\n",
       " 904: {'grad_norm': 321.12074089050293, 'ratio': 0.80859375},\n",
       " 905: {'grad_norm': 321.9853763580322, 'ratio': 0.78125},\n",
       " 906: {'grad_norm': 322.849063873291, 'ratio': 0.78125},\n",
       " 907: {'grad_norm': 323.7130813598633, 'ratio': 0.78515625},\n",
       " 908: {'grad_norm': 324.5718631744385, 'ratio': 0.80078125},\n",
       " 909: {'grad_norm': 325.4400939941406, 'ratio': 0.8046875},\n",
       " 910: {'grad_norm': 326.314884185791, 'ratio': 0.78125},\n",
       " 911: {'grad_norm': 327.18323135375977, 'ratio': 0.796875},\n",
       " 912: {'grad_norm': 328.059513092041, 'ratio': 0.80078125},\n",
       " 913: {'grad_norm': 328.9294128417969, 'ratio': 0.80859375},\n",
       " 914: {'grad_norm': 329.8018856048584, 'ratio': 0.78515625},\n",
       " 915: {'grad_norm': 330.67643547058105, 'ratio': 0.796875},\n",
       " 916: {'grad_norm': 331.5493640899658, 'ratio': 0.78515625},\n",
       " 917: {'grad_norm': 332.4254207611084, 'ratio': 0.77734375},\n",
       " 918: {'grad_norm': 333.30660247802734, 'ratio': 0.796875},\n",
       " 919: {'grad_norm': 334.18836975097656, 'ratio': 0.78515625},\n",
       " 920: {'grad_norm': 335.0719909667969, 'ratio': 0.7890625},\n",
       " 921: {'grad_norm': 335.9584655761719, 'ratio': 0.7890625},\n",
       " 922: {'grad_norm': 336.8412170410156, 'ratio': 0.7890625},\n",
       " 923: {'grad_norm': 337.73385429382324, 'ratio': 0.77734375},\n",
       " 924: {'grad_norm': 338.62182807922363, 'ratio': 0.8046875},\n",
       " 925: {'grad_norm': 339.50796699523926, 'ratio': 0.79296875},\n",
       " 926: {'grad_norm': 340.40389251708984, 'ratio': 0.79296875},\n",
       " 927: {'grad_norm': 341.29989433288574, 'ratio': 0.80859375},\n",
       " 928: {'grad_norm': 342.1861000061035, 'ratio': 0.78515625},\n",
       " 929: {'grad_norm': 343.0802803039551, 'ratio': 0.8046875},\n",
       " 930: {'grad_norm': 343.96914863586426, 'ratio': 0.79296875},\n",
       " 931: {'grad_norm': 344.86845207214355, 'ratio': 0.78515625},\n",
       " 932: {'grad_norm': 345.76843070983887, 'ratio': 0.80078125},\n",
       " 933: {'grad_norm': 346.67375564575195, 'ratio': 0.79296875},\n",
       " 934: {'grad_norm': 347.5704765319824, 'ratio': 0.7890625},\n",
       " 935: {'grad_norm': 348.47873878479004, 'ratio': 0.80078125},\n",
       " 936: {'grad_norm': 349.37855529785156, 'ratio': 0.80859375},\n",
       " 937: {'grad_norm': 350.2897415161133, 'ratio': 0.7890625},\n",
       " 938: {'grad_norm': 351.20518684387207, 'ratio': 0.78515625},\n",
       " 939: {'grad_norm': 352.1177177429199, 'ratio': 0.76953125},\n",
       " 940: {'grad_norm': 353.0358352661133, 'ratio': 0.76953125},\n",
       " 941: {'grad_norm': 353.94046211242676, 'ratio': 0.80078125},\n",
       " 942: {'grad_norm': 354.8575038909912, 'ratio': 0.7890625},\n",
       " 943: {'grad_norm': 355.77513122558594, 'ratio': 0.79296875},\n",
       " 944: {'grad_norm': 356.6923770904541, 'ratio': 0.7734375},\n",
       " 945: {'grad_norm': 357.6113815307617, 'ratio': 0.80078125},\n",
       " 946: {'grad_norm': 358.5333023071289, 'ratio': 0.8125},\n",
       " 947: {'grad_norm': 359.45910263061523, 'ratio': 0.7734375},\n",
       " 948: {'grad_norm': 360.38233184814453, 'ratio': 0.796875},\n",
       " 949: {'grad_norm': 361.30014991760254, 'ratio': 0.78515625},\n",
       " 950: {'grad_norm': 362.23072242736816, 'ratio': 0.81640625},\n",
       " 951: {'grad_norm': 363.1605052947998, 'ratio': 0.80078125},\n",
       " 952: {'grad_norm': 364.08577728271484, 'ratio': 0.8125},\n",
       " 953: {'grad_norm': 365.0171241760254, 'ratio': 0.78515625},\n",
       " 954: {'grad_norm': 365.955078125, 'ratio': 0.79296875},\n",
       " 955: {'grad_norm': 366.88351249694824, 'ratio': 0.78515625},\n",
       " 956: {'grad_norm': 367.8200225830078, 'ratio': 0.765625},\n",
       " 957: {'grad_norm': 368.75003814697266, 'ratio': 0.77734375},\n",
       " 958: {'grad_norm': 369.6947937011719, 'ratio': 0.8203125},\n",
       " 959: {'grad_norm': 370.63373947143555, 'ratio': 0.8046875},\n",
       " 960: {'grad_norm': 371.58285903930664, 'ratio': 0.76953125},\n",
       " 961: {'grad_norm': 372.5336265563965, 'ratio': 0.81640625},\n",
       " 962: {'grad_norm': 373.4596252441406, 'ratio': 0.80859375},\n",
       " 963: {'grad_norm': 374.4080123901367, 'ratio': 0.8046875},\n",
       " 964: {'grad_norm': 375.3562812805176, 'ratio': 0.8046875},\n",
       " 965: {'grad_norm': 376.30715560913086, 'ratio': 0.78125},\n",
       " 966: {'grad_norm': 377.2630729675293, 'ratio': 0.7890625},\n",
       " 967: {'grad_norm': 378.2150993347168, 'ratio': 0.80078125},\n",
       " 968: {'grad_norm': 379.1697006225586, 'ratio': 0.79296875},\n",
       " 969: {'grad_norm': 380.1185607910156, 'ratio': 0.78515625},\n",
       " 970: {'grad_norm': 381.0809898376465, 'ratio': 0.79296875},\n",
       " 971: {'grad_norm': 382.0348815917969, 'ratio': 0.8046875},\n",
       " 972: {'grad_norm': 383.00089263916016, 'ratio': 0.796875},\n",
       " 973: {'grad_norm': 383.9621047973633, 'ratio': 0.80859375},\n",
       " 974: {'grad_norm': 384.92004013061523, 'ratio': 0.8046875},\n",
       " 975: {'grad_norm': 385.88585662841797, 'ratio': 0.796875},\n",
       " 976: {'grad_norm': 386.85120391845703, 'ratio': 0.796875},\n",
       " 977: {'grad_norm': 387.82714462280273, 'ratio': 0.79296875},\n",
       " 978: {'grad_norm': 388.7968940734863, 'ratio': 0.78125},\n",
       " 979: {'grad_norm': 389.76326751708984, 'ratio': 0.78515625},\n",
       " 980: {'grad_norm': 390.73996353149414, 'ratio': 0.79296875},\n",
       " 981: {'grad_norm': 391.7123489379883, 'ratio': 0.78125},\n",
       " 982: {'grad_norm': 392.68806076049805, 'ratio': 0.7890625},\n",
       " 983: {'grad_norm': 393.662540435791, 'ratio': 0.78515625},\n",
       " 984: {'grad_norm': 394.648681640625, 'ratio': 0.79296875},\n",
       " 985: {'grad_norm': 395.6277770996094, 'ratio': 0.78125},\n",
       " 986: {'grad_norm': 396.6123161315918, 'ratio': 0.80859375},\n",
       " 987: {'grad_norm': 397.5926284790039, 'ratio': 0.80078125},\n",
       " 988: {'grad_norm': 398.57362365722656, 'ratio': 0.77734375},\n",
       " 989: {'grad_norm': 399.5634765625, 'ratio': 0.8046875},\n",
       " 990: {'grad_norm': 400.5473098754883, 'ratio': 0.76953125},\n",
       " 991: {'grad_norm': 401.5359916687012, 'ratio': 0.80078125},\n",
       " 992: {'grad_norm': 402.53027725219727, 'ratio': 0.7890625},\n",
       " 993: {'grad_norm': 403.51707458496094, 'ratio': 0.796875},\n",
       " 994: {'grad_norm': 404.5145606994629, 'ratio': 0.78125},\n",
       " 995: {'grad_norm': 405.50288009643555, 'ratio': 0.8046875},\n",
       " 996: {'grad_norm': 406.500732421875, 'ratio': 0.8046875},\n",
       " 997: {'grad_norm': 407.4978790283203, 'ratio': 0.79296875},\n",
       " 998: {'grad_norm': 408.48776626586914, 'ratio': 0.796875},\n",
       " 999: {'grad_norm': 409.4937515258789, 'ratio': 0.78125},\n",
       " ...}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_norm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "687c3d92"
   },
   "outputs": [],
   "source": [
    "val_losses_1 = [r['val_loss'] for r in history_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e2f5c55",
    "outputId": "b19aad1c-1fca-4d00-b722-2df787778084"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "f062c4e1"
   },
   "outputs": [],
   "source": [
    "minimal_ratio_eps = [i['ratio'] for i in grad_norm_1.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "6f17a6cb",
    "outputId": "f3d42300-e43f-48c5-dec2-4419468573b8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(minimal_ratio_eps,val_losses_1,color = 'hotpink')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('minimal ratio')\n",
    "plt.title('minimal ratio to the loss');\n",
    "plt.plot()\n",
    "plt.savefig(path+'/ZeroGrad.pdf',\n",
    "            dpi=700,\n",
    "            bbox_inches='tight')\n",
    "plt.savefig(\"loss_vs_minimal ratio.png\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "008a0d6c9d10496882d7d4c9cb9cb9d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5768f0cbc8474c6e919fbe506f0a97c2",
      "placeholder": "​",
      "style": "IPY_MODEL_0391dc0805d1454cae4a78fbedfd7775",
      "value": "100%"
     }
    },
    "0391dc0805d1454cae4a78fbedfd7775": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09043a55c2ad40a6891fddb2aed5fe56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b91696087f140d1af6ff5f0caf582eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be8a93d4e9244856972cf0a2417895c4",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c8f13f47cf54f6ea4f58bee0d30f143",
      "value": 4542
     }
    },
    "0c08dcb150944e4abd79b126db32d832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_622c57f54d7749dc8ad435c6bdf3c1a7",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49b254d5ccb345dbaf90f6fd469f8ad0",
      "value": 1648877
     }
    },
    "0cb54e62900048118fcba497e738e0ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f16544309b34f33b91078f62467e22d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10178fdd866c4f9e991ec6af220e5deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16290f3f7f11437e8ad3fdb18d98ab4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e125399af7c4b768445ebb283b1e646": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24365b923a754b7fa9afcfc2b56152c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d62f0889fc6c4c4c85bbe15e9bec8f90",
      "placeholder": "​",
      "style": "IPY_MODEL_a0c688254b0444d1aa6c6fa90f6ff683",
      "value": " 9912422/9912422 [00:00&lt;00:00, 32532610.83it/s]"
     }
    },
    "255245d586c4434d949f8a8b0802bf27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cb54e62900048118fcba497e738e0ab",
      "placeholder": "​",
      "style": "IPY_MODEL_91c70b59ab714c578922842db2c9ac82",
      "value": " 28881/28881 [00:00&lt;00:00, 1026104.10it/s]"
     }
    },
    "49b254d5ccb345dbaf90f6fd469f8ad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "49fd3ae2b07f405182c18aa967196f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f00d225b40447fd833957d8afce79fa",
       "IPY_MODEL_0b91696087f140d1af6ff5f0caf582eb",
       "IPY_MODEL_fa472e9026694f3aa748c587fec70a96"
      ],
      "layout": "IPY_MODEL_8395306a5c834e85a8d044da0fe4869f"
     }
    },
    "4f00d225b40447fd833957d8afce79fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3b1d3bc662a4dee95d8915ed64719da",
      "placeholder": "​",
      "style": "IPY_MODEL_0f16544309b34f33b91078f62467e22d",
      "value": "100%"
     }
    },
    "5768f0cbc8474c6e919fbe506f0a97c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "581a5fd9f3214e6bb7c7570210eef316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cad7348b5c84f38865edbc281431f71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6185f8acdcf74de2a0b891b7acbbdfdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_581a5fd9f3214e6bb7c7570210eef316",
      "placeholder": "​",
      "style": "IPY_MODEL_09043a55c2ad40a6891fddb2aed5fe56",
      "value": " 1648877/1648877 [00:00&lt;00:00, 10782190.88it/s]"
     }
    },
    "622c57f54d7749dc8ad435c6bdf3c1a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "649eff8fd4be4821905023645d38fbc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72f57520945248f49111e1ab37ef2e3c",
       "IPY_MODEL_0c08dcb150944e4abd79b126db32d832",
       "IPY_MODEL_6185f8acdcf74de2a0b891b7acbbdfdc"
      ],
      "layout": "IPY_MODEL_7b6e5967c65e45ff82ef633417382e0e"
     }
    },
    "6c8f13f47cf54f6ea4f58bee0d30f143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "71b7f6cabfbc463f972c2df245390dfa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72f57520945248f49111e1ab37ef2e3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71b7f6cabfbc463f972c2df245390dfa",
      "placeholder": "​",
      "style": "IPY_MODEL_b026cfc07b824607be726b27a3ad7ff3",
      "value": "100%"
     }
    },
    "7586d4b6fe804393ab1a84543a65c974": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c1697b8b6fc429ca0d75be196f4864a",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d382227a0484f838d8e398c97942b9e",
      "value": 9912422
     }
    },
    "788dffc4ced742689a11b224e71fa07b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2bb008f21a84df4aab34b5e94b6d270",
      "placeholder": "​",
      "style": "IPY_MODEL_d9da13664c084fdb96dcf9bfacc51861",
      "value": "100%"
     }
    },
    "78ba7f4ace9948cbb3a2ded0a88cb812": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_788dffc4ced742689a11b224e71fa07b",
       "IPY_MODEL_97d5de2b663c4b038d01e76c6fbd52e0",
       "IPY_MODEL_255245d586c4434d949f8a8b0802bf27"
      ],
      "layout": "IPY_MODEL_1e125399af7c4b768445ebb283b1e646"
     }
    },
    "7b6e5967c65e45ff82ef633417382e0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d382227a0484f838d8e398c97942b9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8395306a5c834e85a8d044da0fe4869f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c1697b8b6fc429ca0d75be196f4864a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91c70b59ab714c578922842db2c9ac82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97d5de2b663c4b038d01e76c6fbd52e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cad7348b5c84f38865edbc281431f71",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2a9bdd6c20045d58afa9846794f4e6e",
      "value": 28881
     }
    },
    "9da5181c9de648f09319ce02afa67910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0c688254b0444d1aa6c6fa90f6ff683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b026cfc07b824607be726b27a3ad7ff3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be8a93d4e9244856972cf0a2417895c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2bb008f21a84df4aab34b5e94b6d270": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d62f0889fc6c4c4c85bbe15e9bec8f90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9da13664c084fdb96dcf9bfacc51861": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3b1d3bc662a4dee95d8915ed64719da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a9bdd6c20045d58afa9846794f4e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f71b5d250d2e42aeb7d89dd15018b6ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_008a0d6c9d10496882d7d4c9cb9cb9d9",
       "IPY_MODEL_7586d4b6fe804393ab1a84543a65c974",
       "IPY_MODEL_24365b923a754b7fa9afcfc2b56152c0"
      ],
      "layout": "IPY_MODEL_16290f3f7f11437e8ad3fdb18d98ab4f"
     }
    },
    "fa472e9026694f3aa748c587fec70a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9da5181c9de648f09319ce02afa67910",
      "placeholder": "​",
      "style": "IPY_MODEL_10178fdd866c4f9e991ec6af220e5deb",
      "value": " 4542/4542 [00:00&lt;00:00, 182952.99it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
